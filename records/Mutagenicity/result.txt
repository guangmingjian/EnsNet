Dataset: Mutagenicity,
Model Name: EnsNet
net_params={'num_layers': 3, 'hidden': 64, 'dropout': 0.1, 'ds_name': 'Mutagenicity', 'model_type': 'ThreeModel', 'temperature': 0.5, 'beta': 1, 'gama': 0.0005, 'yta': 50, 'alpha': 1, 'in_channels': 14, 'out_channels': 2, 'device': 'cuda:1'}
train_config={'epochs': 300, 'batch_size': 128, 'seed': 8971, 'patience': 50, 'lr': 0.005, 'weight_decay': 1e-05}
EnsNet(
  (model1): SAGPool(
    (convs): ModuleList(
      (0): SAGEConv(14, 64)
      (1): SAGEConv(64, 64)
      (2): SAGEConv(64, 64)
    )
    (pools): ModuleList(
      (0): SAGPooling(
        (score_layer): GCNConv(192, 1)
      )
    )
    (lin1): Linear(in_features=384, out_features=64, bias=True)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
      (2): BatchNorm(64)
    )
    (lin2): Linear(in_features=64, out_features=32, bias=True)
    (lin3): Linear(in_features=32, out_features=2, bias=True)
  )
  (model2): SAGPool(
    (convs): ModuleList(
      (0): SAGEConv(14, 64)
      (1): SAGEConv(64, 64)
      (2): SAGEConv(64, 64)
    )
    (pools): ModuleList(
      (0): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
      (1): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
      (2): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
    )
    (lin1): Linear(in_features=128, out_features=64, bias=True)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
      (2): BatchNorm(64)
    )
    (lin2): Linear(in_features=64, out_features=32, bias=True)
    (lin3): Linear(in_features=32, out_features=2, bias=True)
  )
  (model3): GlobalAttentionNet(
    (convs): ModuleList(
      (0): SAGEConv(14, 64)
      (1): SAGEConv(64, 64)
      (2): SAGEConv(64, 64)
    )
    (att): GlobalAttention(gate_nn=Linear(in_features=64, out_features=1, bias=True), nn=None)
    (dropout): Dropout(p=0.3, inplace=False)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
      (2): BatchNorm(64)
    )
    (lin1): Linear(in_features=64, out_features=64, bias=True)
    (lin2): Linear(in_features=64, out_features=2, bias=True)
  )
  (confiNet): Sequential(
    (0): Linear(in_features=66, out_features=33, bias=True)
    (1): ReLU()
    (2): Linear(in_features=33, out_features=1, bias=True)
    (3): Sigmoid()
  )
)

fold [0/10] is start!!
[01/0001] | train_loss:2.6118 val_acc:61.6628 val_loss:0.6731
model is saved at epoch 1!![01/0002] | train_loss:2.3336 val_acc:64.8961 val_loss:0.6088
model is saved at epoch 2!![01/0003] | train_loss:2.2581 val_acc:77.8291 val_loss:0.504
model is saved at epoch 3!![01/0004] | train_loss:2.1762 val_acc:77.8291 val_loss:0.4743
[01/0005] | train_loss:2.1671 val_acc:79.2148 val_loss:0.4592
model is saved at epoch 5!![01/0006] | train_loss:2.1191 val_acc:78.291 val_loss:0.5061
[01/0007] | train_loss:2.0935 val_acc:79.2148 val_loss:0.4777
[01/0008] | train_loss:2.0756 val_acc:78.9838 val_loss:0.4509
[01/0009] | train_loss:2.0577 val_acc:81.0624 val_loss:0.454
model is saved at epoch 9!![01/0010] | train_loss:2.0227 val_acc:79.2148 val_loss:0.5068
[01/0011] | train_loss:2.0677 val_acc:75.5196 val_loss:0.5179
[01/0012] | train_loss:2.0557 val_acc:81.0624 val_loss:0.4846
[01/0013] | train_loss:1.9983 val_acc:80.1386 val_loss:0.4575
[01/0014] | train_loss:1.9945 val_acc:79.4457 val_loss:0.4607
[01/0015] | train_loss:1.9675 val_acc:80.6005 val_loss:0.452
[01/0016] | train_loss:1.9034 val_acc:81.2933 val_loss:0.4456
model is saved at epoch 16!![01/0017] | train_loss:1.8803 val_acc:78.291 val_loss:0.4702
[01/0018] | train_loss:1.8819 val_acc:79.9076 val_loss:0.4465
[01/0019] | train_loss:1.9238 val_acc:80.3695 val_loss:0.473
[01/0020] | train_loss:1.8642 val_acc:81.9861 val_loss:0.4423
model is saved at epoch 20!![01/0021] | train_loss:1.882 val_acc:82.9099 val_loss:0.4472
model is saved at epoch 21!![01/0022] | train_loss:1.845 val_acc:81.0624 val_loss:0.4293
[01/0023] | train_loss:1.8338 val_acc:81.9861 val_loss:0.4388
[01/0024] | train_loss:1.8755 val_acc:83.1409 val_loss:0.4115
model is saved at epoch 24!![01/0025] | train_loss:1.878 val_acc:82.2171 val_loss:0.4254
[01/0026] | train_loss:1.825 val_acc:81.5242 val_loss:0.4578
[01/0027] | train_loss:1.7794 val_acc:81.7552 val_loss:0.4882
[01/0028] | train_loss:1.7771 val_acc:80.8314 val_loss:0.4865
[01/0029] | train_loss:1.745 val_acc:82.2171 val_loss:0.4313
[01/0030] | train_loss:1.7321 val_acc:83.3718 val_loss:0.4179
model is saved at epoch 30!![01/0031] | train_loss:1.7386 val_acc:82.679 val_loss:0.4334
[01/0032] | train_loss:1.7242 val_acc:83.3718 val_loss:0.4461
[01/0033] | train_loss:1.7354 val_acc:83.8337 val_loss:0.4335
model is saved at epoch 33!![01/0034] | train_loss:1.6703 val_acc:82.448 val_loss:0.4383
[01/0035] | train_loss:1.7317 val_acc:82.9099 val_loss:0.4534
[01/0036] | train_loss:1.6865 val_acc:82.9099 val_loss:0.4491
[01/0037] | train_loss:1.6764 val_acc:78.291 val_loss:0.5412
[01/0038] | train_loss:1.6852 val_acc:82.2171 val_loss:0.4801
[01/0039] | train_loss:1.6125 val_acc:81.9861 val_loss:0.4798
[01/0040] | train_loss:1.6207 val_acc:83.1409 val_loss:0.4588
[01/0041] | train_loss:1.5744 val_acc:83.6028 val_loss:0.4999
[01/0042] | train_loss:1.6424 val_acc:81.7552 val_loss:0.4856
[01/0043] | train_loss:1.5422 val_acc:80.8314 val_loss:0.4956
[01/0044] | train_loss:1.5396 val_acc:83.3718 val_loss:0.4486
[01/0045] | train_loss:1.5696 val_acc:83.8337 val_loss:0.4315
[01/0046] | train_loss:1.6142 val_acc:81.5242 val_loss:0.4474
[01/0047] | train_loss:1.5435 val_acc:81.0624 val_loss:0.4963
[01/0048] | train_loss:1.5556 val_acc:81.0624 val_loss:0.5486
[01/0049] | train_loss:1.5098 val_acc:84.2956 val_loss:0.4548
model is saved at epoch 49!![01/0050] | train_loss:1.5414 val_acc:83.1409 val_loss:0.4294
[01/0051] | train_loss:1.5147 val_acc:84.9885 val_loss:0.4837
model is saved at epoch 51!![01/0052] | train_loss:1.5051 val_acc:82.448 val_loss:0.4983
[01/0053] | train_loss:1.4875 val_acc:83.8337 val_loss:0.511
[01/0054] | train_loss:1.4865 val_acc:85.4503 val_loss:0.4676
model is saved at epoch 54!![01/0055] | train_loss:1.457 val_acc:84.7575 val_loss:0.4938
[01/0056] | train_loss:1.4828 val_acc:82.9099 val_loss:0.505
[01/0057] | train_loss:1.5262 val_acc:82.2171 val_loss:0.4987
[01/0058] | train_loss:1.5118 val_acc:83.1409 val_loss:0.4931
[01/0059] | train_loss:1.4873 val_acc:81.5242 val_loss:0.5704
[01/0060] | train_loss:1.4511 val_acc:81.7552 val_loss:0.537
[01/0061] | train_loss:1.4447 val_acc:83.8337 val_loss:0.4906
[01/0062] | train_loss:1.4551 val_acc:81.5242 val_loss:0.5384
[01/0063] | train_loss:1.3997 val_acc:82.2171 val_loss:0.549
[01/0064] | train_loss:1.4063 val_acc:82.679 val_loss:0.5021
[01/0065] | train_loss:1.4498 val_acc:83.8337 val_loss:0.4839
[01/0066] | train_loss:1.419 val_acc:81.5242 val_loss:0.499
[01/0067] | train_loss:1.3994 val_acc:83.1409 val_loss:0.5797
[01/0068] | train_loss:1.3461 val_acc:82.2171 val_loss:0.5564
[01/0069] | train_loss:1.3249 val_acc:82.679 val_loss:0.5793
[01/0070] | train_loss:1.4061 val_acc:82.679 val_loss:0.6025
[01/0071] | train_loss:1.4201 val_acc:84.9885 val_loss:0.4927
[01/0072] | train_loss:1.3397 val_acc:83.6028 val_loss:0.5349
[01/0073] | train_loss:1.3453 val_acc:83.6028 val_loss:0.563
[01/0074] | train_loss:1.362 val_acc:84.9885 val_loss:0.5202
[01/0075] | train_loss:1.3349 val_acc:83.1409 val_loss:0.5348
[01/0076] | train_loss:1.4027 val_acc:84.5266 val_loss:0.5325
[01/0077] | train_loss:1.3517 val_acc:84.0647 val_loss:0.621
[01/0078] | train_loss:1.3126 val_acc:82.679 val_loss:0.566
[01/0079] | train_loss:1.2965 val_acc:84.9885 val_loss:0.5127
[01/0080] | train_loss:1.2737 val_acc:82.448 val_loss:0.489
[01/0081] | train_loss:1.3114 val_acc:84.7575 val_loss:0.54
[01/0082] | train_loss:1.2798 val_acc:82.2171 val_loss:0.6255
[01/0083] | train_loss:1.3322 val_acc:82.448 val_loss:0.5542
[01/0084] | train_loss:1.3008 val_acc:84.2956 val_loss:0.5619
[01/0085] | train_loss:1.2293 val_acc:83.6028 val_loss:0.6319
[01/0086] | train_loss:1.2375 val_acc:82.9099 val_loss:0.6401
[01/0087] | train_loss:1.2799 val_acc:84.0647 val_loss:0.5058
[01/0088] | train_loss:1.2738 val_acc:81.9861 val_loss:0.6492
[01/0089] | train_loss:1.2371 val_acc:82.9099 val_loss:0.6184
[01/0090] | train_loss:1.248 val_acc:84.2956 val_loss:0.545
[01/0091] | train_loss:1.27 val_acc:84.9885 val_loss:0.5487
[01/0092] | train_loss:1.3521 val_acc:84.5266 val_loss:0.5125
[01/0093] | train_loss:1.2516 val_acc:83.6028 val_loss:0.5273
[01/0094] | train_loss:1.2633 val_acc:84.9885 val_loss:0.5315
[01/0095] | train_loss:1.2469 val_acc:84.5266 val_loss:0.5161
[01/0096] | train_loss:1.2123 val_acc:84.7575 val_loss:0.528
[01/0097] | train_loss:1.2468 val_acc:84.0647 val_loss:0.5528
[01/0098] | train_loss:1.2198 val_acc:84.2956 val_loss:0.5191
[01/0099] | train_loss:1.172 val_acc:84.9885 val_loss:0.5611
[01/0100] | train_loss:1.3006 val_acc:84.5266 val_loss:0.5066
[01/0101] | train_loss:1.2625 val_acc:83.8337 val_loss:0.5807
[01/0102] | train_loss:1.2196 val_acc:84.0647 val_loss:0.5549
[01/0103] | train_loss:1.1621 val_acc:84.7575 val_loss:0.595
[01/0104] | train_loss:1.1747 val_acc:84.9885 val_loss:0.6079
[01/0105] | train_loss:1.1737 val_acc:84.7575 val_loss:0.6142
Fold: [1/10] Test is finish !! 
 Test Metrics are: test_acc:85.023 test_loss:0.3445fold [1/10] is start!!
[02/0001] | train_loss:2.5483 val_acc:44.0092 val_loss:0.7109
model is saved at epoch 1!![02/0002] | train_loss:2.3362 val_acc:55.53 val_loss:0.66
model is saved at epoch 2!![02/0003] | train_loss:2.2199 val_acc:75.1152 val_loss:0.5328
model is saved at epoch 3!![02/0004] | train_loss:2.1879 val_acc:76.0369 val_loss:0.52
model is saved at epoch 4!![02/0005] | train_loss:2.1406 val_acc:77.4194 val_loss:0.4796
model is saved at epoch 5!![02/0006] | train_loss:2.0897 val_acc:70.9677 val_loss:0.5379
[02/0007] | train_loss:2.0877 val_acc:81.106 val_loss:0.4781
model is saved at epoch 7!![02/0008] | train_loss:2.0426 val_acc:79.2627 val_loss:0.4757
[02/0009] | train_loss:2.0022 val_acc:78.341 val_loss:0.4785
[02/0010] | train_loss:2.0156 val_acc:81.106 val_loss:0.4402
[02/0011] | train_loss:1.9463 val_acc:79.2627 val_loss:0.4565
[02/0012] | train_loss:1.8976 val_acc:79.7235 val_loss:0.4527
[02/0013] | train_loss:1.9389 val_acc:79.9539 val_loss:0.4505
[02/0014] | train_loss:1.8544 val_acc:83.1797 val_loss:0.4149
model is saved at epoch 14!![02/0015] | train_loss:1.8178 val_acc:82.2581 val_loss:0.4342
[02/0016] | train_loss:1.8658 val_acc:82.4885 val_loss:0.4301
[02/0017] | train_loss:1.8578 val_acc:82.4885 val_loss:0.4104
[02/0018] | train_loss:1.8028 val_acc:82.4885 val_loss:0.4081
[02/0019] | train_loss:1.8148 val_acc:82.2581 val_loss:0.4259
[02/0020] | train_loss:1.8168 val_acc:82.4885 val_loss:0.4167
[02/0021] | train_loss:1.7894 val_acc:81.7972 val_loss:0.4008
[02/0022] | train_loss:1.8189 val_acc:84.3318 val_loss:0.3885
model is saved at epoch 22!![02/0023] | train_loss:1.7681 val_acc:82.0276 val_loss:0.416
[02/0024] | train_loss:1.736 val_acc:81.7972 val_loss:0.4152
[02/0025] | train_loss:1.7364 val_acc:82.2581 val_loss:0.4095
[02/0026] | train_loss:1.7472 val_acc:82.4885 val_loss:0.4533
[02/0027] | train_loss:1.7604 val_acc:83.1797 val_loss:0.384
[02/0028] | train_loss:1.6961 val_acc:83.871 val_loss:0.3894
[02/0029] | train_loss:1.6849 val_acc:80.8756 val_loss:0.4706
[02/0030] | train_loss:1.6863 val_acc:80.8756 val_loss:0.42
[02/0031] | train_loss:1.6369 val_acc:82.9493 val_loss:0.401
[02/0032] | train_loss:1.6793 val_acc:82.4885 val_loss:0.4047
[02/0033] | train_loss:1.632 val_acc:83.4101 val_loss:0.4142
[02/0034] | train_loss:1.6876 val_acc:81.7972 val_loss:0.469
[02/0035] | train_loss:1.6562 val_acc:83.1797 val_loss:0.4786
[02/0036] | train_loss:1.5936 val_acc:82.4885 val_loss:0.4939
[02/0037] | train_loss:1.6296 val_acc:83.1797 val_loss:0.4083
[02/0038] | train_loss:1.5863 val_acc:84.1014 val_loss:0.4413
[02/0039] | train_loss:1.5869 val_acc:83.871 val_loss:0.406
[02/0040] | train_loss:1.5971 val_acc:82.0276 val_loss:0.431
[02/0041] | train_loss:1.5346 val_acc:83.4101 val_loss:0.4606
[02/0042] | train_loss:1.5054 val_acc:83.4101 val_loss:0.4665
[02/0043] | train_loss:1.5164 val_acc:82.9493 val_loss:0.4613
[02/0044] | train_loss:1.4919 val_acc:85.4839 val_loss:0.446
model is saved at epoch 44!![02/0045] | train_loss:1.5026 val_acc:85.9447 val_loss:0.4062
model is saved at epoch 45!![02/0046] | train_loss:1.4759 val_acc:84.1014 val_loss:0.4331
[02/0047] | train_loss:1.4937 val_acc:83.871 val_loss:0.429
[02/0048] | train_loss:1.5828 val_acc:82.9493 val_loss:0.4409
[02/0049] | train_loss:1.5371 val_acc:85.2535 val_loss:0.4286
[02/0050] | train_loss:1.4733 val_acc:83.871 val_loss:0.4279
[02/0051] | train_loss:1.4492 val_acc:85.9447 val_loss:0.4686
[02/0052] | train_loss:1.4382 val_acc:83.871 val_loss:0.4239
[02/0053] | train_loss:1.4294 val_acc:84.5622 val_loss:0.4629
[02/0054] | train_loss:1.4347 val_acc:83.4101 val_loss:0.4539
[02/0055] | train_loss:1.4073 val_acc:82.7189 val_loss:0.495
[02/0056] | train_loss:1.3679 val_acc:82.9493 val_loss:0.5485
[02/0057] | train_loss:1.3698 val_acc:83.871 val_loss:0.4889
[02/0058] | train_loss:1.3902 val_acc:83.4101 val_loss:0.5683
[02/0059] | train_loss:1.4345 val_acc:83.1797 val_loss:0.5284
[02/0060] | train_loss:1.4314 val_acc:85.9447 val_loss:0.5065
[02/0061] | train_loss:1.3585 val_acc:83.4101 val_loss:0.4669
[02/0062] | train_loss:1.4008 val_acc:84.1014 val_loss:0.4778
[02/0063] | train_loss:1.338 val_acc:83.4101 val_loss:0.498
[02/0064] | train_loss:1.3217 val_acc:82.7189 val_loss:0.4453
[02/0065] | train_loss:1.3374 val_acc:84.7926 val_loss:0.4313
[02/0066] | train_loss:1.2669 val_acc:82.2581 val_loss:0.5049
[02/0067] | train_loss:1.3609 val_acc:81.5668 val_loss:0.5031
[02/0068] | train_loss:1.3373 val_acc:83.871 val_loss:0.486
[02/0069] | train_loss:1.4009 val_acc:84.5622 val_loss:0.4355
[02/0070] | train_loss:1.3428 val_acc:84.1014 val_loss:0.4263
[02/0071] | train_loss:1.277 val_acc:84.5622 val_loss:0.4721
[02/0072] | train_loss:1.2703 val_acc:83.4101 val_loss:0.4492
[02/0073] | train_loss:1.551 val_acc:80.1843 val_loss:0.4959
[02/0074] | train_loss:1.4721 val_acc:84.3318 val_loss:0.4032
[02/0075] | train_loss:1.3282 val_acc:84.1014 val_loss:0.4556
[02/0076] | train_loss:1.3294 val_acc:82.9493 val_loss:0.4843
[02/0077] | train_loss:1.3072 val_acc:83.1797 val_loss:0.486
[02/0078] | train_loss:1.3161 val_acc:84.3318 val_loss:0.4414
[02/0079] | train_loss:1.2899 val_acc:83.871 val_loss:0.4893
[02/0080] | train_loss:1.2898 val_acc:85.023 val_loss:0.5264
[02/0081] | train_loss:1.2295 val_acc:84.5622 val_loss:0.4875
[02/0082] | train_loss:1.2319 val_acc:85.2535 val_loss:0.443
[02/0083] | train_loss:1.1304 val_acc:84.3318 val_loss:0.4901
[02/0084] | train_loss:1.1576 val_acc:82.9493 val_loss:0.5223
[02/0085] | train_loss:1.2859 val_acc:84.7926 val_loss:0.4923
[02/0086] | train_loss:1.277 val_acc:83.4101 val_loss:0.4684
[02/0087] | train_loss:1.2295 val_acc:85.023 val_loss:0.5137
[02/0088] | train_loss:1.2745 val_acc:84.1014 val_loss:0.4536
[02/0089] | train_loss:1.1643 val_acc:84.3318 val_loss:0.4659
[02/0090] | train_loss:1.1063 val_acc:84.7926 val_loss:0.5181
[02/0091] | train_loss:1.1386 val_acc:85.2535 val_loss:0.501
[02/0092] | train_loss:1.2033 val_acc:82.4885 val_loss:0.4964
[02/0093] | train_loss:1.1987 val_acc:84.7926 val_loss:0.4591
[02/0094] | train_loss:1.2115 val_acc:84.7926 val_loss:0.4416
[02/0095] | train_loss:1.1109 val_acc:82.7189 val_loss:0.493
[02/0096] | train_loss:1.1194 val_acc:84.7926 val_loss:0.4831
Fold: [2/10] Test is finish !! 
 Test Metrics are: test_acc:83.6406 test_loss:0.3963fold [2/10] is start!!
[03/0001] | train_loss:2.5258 val_acc:44.2396 val_loss:0.7016
model is saved at epoch 1!![03/0002] | train_loss:2.2921 val_acc:69.1244 val_loss:0.639
model is saved at epoch 2!![03/0003] | train_loss:2.2288 val_acc:71.1982 val_loss:0.5748
model is saved at epoch 3!![03/0004] | train_loss:2.1635 val_acc:74.1935 val_loss:0.5334
model is saved at epoch 4!![03/0005] | train_loss:2.1584 val_acc:74.424 val_loss:0.5203
model is saved at epoch 5!![03/0006] | train_loss:2.0958 val_acc:74.6544 val_loss:0.4942
model is saved at epoch 6!![03/0007] | train_loss:2.0185 val_acc:75.576 val_loss:0.4859
model is saved at epoch 7!![03/0008] | train_loss:2.0143 val_acc:76.7281 val_loss:0.4916
model is saved at epoch 8!![03/0009] | train_loss:1.9745 val_acc:76.9585 val_loss:0.4787
model is saved at epoch 9!![03/0010] | train_loss:1.9836 val_acc:68.2028 val_loss:0.6053
[03/0011] | train_loss:1.9154 val_acc:77.6498 val_loss:0.4779
model is saved at epoch 11!![03/0012] | train_loss:1.9024 val_acc:78.5714 val_loss:0.4489
model is saved at epoch 12!![03/0013] | train_loss:1.871 val_acc:75.576 val_loss:0.4815
[03/0014] | train_loss:1.8522 val_acc:76.2673 val_loss:0.4533
[03/0015] | train_loss:1.8312 val_acc:75.576 val_loss:0.4904
[03/0016] | train_loss:1.8511 val_acc:79.2627 val_loss:0.4383
model is saved at epoch 16!![03/0017] | train_loss:1.8378 val_acc:78.8018 val_loss:0.4596
[03/0018] | train_loss:1.8721 val_acc:78.5714 val_loss:0.4347
[03/0019] | train_loss:1.8169 val_acc:73.5023 val_loss:0.503
[03/0020] | train_loss:1.7765 val_acc:78.5714 val_loss:0.4471
[03/0021] | train_loss:1.7421 val_acc:79.0323 val_loss:0.4347
[03/0022] | train_loss:1.712 val_acc:79.4931 val_loss:0.4338
model is saved at epoch 22!![03/0023] | train_loss:1.777 val_acc:80.4147 val_loss:0.4297
model is saved at epoch 23!![03/0024] | train_loss:1.7529 val_acc:80.8756 val_loss:0.4188
model is saved at epoch 24!![03/0025] | train_loss:1.6963 val_acc:79.2627 val_loss:0.4249
[03/0026] | train_loss:1.6439 val_acc:81.5668 val_loss:0.4203
model is saved at epoch 26!![03/0027] | train_loss:1.6783 val_acc:80.8756 val_loss:0.4236
[03/0028] | train_loss:1.7693 val_acc:77.8802 val_loss:0.4627
[03/0029] | train_loss:1.6998 val_acc:79.4931 val_loss:0.454
[03/0030] | train_loss:1.6932 val_acc:72.3502 val_loss:0.5258
[03/0031] | train_loss:1.7456 val_acc:82.0276 val_loss:0.4249
model is saved at epoch 31!![03/0032] | train_loss:1.7449 val_acc:80.8756 val_loss:0.4351
[03/0033] | train_loss:1.6979 val_acc:79.7235 val_loss:0.4206
[03/0034] | train_loss:1.6137 val_acc:81.106 val_loss:0.4019
[03/0035] | train_loss:1.6492 val_acc:82.9493 val_loss:0.4152
model is saved at epoch 35!![03/0036] | train_loss:1.6167 val_acc:82.2581 val_loss:0.3983
[03/0037] | train_loss:1.5543 val_acc:80.8756 val_loss:0.4237
[03/0038] | train_loss:1.6278 val_acc:80.6452 val_loss:0.4087
[03/0039] | train_loss:1.6127 val_acc:79.7235 val_loss:0.4436
[03/0040] | train_loss:1.5529 val_acc:80.1843 val_loss:0.4223
[03/0041] | train_loss:1.611 val_acc:81.7972 val_loss:0.4062
[03/0042] | train_loss:1.6609 val_acc:82.9493 val_loss:0.4216
[03/0043] | train_loss:1.5829 val_acc:81.7972 val_loss:0.4459
[03/0044] | train_loss:1.5278 val_acc:81.106 val_loss:0.3987
[03/0045] | train_loss:1.5628 val_acc:82.2581 val_loss:0.4081
[03/0046] | train_loss:1.5711 val_acc:80.8756 val_loss:0.4071
[03/0047] | train_loss:1.5882 val_acc:82.7189 val_loss:0.4165
[03/0048] | train_loss:1.5426 val_acc:81.5668 val_loss:0.4211
[03/0049] | train_loss:1.5059 val_acc:81.5668 val_loss:0.4238
[03/0050] | train_loss:1.4891 val_acc:82.2581 val_loss:0.4197
[03/0051] | train_loss:1.5371 val_acc:81.5668 val_loss:0.4097
[03/0052] | train_loss:1.5014 val_acc:80.8756 val_loss:0.4321
[03/0053] | train_loss:1.4486 val_acc:82.2581 val_loss:0.4039
[03/0054] | train_loss:1.4349 val_acc:80.1843 val_loss:0.4314
[03/0055] | train_loss:1.4863 val_acc:79.9539 val_loss:0.4088
[03/0056] | train_loss:1.463 val_acc:81.3364 val_loss:0.4321
[03/0057] | train_loss:1.4608 val_acc:77.8802 val_loss:0.4525
[03/0058] | train_loss:1.4655 val_acc:81.5668 val_loss:0.4247
[03/0059] | train_loss:1.5057 val_acc:82.9493 val_loss:0.4022
[03/0060] | train_loss:1.4551 val_acc:81.3364 val_loss:0.3902
[03/0061] | train_loss:1.3936 val_acc:81.106 val_loss:0.4054
[03/0062] | train_loss:1.3958 val_acc:82.7189 val_loss:0.4031
[03/0063] | train_loss:1.4186 val_acc:84.7926 val_loss:0.4179
model is saved at epoch 63!![03/0064] | train_loss:1.412 val_acc:82.9493 val_loss:0.4041
[03/0065] | train_loss:1.4359 val_acc:82.2581 val_loss:0.439
[03/0066] | train_loss:1.4363 val_acc:79.9539 val_loss:0.442
[03/0067] | train_loss:1.507 val_acc:82.4885 val_loss:0.419
[03/0068] | train_loss:1.4834 val_acc:84.5622 val_loss:0.3908
[03/0069] | train_loss:1.4178 val_acc:82.4885 val_loss:0.4231
[03/0070] | train_loss:1.3539 val_acc:81.7972 val_loss:0.4305
[03/0071] | train_loss:1.348 val_acc:83.871 val_loss:0.4359
[03/0072] | train_loss:1.361 val_acc:81.7972 val_loss:0.4512
[03/0073] | train_loss:1.3142 val_acc:83.871 val_loss:0.428
[03/0074] | train_loss:1.3581 val_acc:80.8756 val_loss:0.443
[03/0075] | train_loss:1.3836 val_acc:81.5668 val_loss:0.4013
[03/0076] | train_loss:1.3597 val_acc:82.4885 val_loss:0.4166
[03/0077] | train_loss:1.3487 val_acc:81.106 val_loss:0.436
[03/0078] | train_loss:1.4281 val_acc:82.0276 val_loss:0.4025
[03/0079] | train_loss:1.4728 val_acc:81.106 val_loss:0.4525
[03/0080] | train_loss:1.4173 val_acc:83.871 val_loss:0.4149
[03/0081] | train_loss:1.3711 val_acc:82.2581 val_loss:0.4269
[03/0082] | train_loss:1.3895 val_acc:83.1797 val_loss:0.4231
[03/0083] | train_loss:1.4162 val_acc:81.5668 val_loss:0.4498
[03/0084] | train_loss:1.3804 val_acc:82.9493 val_loss:0.4303
[03/0085] | train_loss:1.3996 val_acc:83.1797 val_loss:0.4072
[03/0086] | train_loss:1.2899 val_acc:83.6406 val_loss:0.4245
[03/0087] | train_loss:1.2598 val_acc:82.2581 val_loss:0.4423
[03/0088] | train_loss:1.2647 val_acc:83.1797 val_loss:0.4391
[03/0089] | train_loss:1.2566 val_acc:82.9493 val_loss:0.4346
[03/0090] | train_loss:1.1962 val_acc:81.5668 val_loss:0.44
[03/0091] | train_loss:1.2848 val_acc:82.4885 val_loss:0.4177
[03/0092] | train_loss:1.3102 val_acc:83.1797 val_loss:0.4332
[03/0093] | train_loss:1.3316 val_acc:82.2581 val_loss:0.4766
[03/0094] | train_loss:1.2597 val_acc:80.4147 val_loss:0.4602
[03/0095] | train_loss:1.2569 val_acc:84.5622 val_loss:0.4505
[03/0096] | train_loss:1.278 val_acc:82.0276 val_loss:0.421
[03/0097] | train_loss:1.2516 val_acc:81.3364 val_loss:0.4996
[03/0098] | train_loss:1.256 val_acc:81.3364 val_loss:0.4399
[03/0099] | train_loss:1.1921 val_acc:81.7972 val_loss:0.4519
[03/0100] | train_loss:1.155 val_acc:82.0276 val_loss:0.4785
[03/0101] | train_loss:1.2553 val_acc:81.5668 val_loss:0.4736
[03/0102] | train_loss:1.4363 val_acc:81.5668 val_loss:0.4446
[03/0103] | train_loss:1.4478 val_acc:77.6498 val_loss:0.5007
[03/0104] | train_loss:1.485 val_acc:81.3364 val_loss:0.4258
[03/0105] | train_loss:1.322 val_acc:82.0276 val_loss:0.4211
[03/0106] | train_loss:1.3381 val_acc:83.1797 val_loss:0.4367
[03/0107] | train_loss:1.2707 val_acc:81.5668 val_loss:0.4795
[03/0108] | train_loss:1.3075 val_acc:83.6406 val_loss:0.416
[03/0109] | train_loss:1.2735 val_acc:83.4101 val_loss:0.453
[03/0110] | train_loss:1.232 val_acc:82.2581 val_loss:0.4665
[03/0111] | train_loss:1.1988 val_acc:84.1014 val_loss:0.4342
[03/0112] | train_loss:1.1217 val_acc:82.2581 val_loss:0.4865
[03/0113] | train_loss:1.1477 val_acc:81.5668 val_loss:0.4838
[03/0114] | train_loss:1.1013 val_acc:82.4885 val_loss:0.4829
Fold: [3/10] Test is finish !! 
 Test Metrics are: test_acc:84.5622 test_loss:0.3612fold [3/10] is start!!
[04/0001] | train_loss:2.5524 val_acc:44.7005 val_loss:0.6988
model is saved at epoch 1!![04/0002] | train_loss:2.32 val_acc:58.2949 val_loss:0.6685
model is saved at epoch 2!![04/0003] | train_loss:2.2124 val_acc:69.5853 val_loss:0.5531
model is saved at epoch 3!![04/0004] | train_loss:2.1838 val_acc:73.7327 val_loss:0.5215
model is saved at epoch 4!![04/0005] | train_loss:2.1732 val_acc:73.5023 val_loss:0.4933
[04/0006] | train_loss:2.0829 val_acc:73.9631 val_loss:0.4851
model is saved at epoch 6!![04/0007] | train_loss:2.0328 val_acc:77.1889 val_loss:0.4705
model is saved at epoch 7!![04/0008] | train_loss:2.0532 val_acc:76.0369 val_loss:0.4887
[04/0009] | train_loss:2.0419 val_acc:77.1889 val_loss:0.4701
[04/0010] | train_loss:1.9368 val_acc:76.9585 val_loss:0.4723
[04/0011] | train_loss:1.955 val_acc:76.2673 val_loss:0.4751
[04/0012] | train_loss:1.9275 val_acc:73.0415 val_loss:0.5184
[04/0013] | train_loss:1.9151 val_acc:79.9539 val_loss:0.4232
model is saved at epoch 13!![04/0014] | train_loss:1.8805 val_acc:80.6452 val_loss:0.4486
model is saved at epoch 14!![04/0015] | train_loss:1.8346 val_acc:77.4194 val_loss:0.4467
[04/0016] | train_loss:1.8885 val_acc:80.8756 val_loss:0.4387
model is saved at epoch 16!![04/0017] | train_loss:1.8502 val_acc:80.4147 val_loss:0.4195
[04/0018] | train_loss:1.8299 val_acc:80.6452 val_loss:0.4329
[04/0019] | train_loss:1.7874 val_acc:81.106 val_loss:0.4246
model is saved at epoch 19!![04/0020] | train_loss:1.7825 val_acc:80.8756 val_loss:0.4252
[04/0021] | train_loss:1.7744 val_acc:81.3364 val_loss:0.4097
model is saved at epoch 21!![04/0022] | train_loss:1.7404 val_acc:82.0276 val_loss:0.4008
model is saved at epoch 22!![04/0023] | train_loss:1.7755 val_acc:81.3364 val_loss:0.4249
[04/0024] | train_loss:1.7639 val_acc:82.2581 val_loss:0.4001
model is saved at epoch 24!![04/0025] | train_loss:1.7059 val_acc:80.1843 val_loss:0.4071
[04/0026] | train_loss:1.7042 val_acc:82.4885 val_loss:0.4016
model is saved at epoch 26!![04/0027] | train_loss:1.7093 val_acc:82.7189 val_loss:0.3916
model is saved at epoch 27!![04/0028] | train_loss:1.6427 val_acc:80.4147 val_loss:0.3987
[04/0029] | train_loss:1.6262 val_acc:81.3364 val_loss:0.4074
[04/0030] | train_loss:1.671 val_acc:80.8756 val_loss:0.389
[04/0031] | train_loss:1.6355 val_acc:84.1014 val_loss:0.404
model is saved at epoch 31!![04/0032] | train_loss:1.7454 val_acc:80.4147 val_loss:0.4328
[04/0033] | train_loss:1.5787 val_acc:81.7972 val_loss:0.3982
[04/0034] | train_loss:1.6716 val_acc:81.7972 val_loss:0.4084
[04/0035] | train_loss:1.6147 val_acc:83.1797 val_loss:0.4063
[04/0036] | train_loss:1.5666 val_acc:84.7926 val_loss:0.373
model is saved at epoch 36!![04/0037] | train_loss:1.5381 val_acc:83.1797 val_loss:0.3955
[04/0038] | train_loss:1.5187 val_acc:82.4885 val_loss:0.4039
[04/0039] | train_loss:1.5424 val_acc:83.871 val_loss:0.3802
[04/0040] | train_loss:1.4965 val_acc:82.9493 val_loss:0.4036
[04/0041] | train_loss:1.52 val_acc:81.5668 val_loss:0.4025
[04/0042] | train_loss:1.5628 val_acc:82.9493 val_loss:0.4208
[04/0043] | train_loss:1.5243 val_acc:82.7189 val_loss:0.3991
[04/0044] | train_loss:1.5555 val_acc:84.3318 val_loss:0.3967
[04/0045] | train_loss:1.5063 val_acc:82.0276 val_loss:0.425
[04/0046] | train_loss:1.4889 val_acc:81.7972 val_loss:0.4145
[04/0047] | train_loss:1.4423 val_acc:83.871 val_loss:0.3982
[04/0048] | train_loss:1.421 val_acc:84.1014 val_loss:0.4067
[04/0049] | train_loss:1.3583 val_acc:84.1014 val_loss:0.4083
[04/0050] | train_loss:1.3493 val_acc:82.9493 val_loss:0.4333
[04/0051] | train_loss:1.3687 val_acc:82.4885 val_loss:0.4302
[04/0052] | train_loss:1.3996 val_acc:81.7972 val_loss:0.4506
[04/0053] | train_loss:1.3671 val_acc:82.2581 val_loss:0.4081
[04/0054] | train_loss:1.3548 val_acc:80.8756 val_loss:0.5274
[04/0055] | train_loss:1.449 val_acc:83.1797 val_loss:0.4072
[04/0056] | train_loss:1.5098 val_acc:83.871 val_loss:0.404
[04/0057] | train_loss:1.4564 val_acc:82.2581 val_loss:0.4138
[04/0058] | train_loss:1.3746 val_acc:82.9493 val_loss:0.4245
[04/0059] | train_loss:1.364 val_acc:81.5668 val_loss:0.4568
[04/0060] | train_loss:1.3363 val_acc:80.6452 val_loss:0.4691
[04/0061] | train_loss:1.3046 val_acc:81.3364 val_loss:0.4285
[04/0062] | train_loss:1.2248 val_acc:82.9493 val_loss:0.4663
[04/0063] | train_loss:1.1966 val_acc:82.4885 val_loss:0.4549
[04/0064] | train_loss:1.32 val_acc:82.2581 val_loss:0.4426
[04/0065] | train_loss:1.2771 val_acc:83.4101 val_loss:0.4445
[04/0066] | train_loss:1.3276 val_acc:83.1797 val_loss:0.4368
[04/0067] | train_loss:1.215 val_acc:83.4101 val_loss:0.4183
[04/0068] | train_loss:1.2051 val_acc:82.4885 val_loss:0.5392
[04/0069] | train_loss:1.3571 val_acc:84.1014 val_loss:0.4345
[04/0070] | train_loss:1.2795 val_acc:80.6452 val_loss:0.5423
[04/0071] | train_loss:1.2364 val_acc:83.871 val_loss:0.4505
[04/0072] | train_loss:1.2288 val_acc:82.4885 val_loss:0.4778
[04/0073] | train_loss:1.1686 val_acc:81.7972 val_loss:0.4603
[04/0074] | train_loss:1.1862 val_acc:82.7189 val_loss:0.4253
[04/0075] | train_loss:1.2784 val_acc:83.4101 val_loss:0.4501
[04/0076] | train_loss:1.2653 val_acc:85.2535 val_loss:0.4943
model is saved at epoch 76!![04/0077] | train_loss:1.2825 val_acc:82.4885 val_loss:0.4945
[04/0078] | train_loss:1.2697 val_acc:82.7189 val_loss:0.4481
[04/0079] | train_loss:1.2173 val_acc:83.4101 val_loss:0.4699
[04/0080] | train_loss:1.2337 val_acc:82.7189 val_loss:0.452
[04/0081] | train_loss:1.223 val_acc:82.4885 val_loss:0.5021
[04/0082] | train_loss:1.2374 val_acc:83.6406 val_loss:0.4712
[04/0083] | train_loss:1.2046 val_acc:82.9493 val_loss:0.4803
[04/0084] | train_loss:1.1939 val_acc:83.6406 val_loss:0.4973
[04/0085] | train_loss:1.1946 val_acc:84.1014 val_loss:0.4816
[04/0086] | train_loss:1.1537 val_acc:83.6406 val_loss:0.4978
[04/0087] | train_loss:1.1449 val_acc:84.1014 val_loss:0.5434
[04/0088] | train_loss:1.1391 val_acc:84.3318 val_loss:0.599
[04/0089] | train_loss:1.0957 val_acc:83.871 val_loss:0.531
[04/0090] | train_loss:1.1603 val_acc:84.1014 val_loss:0.4666
[04/0091] | train_loss:1.2063 val_acc:82.0276 val_loss:0.5151
[04/0092] | train_loss:1.1713 val_acc:83.1797 val_loss:0.5581
[04/0093] | train_loss:1.2113 val_acc:82.7189 val_loss:0.4807
[04/0094] | train_loss:1.1164 val_acc:81.3364 val_loss:0.4625
[04/0095] | train_loss:1.161 val_acc:82.2581 val_loss:0.4863
[04/0096] | train_loss:1.1242 val_acc:84.1014 val_loss:0.4733
[04/0097] | train_loss:1.0766 val_acc:82.4885 val_loss:0.5391
[04/0098] | train_loss:1.1318 val_acc:81.106 val_loss:0.5364
[04/0099] | train_loss:1.0832 val_acc:84.7926 val_loss:0.6215
[04/0100] | train_loss:1.0409 val_acc:82.7189 val_loss:0.5276
[04/0101] | train_loss:1.044 val_acc:85.7143 val_loss:0.4948
model is saved at epoch 101!![04/0102] | train_loss:1.0819 val_acc:84.7926 val_loss:0.5218
[04/0103] | train_loss:1.0346 val_acc:83.4101 val_loss:0.5055
[04/0104] | train_loss:1.1328 val_acc:82.7189 val_loss:0.4704
[04/0105] | train_loss:1.1188 val_acc:83.6406 val_loss:0.4897
[04/0106] | train_loss:1.0559 val_acc:84.1014 val_loss:0.5804
[04/0107] | train_loss:1.1497 val_acc:82.4885 val_loss:0.5088
[04/0108] | train_loss:1.0341 val_acc:83.871 val_loss:0.5268
[04/0109] | train_loss:0.9708 val_acc:82.7189 val_loss:0.5776
[04/0110] | train_loss:1.017 val_acc:82.2581 val_loss:0.6169
[04/0111] | train_loss:1.051 val_acc:83.4101 val_loss:0.5939
[04/0112] | train_loss:0.9716 val_acc:81.3364 val_loss:0.6327
[04/0113] | train_loss:1.0638 val_acc:84.5622 val_loss:0.4814
[04/0114] | train_loss:1.0297 val_acc:84.5622 val_loss:0.5284
[04/0115] | train_loss:1.0066 val_acc:85.2535 val_loss:0.5645
[04/0116] | train_loss:0.9625 val_acc:84.1014 val_loss:0.5641
[04/0117] | train_loss:0.9775 val_acc:84.5622 val_loss:0.6065
[04/0118] | train_loss:0.9829 val_acc:82.7189 val_loss:0.5951
[04/0119] | train_loss:0.8751 val_acc:83.1797 val_loss:0.5576
[04/0120] | train_loss:0.9686 val_acc:82.4885 val_loss:0.5288
[04/0121] | train_loss:0.9564 val_acc:84.5622 val_loss:0.6237
[04/0122] | train_loss:0.9461 val_acc:83.4101 val_loss:0.5725
[04/0123] | train_loss:1.0041 val_acc:83.871 val_loss:0.5692
[04/0124] | train_loss:0.8982 val_acc:82.0276 val_loss:0.6121
[04/0125] | train_loss:0.9343 val_acc:82.9493 val_loss:0.6001
[04/0126] | train_loss:0.974 val_acc:85.023 val_loss:0.486
[04/0127] | train_loss:0.9236 val_acc:84.7926 val_loss:0.5388
[04/0128] | train_loss:0.8873 val_acc:84.7926 val_loss:0.5034
[04/0129] | train_loss:1.0461 val_acc:82.4885 val_loss:0.515
[04/0130] | train_loss:1.0014 val_acc:83.871 val_loss:0.4769
[04/0131] | train_loss:0.9337 val_acc:84.5622 val_loss:0.5093
[04/0132] | train_loss:1.1672 val_acc:83.871 val_loss:0.4869
[04/0133] | train_loss:1.0562 val_acc:83.871 val_loss:0.5247
[04/0134] | train_loss:0.9978 val_acc:82.0276 val_loss:0.5814
[04/0135] | train_loss:1.1554 val_acc:82.0276 val_loss:0.525
[04/0136] | train_loss:1.0887 val_acc:84.3318 val_loss:0.5429
[04/0137] | train_loss:1.1178 val_acc:82.4885 val_loss:0.5303
[04/0138] | train_loss:1.083 val_acc:83.1797 val_loss:0.579
[04/0139] | train_loss:1.1501 val_acc:85.2535 val_loss:0.4914
[04/0140] | train_loss:1.0051 val_acc:84.5622 val_loss:0.5392
[04/0141] | train_loss:0.966 val_acc:82.9493 val_loss:0.6069
[04/0142] | train_loss:0.8885 val_acc:82.7189 val_loss:0.641
[04/0143] | train_loss:0.9517 val_acc:82.2581 val_loss:0.5952
[04/0144] | train_loss:0.9641 val_acc:83.871 val_loss:0.4997
[04/0145] | train_loss:0.9555 val_acc:83.4101 val_loss:0.5389
[04/0146] | train_loss:0.8874 val_acc:83.1797 val_loss:0.5966
[04/0147] | train_loss:0.8296 val_acc:83.1797 val_loss:0.609
[04/0148] | train_loss:0.814 val_acc:84.5622 val_loss:0.555
[04/0149] | train_loss:0.8571 val_acc:85.7143 val_loss:0.6101
[04/0150] | train_loss:0.8539 val_acc:84.1014 val_loss:0.5595
[04/0151] | train_loss:0.8357 val_acc:82.9493 val_loss:0.5919
[04/0152] | train_loss:1.0019 val_acc:83.1797 val_loss:0.5286
Fold: [4/10] Test is finish !! 
 Test Metrics are: test_acc:79.2627 test_loss:0.5424fold [4/10] is start!!
[05/0001] | train_loss:2.4586 val_acc:43.7788 val_loss:0.707
model is saved at epoch 1!![05/0002] | train_loss:2.297 val_acc:60.8295 val_loss:0.6406
model is saved at epoch 2!![05/0003] | train_loss:2.1761 val_acc:73.0415 val_loss:0.5412
model is saved at epoch 3!![05/0004] | train_loss:2.1168 val_acc:78.5714 val_loss:0.4899
model is saved at epoch 4!![05/0005] | train_loss:2.1193 val_acc:75.8065 val_loss:0.5332
[05/0006] | train_loss:2.0714 val_acc:75.8065 val_loss:0.5146
[05/0007] | train_loss:2.0058 val_acc:76.9585 val_loss:0.4959
[05/0008] | train_loss:1.9942 val_acc:76.7281 val_loss:0.5252
[05/0009] | train_loss:1.9875 val_acc:79.0323 val_loss:0.4748
model is saved at epoch 9!![05/0010] | train_loss:1.9401 val_acc:76.0369 val_loss:0.5147
[05/0011] | train_loss:1.9856 val_acc:76.4977 val_loss:0.5048
[05/0012] | train_loss:1.9062 val_acc:76.9585 val_loss:0.5159
[05/0013] | train_loss:1.8531 val_acc:76.9585 val_loss:0.5436
[05/0014] | train_loss:1.8484 val_acc:78.341 val_loss:0.4972
[05/0015] | train_loss:1.8445 val_acc:79.2627 val_loss:0.4869
model is saved at epoch 15!![05/0016] | train_loss:1.9124 val_acc:78.5714 val_loss:0.501
[05/0017] | train_loss:1.9041 val_acc:79.0323 val_loss:0.4664
[05/0018] | train_loss:1.8342 val_acc:79.2627 val_loss:0.4844
[05/0019] | train_loss:1.7936 val_acc:77.6498 val_loss:0.4916
[05/0020] | train_loss:1.8039 val_acc:79.2627 val_loss:0.4656
[05/0021] | train_loss:1.7546 val_acc:78.341 val_loss:0.48
[05/0022] | train_loss:1.7278 val_acc:80.8756 val_loss:0.4864
model is saved at epoch 22!![05/0023] | train_loss:1.6983 val_acc:79.4931 val_loss:0.4984
[05/0024] | train_loss:1.7226 val_acc:78.1106 val_loss:0.4879
[05/0025] | train_loss:1.6899 val_acc:79.2627 val_loss:0.4804
[05/0026] | train_loss:1.6897 val_acc:77.4194 val_loss:0.5057
[05/0027] | train_loss:1.6836 val_acc:79.4931 val_loss:0.4929
[05/0028] | train_loss:1.6795 val_acc:79.9539 val_loss:0.4966
[05/0029] | train_loss:1.7429 val_acc:81.106 val_loss:0.4802
model is saved at epoch 29!![05/0030] | train_loss:1.7174 val_acc:78.5714 val_loss:0.4843
[05/0031] | train_loss:1.6456 val_acc:79.9539 val_loss:0.4794
[05/0032] | train_loss:1.6316 val_acc:80.4147 val_loss:0.4662
[05/0033] | train_loss:1.6258 val_acc:79.0323 val_loss:0.4915
[05/0034] | train_loss:1.5846 val_acc:81.3364 val_loss:0.5005
model is saved at epoch 34!![05/0035] | train_loss:1.6031 val_acc:79.4931 val_loss:0.4617
[05/0036] | train_loss:1.6533 val_acc:80.6452 val_loss:0.5381
[05/0037] | train_loss:1.6123 val_acc:80.8756 val_loss:0.4922
[05/0038] | train_loss:1.6709 val_acc:80.4147 val_loss:0.4739
[05/0039] | train_loss:1.6125 val_acc:78.1106 val_loss:0.4903
[05/0040] | train_loss:1.5832 val_acc:80.8756 val_loss:0.4689
[05/0041] | train_loss:1.5355 val_acc:80.8756 val_loss:0.516
[05/0042] | train_loss:1.526 val_acc:79.4931 val_loss:0.5096
[05/0043] | train_loss:1.5563 val_acc:80.1843 val_loss:0.5153
[05/0044] | train_loss:1.561 val_acc:78.8018 val_loss:0.4872
[05/0045] | train_loss:1.5726 val_acc:79.9539 val_loss:0.4865
[05/0046] | train_loss:1.5725 val_acc:80.4147 val_loss:0.5013
[05/0047] | train_loss:1.4915 val_acc:79.2627 val_loss:0.5518
[05/0048] | train_loss:1.5193 val_acc:77.8802 val_loss:0.5167
[05/0049] | train_loss:1.5064 val_acc:79.9539 val_loss:0.5557
[05/0050] | train_loss:1.5158 val_acc:79.7235 val_loss:0.5365
[05/0051] | train_loss:1.4147 val_acc:81.106 val_loss:0.4755
[05/0052] | train_loss:1.4645 val_acc:79.2627 val_loss:0.4886
[05/0053] | train_loss:1.398 val_acc:79.4931 val_loss:0.5182
[05/0054] | train_loss:1.3913 val_acc:80.4147 val_loss:0.5401
[05/0055] | train_loss:1.4039 val_acc:79.4931 val_loss:0.526
[05/0056] | train_loss:1.3929 val_acc:80.8756 val_loss:0.5339
[05/0057] | train_loss:1.3701 val_acc:82.2581 val_loss:0.5147
model is saved at epoch 57!![05/0058] | train_loss:1.3321 val_acc:82.0276 val_loss:0.4966
[05/0059] | train_loss:1.2907 val_acc:80.6452 val_loss:0.5882
[05/0060] | train_loss:1.3505 val_acc:79.9539 val_loss:0.5583
[05/0061] | train_loss:1.3441 val_acc:80.8756 val_loss:0.5437
[05/0062] | train_loss:1.3202 val_acc:81.5668 val_loss:0.5539
[05/0063] | train_loss:1.3039 val_acc:81.3364 val_loss:0.4962
[05/0064] | train_loss:1.368 val_acc:81.5668 val_loss:0.5204
[05/0065] | train_loss:1.4078 val_acc:79.4931 val_loss:0.5012
[05/0066] | train_loss:1.31 val_acc:78.8018 val_loss:0.5531
[05/0067] | train_loss:1.2782 val_acc:79.0323 val_loss:0.5278
[05/0068] | train_loss:1.3579 val_acc:81.106 val_loss:0.5265
[05/0069] | train_loss:1.2913 val_acc:80.6452 val_loss:0.515
[05/0070] | train_loss:1.2516 val_acc:80.1843 val_loss:0.5336
[05/0071] | train_loss:1.292 val_acc:81.5668 val_loss:0.556
[05/0072] | train_loss:1.2514 val_acc:81.7972 val_loss:0.5746
[05/0073] | train_loss:1.342 val_acc:80.6452 val_loss:0.6036
[05/0074] | train_loss:1.2722 val_acc:80.4147 val_loss:0.5683
[05/0075] | train_loss:1.2254 val_acc:78.1106 val_loss:0.5652
[05/0076] | train_loss:1.3331 val_acc:79.9539 val_loss:0.5937
[05/0077] | train_loss:1.2598 val_acc:81.106 val_loss:0.5737
[05/0078] | train_loss:1.267 val_acc:80.6452 val_loss:0.5909
[05/0079] | train_loss:1.2586 val_acc:80.4147 val_loss:0.5518
[05/0080] | train_loss:1.2433 val_acc:79.9539 val_loss:0.6047
[05/0081] | train_loss:1.2411 val_acc:79.7235 val_loss:0.5198
[05/0082] | train_loss:1.2956 val_acc:80.8756 val_loss:0.5826
[05/0083] | train_loss:1.2624 val_acc:80.8756 val_loss:0.5067
[05/0084] | train_loss:1.2179 val_acc:80.6452 val_loss:0.5675
[05/0085] | train_loss:1.243 val_acc:82.4885 val_loss:0.5379
model is saved at epoch 85!![05/0086] | train_loss:1.2339 val_acc:79.4931 val_loss:0.5818
[05/0087] | train_loss:1.2019 val_acc:80.1843 val_loss:0.5622
[05/0088] | train_loss:1.1744 val_acc:79.9539 val_loss:0.592
[05/0089] | train_loss:1.1754 val_acc:80.1843 val_loss:0.5545
[05/0090] | train_loss:1.2084 val_acc:80.8756 val_loss:0.5581
[05/0091] | train_loss:1.1959 val_acc:81.106 val_loss:0.5468
[05/0092] | train_loss:1.2097 val_acc:80.4147 val_loss:0.5661
[05/0093] | train_loss:1.1825 val_acc:82.0276 val_loss:0.539
[05/0094] | train_loss:1.1303 val_acc:81.3364 val_loss:0.6198
[05/0095] | train_loss:1.2153 val_acc:80.8756 val_loss:0.6129
[05/0096] | train_loss:1.1167 val_acc:79.0323 val_loss:0.6685
[05/0097] | train_loss:1.0781 val_acc:80.4147 val_loss:0.6512
[05/0098] | train_loss:1.0631 val_acc:79.7235 val_loss:0.6691
[05/0099] | train_loss:1.1388 val_acc:80.1843 val_loss:0.6391
[05/0100] | train_loss:1.1066 val_acc:80.1843 val_loss:0.6954
[05/0101] | train_loss:1.2122 val_acc:80.6452 val_loss:0.5273
[05/0102] | train_loss:1.3701 val_acc:80.6452 val_loss:0.4805
[05/0103] | train_loss:1.2734 val_acc:80.8756 val_loss:0.4613
[05/0104] | train_loss:1.1604 val_acc:81.3364 val_loss:0.5849
[05/0105] | train_loss:1.132 val_acc:79.9539 val_loss:0.5598
[05/0106] | train_loss:1.0784 val_acc:80.1843 val_loss:0.5546
[05/0107] | train_loss:1.1131 val_acc:79.7235 val_loss:0.5921
[05/0108] | train_loss:1.1595 val_acc:80.4147 val_loss:0.5541
[05/0109] | train_loss:1.2144 val_acc:78.341 val_loss:0.5701
[05/0110] | train_loss:1.1121 val_acc:80.1843 val_loss:0.576
[05/0111] | train_loss:1.1745 val_acc:81.106 val_loss:0.682
[05/0112] | train_loss:1.2162 val_acc:80.4147 val_loss:0.57
[05/0113] | train_loss:1.1799 val_acc:80.8756 val_loss:0.5692
[05/0114] | train_loss:1.1159 val_acc:80.8756 val_loss:0.6131
[05/0115] | train_loss:1.1344 val_acc:80.8756 val_loss:0.657
[05/0116] | train_loss:1.096 val_acc:82.2581 val_loss:0.6921
[05/0117] | train_loss:1.0828 val_acc:80.4147 val_loss:0.6017
[05/0118] | train_loss:1.0731 val_acc:79.2627 val_loss:0.7284
[05/0119] | train_loss:1.1255 val_acc:79.9539 val_loss:0.6149
[05/0120] | train_loss:1.0986 val_acc:81.7972 val_loss:0.6034
[05/0121] | train_loss:1.1289 val_acc:80.6452 val_loss:0.6559
[05/0122] | train_loss:1.1077 val_acc:79.9539 val_loss:0.6745
[05/0123] | train_loss:1.0696 val_acc:80.1843 val_loss:0.8023
[05/0124] | train_loss:1.0596 val_acc:80.8756 val_loss:0.6477
[05/0125] | train_loss:1.2124 val_acc:79.9539 val_loss:0.5593
[05/0126] | train_loss:1.1912 val_acc:80.8756 val_loss:0.6706
[05/0127] | train_loss:1.1296 val_acc:81.5668 val_loss:0.611
[05/0128] | train_loss:1.1714 val_acc:82.2581 val_loss:0.6648
[05/0129] | train_loss:1.1187 val_acc:79.9539 val_loss:0.6697
[05/0130] | train_loss:1.0685 val_acc:80.8756 val_loss:0.6553
[05/0131] | train_loss:1.0951 val_acc:79.0323 val_loss:0.6859
[05/0132] | train_loss:1.0064 val_acc:80.6452 val_loss:0.7087
[05/0133] | train_loss:1.0339 val_acc:79.7235 val_loss:0.6746
[05/0134] | train_loss:1.0417 val_acc:78.5714 val_loss:0.7017
[05/0135] | train_loss:1.0159 val_acc:80.4147 val_loss:0.7124
[05/0136] | train_loss:1.1459 val_acc:78.5714 val_loss:0.7367
Fold: [5/10] Test is finish !! 
 Test Metrics are: test_acc:83.4101 test_loss:0.4518fold [5/10] is start!!
[06/0001] | train_loss:2.6275 val_acc:51.1521 val_loss:0.6909
model is saved at epoch 1!![06/0002] | train_loss:2.3622 val_acc:52.3041 val_loss:0.7101
model is saved at epoch 2!![06/0003] | train_loss:2.264 val_acc:75.576 val_loss:0.549
model is saved at epoch 3!![06/0004] | train_loss:2.2612 val_acc:79.0323 val_loss:0.5074
model is saved at epoch 4!![06/0005] | train_loss:2.2247 val_acc:79.4931 val_loss:0.5212
model is saved at epoch 5!![06/0006] | train_loss:2.2212 val_acc:73.9631 val_loss:0.5155
[06/0007] | train_loss:2.1168 val_acc:77.1889 val_loss:0.4876
[06/0008] | train_loss:2.0943 val_acc:78.8018 val_loss:0.4814
[06/0009] | train_loss:2.0811 val_acc:75.3456 val_loss:0.5244
[06/0010] | train_loss:2.0767 val_acc:78.341 val_loss:0.4563
[06/0011] | train_loss:2.0106 val_acc:78.341 val_loss:0.4622
[06/0012] | train_loss:2.0844 val_acc:78.8018 val_loss:0.4699
[06/0013] | train_loss:2.0042 val_acc:79.2627 val_loss:0.4454
[06/0014] | train_loss:1.9716 val_acc:79.0323 val_loss:0.4726
[06/0015] | train_loss:1.9134 val_acc:78.8018 val_loss:0.4534
[06/0016] | train_loss:1.907 val_acc:79.4931 val_loss:0.4455
[06/0017] | train_loss:1.8422 val_acc:77.6498 val_loss:0.4419
[06/0018] | train_loss:1.8878 val_acc:79.9539 val_loss:0.4508
model is saved at epoch 18!![06/0019] | train_loss:1.9592 val_acc:76.2673 val_loss:0.4927
[06/0020] | train_loss:1.8549 val_acc:82.4885 val_loss:0.4319
model is saved at epoch 20!![06/0021] | train_loss:1.8228 val_acc:80.8756 val_loss:0.4209
[06/0022] | train_loss:1.8411 val_acc:79.0323 val_loss:0.4313
[06/0023] | train_loss:1.8124 val_acc:79.9539 val_loss:0.442
[06/0024] | train_loss:1.7342 val_acc:79.9539 val_loss:0.4351
[06/0025] | train_loss:1.7208 val_acc:78.8018 val_loss:0.4471
[06/0026] | train_loss:1.805 val_acc:77.4194 val_loss:0.4781
[06/0027] | train_loss:1.7379 val_acc:81.106 val_loss:0.4154
[06/0028] | train_loss:1.77 val_acc:79.9539 val_loss:0.4421
[06/0029] | train_loss:1.81 val_acc:79.4931 val_loss:0.4475
[06/0030] | train_loss:1.8487 val_acc:80.1843 val_loss:0.4195
[06/0031] | train_loss:1.794 val_acc:77.6498 val_loss:0.484
[06/0032] | train_loss:1.7486 val_acc:80.6452 val_loss:0.4396
[06/0033] | train_loss:1.6853 val_acc:81.3364 val_loss:0.4224
[06/0034] | train_loss:1.7003 val_acc:77.8802 val_loss:0.4636
[06/0035] | train_loss:1.6788 val_acc:80.1843 val_loss:0.4153
[06/0036] | train_loss:1.6183 val_acc:79.9539 val_loss:0.4462
[06/0037] | train_loss:1.6296 val_acc:80.8756 val_loss:0.4278
[06/0038] | train_loss:1.6305 val_acc:84.1014 val_loss:0.3812
model is saved at epoch 38!![06/0039] | train_loss:1.6628 val_acc:81.5668 val_loss:0.4496
[06/0040] | train_loss:1.694 val_acc:78.8018 val_loss:0.5085
[06/0041] | train_loss:1.6582 val_acc:82.0276 val_loss:0.4329
[06/0042] | train_loss:1.5804 val_acc:81.3364 val_loss:0.4404
[06/0043] | train_loss:1.5838 val_acc:80.8756 val_loss:0.4242
[06/0044] | train_loss:1.5328 val_acc:81.5668 val_loss:0.4793
[06/0045] | train_loss:1.5712 val_acc:80.4147 val_loss:0.475
[06/0046] | train_loss:1.5122 val_acc:83.6406 val_loss:0.4255
[06/0047] | train_loss:1.5183 val_acc:80.8756 val_loss:0.4032
[06/0048] | train_loss:1.5106 val_acc:83.871 val_loss:0.4387
[06/0049] | train_loss:1.5055 val_acc:82.0276 val_loss:0.4793
[06/0050] | train_loss:1.5619 val_acc:81.3364 val_loss:0.4465
[06/0051] | train_loss:1.5255 val_acc:82.9493 val_loss:0.421
[06/0052] | train_loss:1.5037 val_acc:81.106 val_loss:0.4397
[06/0053] | train_loss:1.5869 val_acc:80.8756 val_loss:0.4326
[06/0054] | train_loss:1.4913 val_acc:82.2581 val_loss:0.4193
[06/0055] | train_loss:1.5117 val_acc:82.7189 val_loss:0.4327
[06/0056] | train_loss:1.4978 val_acc:79.7235 val_loss:0.4835
[06/0057] | train_loss:1.5347 val_acc:80.8756 val_loss:0.4216
[06/0058] | train_loss:1.4373 val_acc:81.106 val_loss:0.4418
[06/0059] | train_loss:1.4604 val_acc:82.2581 val_loss:0.4312
[06/0060] | train_loss:1.4181 val_acc:82.2581 val_loss:0.4638
[06/0061] | train_loss:1.4974 val_acc:82.4885 val_loss:0.4065
[06/0062] | train_loss:1.4342 val_acc:82.0276 val_loss:0.4128
[06/0063] | train_loss:1.4272 val_acc:81.5668 val_loss:0.4257
[06/0064] | train_loss:1.4762 val_acc:81.7972 val_loss:0.4121
[06/0065] | train_loss:1.4734 val_acc:82.0276 val_loss:0.5212
[06/0066] | train_loss:1.4421 val_acc:81.5668 val_loss:0.4164
[06/0067] | train_loss:1.4476 val_acc:80.8756 val_loss:0.5123
[06/0068] | train_loss:1.4424 val_acc:79.7235 val_loss:0.462
[06/0069] | train_loss:1.3811 val_acc:82.4885 val_loss:0.4553
[06/0070] | train_loss:1.3654 val_acc:83.6406 val_loss:0.4048
[06/0071] | train_loss:1.4773 val_acc:80.8756 val_loss:0.4674
[06/0072] | train_loss:1.4452 val_acc:82.0276 val_loss:0.4709
[06/0073] | train_loss:1.4061 val_acc:81.5668 val_loss:0.5284
[06/0074] | train_loss:1.4763 val_acc:80.1843 val_loss:0.4388
[06/0075] | train_loss:1.4912 val_acc:80.8756 val_loss:0.5217
[06/0076] | train_loss:1.4383 val_acc:83.4101 val_loss:0.4644
[06/0077] | train_loss:1.3717 val_acc:81.106 val_loss:0.5268
[06/0078] | train_loss:1.3336 val_acc:81.7972 val_loss:0.4982
[06/0079] | train_loss:1.4038 val_acc:83.1797 val_loss:0.5408
[06/0080] | train_loss:1.5064 val_acc:81.5668 val_loss:0.496
[06/0081] | train_loss:1.4465 val_acc:80.6452 val_loss:0.4587
[06/0082] | train_loss:1.4421 val_acc:79.9539 val_loss:0.5509
[06/0083] | train_loss:1.3708 val_acc:82.0276 val_loss:0.482
[06/0084] | train_loss:1.3579 val_acc:81.7972 val_loss:0.445
[06/0085] | train_loss:1.3369 val_acc:83.1797 val_loss:0.4805
[06/0086] | train_loss:1.3561 val_acc:82.9493 val_loss:0.4596
[06/0087] | train_loss:1.376 val_acc:83.4101 val_loss:0.4626
[06/0088] | train_loss:1.3573 val_acc:81.5668 val_loss:0.4811
[06/0089] | train_loss:1.4079 val_acc:82.4885 val_loss:0.4503
Fold: [6/10] Test is finish !! 
 Test Metrics are: test_acc:84.7926 test_loss:0.4169fold [6/10] is start!!
[07/0001] | train_loss:2.5787 val_acc:45.8525 val_loss:0.7037
model is saved at epoch 1!![07/0002] | train_loss:2.366 val_acc:58.9862 val_loss:0.6595
model is saved at epoch 2!![07/0003] | train_loss:2.2598 val_acc:71.8894 val_loss:0.5623
model is saved at epoch 3!![07/0004] | train_loss:2.2014 val_acc:73.7327 val_loss:0.5103
model is saved at epoch 4!![07/0005] | train_loss:2.1416 val_acc:75.8065 val_loss:0.4956
model is saved at epoch 5!![07/0006] | train_loss:2.1303 val_acc:75.3456 val_loss:0.4972
[07/0007] | train_loss:2.1177 val_acc:76.9585 val_loss:0.4876
model is saved at epoch 7!![07/0008] | train_loss:2.0881 val_acc:78.8018 val_loss:0.4745
model is saved at epoch 8!![07/0009] | train_loss:2.0282 val_acc:78.341 val_loss:0.4512
[07/0010] | train_loss:2.0325 val_acc:73.2719 val_loss:0.5447
[07/0011] | train_loss:2.0111 val_acc:79.4931 val_loss:0.4537
model is saved at epoch 11!![07/0012] | train_loss:2.0037 val_acc:80.6452 val_loss:0.4561
model is saved at epoch 12!![07/0013] | train_loss:1.9713 val_acc:80.4147 val_loss:0.4297
[07/0014] | train_loss:1.9449 val_acc:79.7235 val_loss:0.4584
[07/0015] | train_loss:1.9062 val_acc:78.8018 val_loss:0.4361
[07/0016] | train_loss:1.9112 val_acc:82.2581 val_loss:0.4161
model is saved at epoch 16!![07/0017] | train_loss:1.9449 val_acc:79.7235 val_loss:0.4537
[07/0018] | train_loss:1.8634 val_acc:81.3364 val_loss:0.4306
[07/0019] | train_loss:1.8979 val_acc:78.8018 val_loss:0.4295
[07/0020] | train_loss:1.9288 val_acc:80.6452 val_loss:0.4283
[07/0021] | train_loss:1.8374 val_acc:80.6452 val_loss:0.4204
[07/0022] | train_loss:1.7982 val_acc:83.4101 val_loss:0.4113
model is saved at epoch 22!![07/0023] | train_loss:1.7965 val_acc:82.4885 val_loss:0.4141
[07/0024] | train_loss:1.7738 val_acc:81.3364 val_loss:0.434
[07/0025] | train_loss:1.7696 val_acc:81.5668 val_loss:0.4234
[07/0026] | train_loss:1.7318 val_acc:83.6406 val_loss:0.4134
model is saved at epoch 26!![07/0027] | train_loss:1.7489 val_acc:79.2627 val_loss:0.4459
[07/0028] | train_loss:1.7406 val_acc:83.6406 val_loss:0.4122
[07/0029] | train_loss:1.7613 val_acc:82.4885 val_loss:0.4064
[07/0030] | train_loss:1.7102 val_acc:83.6406 val_loss:0.3942
[07/0031] | train_loss:1.6882 val_acc:82.7189 val_loss:0.4284
[07/0032] | train_loss:1.6838 val_acc:84.3318 val_loss:0.3821
model is saved at epoch 32!![07/0033] | train_loss:1.6634 val_acc:84.1014 val_loss:0.3951
[07/0034] | train_loss:1.6527 val_acc:82.4885 val_loss:0.4683
[07/0035] | train_loss:1.6699 val_acc:81.7972 val_loss:0.396
[07/0036] | train_loss:1.6421 val_acc:81.5668 val_loss:0.4441
[07/0037] | train_loss:1.6962 val_acc:78.5714 val_loss:0.4672
[07/0038] | train_loss:1.6824 val_acc:82.7189 val_loss:0.392
[07/0039] | train_loss:1.6231 val_acc:84.5622 val_loss:0.3977
model is saved at epoch 39!![07/0040] | train_loss:1.6385 val_acc:82.7189 val_loss:0.4313
[07/0041] | train_loss:1.6332 val_acc:82.2581 val_loss:0.4165
[07/0042] | train_loss:1.606 val_acc:81.106 val_loss:0.4333
[07/0043] | train_loss:1.6608 val_acc:84.1014 val_loss:0.4098
[07/0044] | train_loss:1.5824 val_acc:79.4931 val_loss:0.4431
[07/0045] | train_loss:1.6541 val_acc:81.5668 val_loss:0.4493
[07/0046] | train_loss:1.6281 val_acc:81.7972 val_loss:0.4425
[07/0047] | train_loss:1.6331 val_acc:79.7235 val_loss:0.4513
[07/0048] | train_loss:1.5689 val_acc:81.5668 val_loss:0.4267
[07/0049] | train_loss:1.5267 val_acc:83.6406 val_loss:0.4245
[07/0050] | train_loss:1.5465 val_acc:81.7972 val_loss:0.4462
[07/0051] | train_loss:1.554 val_acc:83.1797 val_loss:0.4246
[07/0052] | train_loss:1.5239 val_acc:82.7189 val_loss:0.4139
[07/0053] | train_loss:1.4984 val_acc:83.1797 val_loss:0.4321
[07/0054] | train_loss:1.4659 val_acc:83.6406 val_loss:0.4345
[07/0055] | train_loss:1.4896 val_acc:82.4885 val_loss:0.4625
[07/0056] | train_loss:1.4926 val_acc:83.6406 val_loss:0.4822
[07/0057] | train_loss:1.4996 val_acc:84.7926 val_loss:0.4321
model is saved at epoch 57!![07/0058] | train_loss:1.5335 val_acc:83.4101 val_loss:0.4382
[07/0059] | train_loss:1.4811 val_acc:83.4101 val_loss:0.492
[07/0060] | train_loss:1.5276 val_acc:83.6406 val_loss:0.4492
[07/0061] | train_loss:1.4746 val_acc:84.5622 val_loss:0.4854
[07/0062] | train_loss:1.4719 val_acc:83.4101 val_loss:0.4711
[07/0063] | train_loss:1.4187 val_acc:83.871 val_loss:0.4609
[07/0064] | train_loss:1.4009 val_acc:83.4101 val_loss:0.4934
[07/0065] | train_loss:1.4527 val_acc:81.7972 val_loss:0.5232
[07/0066] | train_loss:1.4995 val_acc:84.5622 val_loss:0.4198
[07/0067] | train_loss:1.5248 val_acc:82.4885 val_loss:0.4193
[07/0068] | train_loss:1.4505 val_acc:82.0276 val_loss:0.4488
[07/0069] | train_loss:1.4446 val_acc:82.9493 val_loss:0.4509
[07/0070] | train_loss:1.447 val_acc:82.7189 val_loss:0.4275
[07/0071] | train_loss:1.412 val_acc:84.1014 val_loss:0.4008
[07/0072] | train_loss:1.4337 val_acc:84.5622 val_loss:0.4856
[07/0073] | train_loss:1.5074 val_acc:82.7189 val_loss:0.4255
[07/0074] | train_loss:1.4297 val_acc:81.7972 val_loss:0.4522
[07/0075] | train_loss:1.4083 val_acc:85.4839 val_loss:0.4381
model is saved at epoch 75!![07/0076] | train_loss:1.3646 val_acc:85.2535 val_loss:0.4591
[07/0077] | train_loss:1.3644 val_acc:83.1797 val_loss:0.4405
[07/0078] | train_loss:1.3288 val_acc:83.6406 val_loss:0.4587
[07/0079] | train_loss:1.2976 val_acc:81.7972 val_loss:0.5052
[07/0080] | train_loss:1.3469 val_acc:85.2535 val_loss:0.4567
[07/0081] | train_loss:1.3799 val_acc:84.3318 val_loss:0.486
[07/0082] | train_loss:1.4474 val_acc:83.6406 val_loss:0.4414
[07/0083] | train_loss:1.3335 val_acc:85.7143 val_loss:0.4208
model is saved at epoch 83!![07/0084] | train_loss:1.2789 val_acc:86.6359 val_loss:0.4362
model is saved at epoch 84!![07/0085] | train_loss:1.2939 val_acc:83.4101 val_loss:0.4961
[07/0086] | train_loss:1.331 val_acc:84.1014 val_loss:0.4608
[07/0087] | train_loss:1.3494 val_acc:84.3318 val_loss:0.4215
[07/0088] | train_loss:1.3529 val_acc:82.9493 val_loss:0.4425
[07/0089] | train_loss:1.2816 val_acc:84.1014 val_loss:0.4366
[07/0090] | train_loss:1.4002 val_acc:81.3364 val_loss:0.5629
[07/0091] | train_loss:1.3941 val_acc:82.9493 val_loss:0.4417
[07/0092] | train_loss:1.3171 val_acc:83.6406 val_loss:0.4829
[07/0093] | train_loss:1.3624 val_acc:81.106 val_loss:0.4985
[07/0094] | train_loss:1.4122 val_acc:82.9493 val_loss:0.4184
[07/0095] | train_loss:1.3123 val_acc:82.2581 val_loss:0.5135
[07/0096] | train_loss:1.2647 val_acc:82.9493 val_loss:0.4959
[07/0097] | train_loss:1.1992 val_acc:82.2581 val_loss:0.5217
[07/0098] | train_loss:1.223 val_acc:83.871 val_loss:0.5332
[07/0099] | train_loss:1.2507 val_acc:84.3318 val_loss:0.4642
[07/0100] | train_loss:1.2174 val_acc:82.4885 val_loss:0.5643
[07/0101] | train_loss:1.1845 val_acc:84.1014 val_loss:0.5375
[07/0102] | train_loss:1.2623 val_acc:83.1797 val_loss:0.5446
[07/0103] | train_loss:1.4606 val_acc:82.4885 val_loss:0.4862
[07/0104] | train_loss:1.3557 val_acc:83.1797 val_loss:0.4608
[07/0105] | train_loss:1.2993 val_acc:82.7189 val_loss:0.5282
[07/0106] | train_loss:1.2833 val_acc:84.3318 val_loss:0.4481
[07/0107] | train_loss:1.2302 val_acc:80.8756 val_loss:0.4668
[07/0108] | train_loss:1.1828 val_acc:83.4101 val_loss:0.4829
[07/0109] | train_loss:1.1718 val_acc:83.1797 val_loss:0.5424
[07/0110] | train_loss:1.1886 val_acc:83.871 val_loss:0.5181
[07/0111] | train_loss:1.2343 val_acc:81.7972 val_loss:0.5082
[07/0112] | train_loss:1.2374 val_acc:83.6406 val_loss:0.495
[07/0113] | train_loss:1.2959 val_acc:82.4885 val_loss:0.5594
[07/0114] | train_loss:1.4156 val_acc:82.0276 val_loss:0.4849
[07/0115] | train_loss:1.3746 val_acc:80.1843 val_loss:0.4914
[07/0116] | train_loss:1.3147 val_acc:82.7189 val_loss:0.4513
[07/0117] | train_loss:1.2458 val_acc:82.7189 val_loss:0.4999
[07/0118] | train_loss:1.22 val_acc:84.1014 val_loss:0.4782
[07/0119] | train_loss:1.1512 val_acc:82.7189 val_loss:0.4938
[07/0120] | train_loss:1.1407 val_acc:83.1797 val_loss:0.5096
[07/0121] | train_loss:1.1821 val_acc:81.7972 val_loss:0.5613
[07/0122] | train_loss:1.2168 val_acc:82.2581 val_loss:0.694
[07/0123] | train_loss:1.2673 val_acc:82.4885 val_loss:0.5531
[07/0124] | train_loss:1.2445 val_acc:82.9493 val_loss:0.539
[07/0125] | train_loss:1.1662 val_acc:83.6406 val_loss:0.5639
[07/0126] | train_loss:1.1002 val_acc:83.871 val_loss:0.5627
[07/0127] | train_loss:1.2734 val_acc:81.7972 val_loss:0.5691
[07/0128] | train_loss:1.5314 val_acc:80.8756 val_loss:0.5437
[07/0129] | train_loss:1.4281 val_acc:82.7189 val_loss:0.4603
[07/0130] | train_loss:1.3393 val_acc:81.106 val_loss:0.5364
[07/0131] | train_loss:1.4046 val_acc:82.9493 val_loss:0.5438
[07/0132] | train_loss:1.3744 val_acc:82.2581 val_loss:0.5291
[07/0133] | train_loss:1.2829 val_acc:82.2581 val_loss:0.5127
[07/0134] | train_loss:1.3172 val_acc:80.8756 val_loss:0.5739
[07/0135] | train_loss:1.2803 val_acc:81.3364 val_loss:0.5539
Fold: [7/10] Test is finish !! 
 Test Metrics are: test_acc:83.4101 test_loss:0.5672fold [7/10] is start!!
[08/0001] | train_loss:2.5307 val_acc:60.3687 val_loss:0.6864
model is saved at epoch 1!![08/0002] | train_loss:2.3256 val_acc:64.7465 val_loss:0.6222
model is saved at epoch 2!![08/0003] | train_loss:2.2287 val_acc:73.7327 val_loss:0.5215
model is saved at epoch 3!![08/0004] | train_loss:2.1605 val_acc:73.2719 val_loss:0.4888
[08/0005] | train_loss:2.1539 val_acc:77.1889 val_loss:0.4902
model is saved at epoch 5!![08/0006] | train_loss:2.0436 val_acc:76.7281 val_loss:0.4828
[08/0007] | train_loss:2.0427 val_acc:77.1889 val_loss:0.4928
[08/0008] | train_loss:2.0446 val_acc:79.9539 val_loss:0.467
model is saved at epoch 8!![08/0009] | train_loss:2.0202 val_acc:76.9585 val_loss:0.4794
[08/0010] | train_loss:1.9953 val_acc:79.9539 val_loss:0.4663
[08/0011] | train_loss:1.9595 val_acc:79.0323 val_loss:0.4578
[08/0012] | train_loss:2.0041 val_acc:78.5714 val_loss:0.4574
[08/0013] | train_loss:1.9809 val_acc:80.8756 val_loss:0.4448
model is saved at epoch 13!![08/0014] | train_loss:1.9153 val_acc:74.424 val_loss:0.4892
[08/0015] | train_loss:1.875 val_acc:79.9539 val_loss:0.4476
[08/0016] | train_loss:1.844 val_acc:77.8802 val_loss:0.4633
[08/0017] | train_loss:1.8335 val_acc:79.0323 val_loss:0.4442
[08/0018] | train_loss:1.8381 val_acc:79.0323 val_loss:0.4381
[08/0019] | train_loss:1.8478 val_acc:78.8018 val_loss:0.4576
[08/0020] | train_loss:1.8232 val_acc:78.1106 val_loss:0.4819
[08/0021] | train_loss:1.8712 val_acc:78.1106 val_loss:0.481
[08/0022] | train_loss:1.8626 val_acc:77.8802 val_loss:0.465
[08/0023] | train_loss:1.7835 val_acc:80.4147 val_loss:0.4438
[08/0024] | train_loss:1.785 val_acc:80.6452 val_loss:0.4385
[08/0025] | train_loss:1.7385 val_acc:81.5668 val_loss:0.4394
model is saved at epoch 25!![08/0026] | train_loss:1.7429 val_acc:80.4147 val_loss:0.4271
[08/0027] | train_loss:1.7269 val_acc:81.3364 val_loss:0.4292
[08/0028] | train_loss:1.741 val_acc:79.9539 val_loss:0.4485
[08/0029] | train_loss:1.7075 val_acc:80.1843 val_loss:0.4216
[08/0030] | train_loss:1.6538 val_acc:79.9539 val_loss:0.4692
[08/0031] | train_loss:1.6357 val_acc:79.9539 val_loss:0.4665
[08/0032] | train_loss:1.6125 val_acc:80.6452 val_loss:0.4862
[08/0033] | train_loss:1.6259 val_acc:80.1843 val_loss:0.4504
[08/0034] | train_loss:1.5787 val_acc:81.7972 val_loss:0.4592
model is saved at epoch 34!![08/0035] | train_loss:1.567 val_acc:82.2581 val_loss:0.4581
model is saved at epoch 35!![08/0036] | train_loss:1.5963 val_acc:80.1843 val_loss:0.447
[08/0037] | train_loss:1.6125 val_acc:81.106 val_loss:0.4505
[08/0038] | train_loss:1.6105 val_acc:80.6452 val_loss:0.4671
[08/0039] | train_loss:1.6113 val_acc:80.1843 val_loss:0.4752
[08/0040] | train_loss:1.6151 val_acc:77.1889 val_loss:0.4694
[08/0041] | train_loss:1.5876 val_acc:77.6498 val_loss:0.5009
[08/0042] | train_loss:1.5809 val_acc:80.4147 val_loss:0.4465
[08/0043] | train_loss:1.5286 val_acc:81.3364 val_loss:0.4681
[08/0044] | train_loss:1.4937 val_acc:81.106 val_loss:0.4729
[08/0045] | train_loss:1.4513 val_acc:82.9493 val_loss:0.467
model is saved at epoch 45!![08/0046] | train_loss:1.4662 val_acc:81.3364 val_loss:0.4775
[08/0047] | train_loss:1.4659 val_acc:81.5668 val_loss:0.4718
[08/0048] | train_loss:1.4794 val_acc:81.106 val_loss:0.4566
[08/0049] | train_loss:1.4521 val_acc:81.7972 val_loss:0.467
[08/0050] | train_loss:1.495 val_acc:82.2581 val_loss:0.4522
[08/0051] | train_loss:1.4578 val_acc:80.6452 val_loss:0.5016
[08/0052] | train_loss:1.5251 val_acc:79.2627 val_loss:0.4693
[08/0053] | train_loss:1.4972 val_acc:80.1843 val_loss:0.5152
[08/0054] | train_loss:1.433 val_acc:79.7235 val_loss:0.4859
[08/0055] | train_loss:1.3887 val_acc:79.4931 val_loss:0.4995
[08/0056] | train_loss:1.4044 val_acc:80.4147 val_loss:0.5388
[08/0057] | train_loss:1.3383 val_acc:80.4147 val_loss:0.5251
[08/0058] | train_loss:1.4661 val_acc:80.1843 val_loss:0.5262
[08/0059] | train_loss:1.4231 val_acc:79.7235 val_loss:0.573
[08/0060] | train_loss:1.4238 val_acc:81.106 val_loss:0.4855
[08/0061] | train_loss:1.3421 val_acc:81.7972 val_loss:0.5066
[08/0062] | train_loss:1.3249 val_acc:79.9539 val_loss:0.5055
[08/0063] | train_loss:1.3499 val_acc:78.8018 val_loss:0.5317
[08/0064] | train_loss:1.4134 val_acc:79.7235 val_loss:0.4934
[08/0065] | train_loss:1.4564 val_acc:79.9539 val_loss:0.471
[08/0066] | train_loss:1.4151 val_acc:80.6452 val_loss:0.471
[08/0067] | train_loss:1.3536 val_acc:80.6452 val_loss:0.483
[08/0068] | train_loss:1.3534 val_acc:79.9539 val_loss:0.5115
[08/0069] | train_loss:1.2997 val_acc:80.6452 val_loss:0.5193
[08/0070] | train_loss:1.2863 val_acc:78.5714 val_loss:0.5184
[08/0071] | train_loss:1.373 val_acc:79.9539 val_loss:0.508
[08/0072] | train_loss:1.2544 val_acc:79.7235 val_loss:0.5747
[08/0073] | train_loss:1.2227 val_acc:79.4931 val_loss:0.5698
[08/0074] | train_loss:1.2691 val_acc:81.3364 val_loss:0.5792
[08/0075] | train_loss:1.2129 val_acc:81.106 val_loss:0.5694
[08/0076] | train_loss:1.3197 val_acc:82.7189 val_loss:0.4791
[08/0077] | train_loss:1.3699 val_acc:78.5714 val_loss:0.5285
[08/0078] | train_loss:1.3389 val_acc:82.2581 val_loss:0.5455
[08/0079] | train_loss:1.2913 val_acc:80.1843 val_loss:0.5211
[08/0080] | train_loss:1.315 val_acc:80.8756 val_loss:0.5469
[08/0081] | train_loss:1.2605 val_acc:82.2581 val_loss:0.5364
[08/0082] | train_loss:1.2237 val_acc:79.0323 val_loss:0.5664
[08/0083] | train_loss:1.2723 val_acc:80.6452 val_loss:0.5043
[08/0084] | train_loss:1.2999 val_acc:80.8756 val_loss:0.4761
[08/0085] | train_loss:1.3155 val_acc:79.7235 val_loss:0.4949
[08/0086] | train_loss:1.2439 val_acc:80.4147 val_loss:0.5518
[08/0087] | train_loss:1.2733 val_acc:79.2627 val_loss:0.573
[08/0088] | train_loss:1.1883 val_acc:79.7235 val_loss:0.5688
[08/0089] | train_loss:1.1548 val_acc:80.4147 val_loss:0.5505
[08/0090] | train_loss:1.1909 val_acc:82.4885 val_loss:0.5561
[08/0091] | train_loss:1.2125 val_acc:81.3364 val_loss:0.5181
[08/0092] | train_loss:1.1793 val_acc:80.4147 val_loss:0.5947
[08/0093] | train_loss:1.2048 val_acc:79.9539 val_loss:0.5797
[08/0094] | train_loss:1.1523 val_acc:81.3364 val_loss:0.6189
[08/0095] | train_loss:1.1663 val_acc:79.4931 val_loss:0.5805
[08/0096] | train_loss:1.1711 val_acc:81.3364 val_loss:0.5745
Fold: [8/10] Test is finish !! 
 Test Metrics are: test_acc:83.6028 test_loss:0.4924fold [8/10] is start!!
[09/0001] | train_loss:2.6379 val_acc:58.8915 val_loss:0.6901
model is saved at epoch 1!![09/0002] | train_loss:2.4035 val_acc:59.8152 val_loss:0.6856
model is saved at epoch 2!![09/0003] | train_loss:2.2668 val_acc:72.9792 val_loss:0.5444
model is saved at epoch 3!![09/0004] | train_loss:2.2309 val_acc:73.4411 val_loss:0.5474
model is saved at epoch 4!![09/0005] | train_loss:2.2022 val_acc:76.9053 val_loss:0.4874
model is saved at epoch 5!![09/0006] | train_loss:2.1269 val_acc:77.3672 val_loss:0.4861
model is saved at epoch 6!![09/0007] | train_loss:2.1162 val_acc:78.06 val_loss:0.4704
model is saved at epoch 7!![09/0008] | train_loss:2.0716 val_acc:78.291 val_loss:0.4702
model is saved at epoch 8!![09/0009] | train_loss:2.0463 val_acc:78.5219 val_loss:0.4644
model is saved at epoch 9!![09/0010] | train_loss:2.0495 val_acc:77.8291 val_loss:0.4624
[09/0011] | train_loss:2.0556 val_acc:78.7529 val_loss:0.4878
model is saved at epoch 11!![09/0012] | train_loss:2.0177 val_acc:78.9838 val_loss:0.4594
model is saved at epoch 12!![09/0013] | train_loss:1.9586 val_acc:79.4457 val_loss:0.449
model is saved at epoch 13!![09/0014] | train_loss:1.9235 val_acc:78.06 val_loss:0.4604
[09/0015] | train_loss:1.9107 val_acc:81.2933 val_loss:0.42
model is saved at epoch 15!![09/0016] | train_loss:1.8873 val_acc:79.4457 val_loss:0.421
[09/0017] | train_loss:1.8817 val_acc:80.6005 val_loss:0.4441
[09/0018] | train_loss:1.8519 val_acc:80.8314 val_loss:0.4269
[09/0019] | train_loss:1.9114 val_acc:81.2933 val_loss:0.4544
[09/0020] | train_loss:1.8415 val_acc:79.9076 val_loss:0.4434
[09/0021] | train_loss:1.7952 val_acc:80.3695 val_loss:0.4699
[09/0022] | train_loss:1.8099 val_acc:81.0624 val_loss:0.4473
[09/0023] | train_loss:1.7971 val_acc:80.3695 val_loss:0.4533
[09/0024] | train_loss:1.7346 val_acc:79.2148 val_loss:0.4515
[09/0025] | train_loss:1.8506 val_acc:80.6005 val_loss:0.4295
[09/0026] | train_loss:1.7888 val_acc:81.0624 val_loss:0.4143
[09/0027] | train_loss:1.7683 val_acc:82.2171 val_loss:0.4268
model is saved at epoch 27!![09/0028] | train_loss:1.7367 val_acc:80.3695 val_loss:0.4489
[09/0029] | train_loss:1.7243 val_acc:81.7552 val_loss:0.4318
[09/0030] | train_loss:1.7061 val_acc:83.1409 val_loss:0.4139
model is saved at epoch 30!![09/0031] | train_loss:1.7132 val_acc:79.6767 val_loss:0.4615
[09/0032] | train_loss:1.7368 val_acc:83.6028 val_loss:0.4202
model is saved at epoch 32!![09/0033] | train_loss:1.6999 val_acc:81.2933 val_loss:0.4427
[09/0034] | train_loss:1.6435 val_acc:79.6767 val_loss:0.5203
[09/0035] | train_loss:1.6376 val_acc:83.1409 val_loss:0.4638
[09/0036] | train_loss:1.7223 val_acc:82.2171 val_loss:0.4402
[09/0037] | train_loss:1.681 val_acc:82.448 val_loss:0.4437
[09/0038] | train_loss:1.6366 val_acc:82.448 val_loss:0.4355
[09/0039] | train_loss:1.6131 val_acc:81.0624 val_loss:0.4594
[09/0040] | train_loss:1.6504 val_acc:81.9861 val_loss:0.4699
[09/0041] | train_loss:1.5852 val_acc:81.9861 val_loss:0.435
[09/0042] | train_loss:1.5607 val_acc:82.2171 val_loss:0.4491
[09/0043] | train_loss:1.5593 val_acc:82.679 val_loss:0.4479
[09/0044] | train_loss:1.5858 val_acc:81.9861 val_loss:0.5095
[09/0045] | train_loss:1.5746 val_acc:83.1409 val_loss:0.4857
[09/0046] | train_loss:1.5446 val_acc:78.7529 val_loss:0.4812
[09/0047] | train_loss:1.5789 val_acc:82.679 val_loss:0.4261
[09/0048] | train_loss:1.6579 val_acc:83.3718 val_loss:0.44
[09/0049] | train_loss:1.5887 val_acc:85.2194 val_loss:0.4192
model is saved at epoch 49!![09/0050] | train_loss:1.5647 val_acc:82.679 val_loss:0.4529
[09/0051] | train_loss:1.5346 val_acc:83.1409 val_loss:0.49
[09/0052] | train_loss:1.5147 val_acc:81.5242 val_loss:0.4752
[09/0053] | train_loss:1.5149 val_acc:83.3718 val_loss:0.4806
[09/0054] | train_loss:1.505 val_acc:82.9099 val_loss:0.485
[09/0055] | train_loss:1.5067 val_acc:82.9099 val_loss:0.4632
[09/0056] | train_loss:1.4529 val_acc:79.4457 val_loss:0.504
[09/0057] | train_loss:1.5006 val_acc:82.679 val_loss:0.4898
[09/0058] | train_loss:1.5093 val_acc:79.4457 val_loss:0.477
[09/0059] | train_loss:1.5784 val_acc:82.2171 val_loss:0.5048
[09/0060] | train_loss:1.5753 val_acc:80.8314 val_loss:0.4943
[09/0061] | train_loss:1.4375 val_acc:81.0624 val_loss:0.4833
[09/0062] | train_loss:1.4102 val_acc:83.1409 val_loss:0.5193
[09/0063] | train_loss:1.4518 val_acc:82.448 val_loss:0.5283
[09/0064] | train_loss:1.5356 val_acc:81.5242 val_loss:0.5142
[09/0065] | train_loss:1.563 val_acc:82.448 val_loss:0.4891
[09/0066] | train_loss:1.5614 val_acc:81.0624 val_loss:0.4629
[09/0067] | train_loss:1.4833 val_acc:82.2171 val_loss:0.4683
[09/0068] | train_loss:1.4123 val_acc:81.2933 val_loss:0.5069
[09/0069] | train_loss:1.408 val_acc:81.9861 val_loss:0.4937
[09/0070] | train_loss:1.3848 val_acc:81.5242 val_loss:0.5276
[09/0071] | train_loss:1.3646 val_acc:79.4457 val_loss:0.5693
[09/0072] | train_loss:1.4147 val_acc:80.1386 val_loss:0.5454
[09/0073] | train_loss:1.3378 val_acc:81.5242 val_loss:0.5669
[09/0074] | train_loss:1.3417 val_acc:81.7552 val_loss:0.5275
[09/0075] | train_loss:1.3285 val_acc:81.7552 val_loss:0.6283
[09/0076] | train_loss:1.3544 val_acc:81.0624 val_loss:0.5125
[09/0077] | train_loss:1.3263 val_acc:80.6005 val_loss:0.5151
[09/0078] | train_loss:1.3152 val_acc:81.7552 val_loss:0.5935
[09/0079] | train_loss:1.2999 val_acc:80.1386 val_loss:0.5935
[09/0080] | train_loss:1.3076 val_acc:79.2148 val_loss:0.5535
[09/0081] | train_loss:1.3364 val_acc:81.9861 val_loss:0.5474
[09/0082] | train_loss:1.3982 val_acc:81.5242 val_loss:0.5147
[09/0083] | train_loss:1.4119 val_acc:81.5242 val_loss:0.5682
[09/0084] | train_loss:1.3674 val_acc:82.448 val_loss:0.5098
[09/0085] | train_loss:1.3674 val_acc:81.2933 val_loss:0.5375
[09/0086] | train_loss:1.4059 val_acc:80.8314 val_loss:0.5317
[09/0087] | train_loss:1.3961 val_acc:81.9861 val_loss:0.5135
[09/0088] | train_loss:1.3473 val_acc:83.1409 val_loss:0.5136
[09/0089] | train_loss:1.4575 val_acc:82.679 val_loss:0.5089
[09/0090] | train_loss:1.4464 val_acc:82.679 val_loss:0.4968
[09/0091] | train_loss:1.3929 val_acc:83.1409 val_loss:0.4861
[09/0092] | train_loss:1.5163 val_acc:82.2171 val_loss:0.4747
[09/0093] | train_loss:1.4364 val_acc:81.7552 val_loss:0.5024
[09/0094] | train_loss:1.3216 val_acc:81.9861 val_loss:0.544
[09/0095] | train_loss:1.2734 val_acc:80.8314 val_loss:0.5898
[09/0096] | train_loss:1.2792 val_acc:82.679 val_loss:0.5886
[09/0097] | train_loss:1.2489 val_acc:81.0624 val_loss:0.5939
[09/0098] | train_loss:1.2595 val_acc:81.2933 val_loss:0.5377
[09/0099] | train_loss:1.2519 val_acc:82.2171 val_loss:0.544
[09/0100] | train_loss:1.2067 val_acc:81.0624 val_loss:0.6153
Fold: [9/10] Test is finish !! 
 Test Metrics are: test_acc:82.679 test_loss:0.4561fold [9/10] is start!!
[10/0001] | train_loss:2.5495 val_acc:54.7344 val_loss:0.6884
model is saved at epoch 1!![10/0002] | train_loss:2.3804 val_acc:67.4365 val_loss:0.6425
model is saved at epoch 2!![10/0003] | train_loss:2.2549 val_acc:77.3672 val_loss:0.5063
model is saved at epoch 3!![10/0004] | train_loss:2.2188 val_acc:79.9076 val_loss:0.4742
model is saved at epoch 4!![10/0005] | train_loss:2.1801 val_acc:80.1386 val_loss:0.4883
model is saved at epoch 5!![10/0006] | train_loss:2.1032 val_acc:78.7529 val_loss:0.4788
[10/0007] | train_loss:2.1144 val_acc:74.134 val_loss:0.5137
[10/0008] | train_loss:2.0815 val_acc:77.3672 val_loss:0.4751
[10/0009] | train_loss:2.0007 val_acc:79.9076 val_loss:0.4466
[10/0010] | train_loss:2.0675 val_acc:79.6767 val_loss:0.4677
[10/0011] | train_loss:2.0176 val_acc:78.5219 val_loss:0.4539
[10/0012] | train_loss:2.0097 val_acc:79.2148 val_loss:0.4879
[10/0013] | train_loss:1.9662 val_acc:78.7529 val_loss:0.4292
[10/0014] | train_loss:1.9343 val_acc:80.8314 val_loss:0.4383
model is saved at epoch 14!![10/0015] | train_loss:1.9386 val_acc:76.4434 val_loss:0.4935
[10/0016] | train_loss:1.9199 val_acc:78.06 val_loss:0.4517
[10/0017] | train_loss:1.9152 val_acc:78.7529 val_loss:0.4721
[10/0018] | train_loss:1.9139 val_acc:80.1386 val_loss:0.4742
[10/0019] | train_loss:1.8714 val_acc:81.2933 val_loss:0.4422
model is saved at epoch 19!![10/0020] | train_loss:1.8482 val_acc:80.1386 val_loss:0.445
[10/0021] | train_loss:1.8204 val_acc:80.3695 val_loss:0.451
[10/0022] | train_loss:1.8135 val_acc:81.2933 val_loss:0.4773
[10/0023] | train_loss:1.7922 val_acc:78.06 val_loss:0.4698
[10/0024] | train_loss:1.7699 val_acc:80.6005 val_loss:0.4369
[10/0025] | train_loss:1.8048 val_acc:78.291 val_loss:0.4739
[10/0026] | train_loss:1.7333 val_acc:78.9838 val_loss:0.4319
[10/0027] | train_loss:1.6978 val_acc:80.3695 val_loss:0.4153
[10/0028] | train_loss:1.6587 val_acc:81.2933 val_loss:0.4384
[10/0029] | train_loss:1.6453 val_acc:78.7529 val_loss:0.4449
[10/0030] | train_loss:1.701 val_acc:78.5219 val_loss:0.472
[10/0031] | train_loss:1.6857 val_acc:82.448 val_loss:0.4145
model is saved at epoch 31!![10/0032] | train_loss:1.6563 val_acc:80.6005 val_loss:0.4241
[10/0033] | train_loss:1.6678 val_acc:81.0624 val_loss:0.4369
[10/0034] | train_loss:1.6651 val_acc:81.0624 val_loss:0.4113
[10/0035] | train_loss:1.6418 val_acc:80.3695 val_loss:0.4296
[10/0036] | train_loss:1.5905 val_acc:79.9076 val_loss:0.4637
[10/0037] | train_loss:1.5527 val_acc:81.9861 val_loss:0.4312
[10/0038] | train_loss:1.5862 val_acc:80.6005 val_loss:0.4424
[10/0039] | train_loss:1.5719 val_acc:80.3695 val_loss:0.4031
[10/0040] | train_loss:1.4865 val_acc:82.448 val_loss:0.4362
[10/0041] | train_loss:1.5196 val_acc:83.1409 val_loss:0.4316
model is saved at epoch 41!![10/0042] | train_loss:1.5215 val_acc:81.5242 val_loss:0.4483
[10/0043] | train_loss:1.508 val_acc:83.1409 val_loss:0.416
[10/0044] | train_loss:1.4976 val_acc:82.9099 val_loss:0.4323
[10/0045] | train_loss:1.4765 val_acc:81.9861 val_loss:0.4588
[10/0046] | train_loss:1.5583 val_acc:82.679 val_loss:0.4536
[10/0047] | train_loss:1.5354 val_acc:81.2933 val_loss:0.4619
[10/0048] | train_loss:1.4903 val_acc:82.448 val_loss:0.4336
[10/0049] | train_loss:1.446 val_acc:82.9099 val_loss:0.4228
[10/0050] | train_loss:1.4194 val_acc:80.8314 val_loss:0.4986
[10/0051] | train_loss:1.4239 val_acc:81.2933 val_loss:0.4682
[10/0052] | train_loss:1.4145 val_acc:82.9099 val_loss:0.4025
[10/0053] | train_loss:1.4415 val_acc:82.448 val_loss:0.4614
[10/0054] | train_loss:1.5143 val_acc:83.6028 val_loss:0.4457
model is saved at epoch 54!![10/0055] | train_loss:1.3977 val_acc:80.8314 val_loss:0.486
[10/0056] | train_loss:1.3894 val_acc:81.0624 val_loss:0.4573
[10/0057] | train_loss:1.381 val_acc:82.2171 val_loss:0.4663
[10/0058] | train_loss:1.4678 val_acc:83.6028 val_loss:0.4462
[10/0059] | train_loss:1.3963 val_acc:79.2148 val_loss:0.4888
[10/0060] | train_loss:1.4597 val_acc:81.9861 val_loss:0.427
[10/0061] | train_loss:1.3624 val_acc:81.5242 val_loss:0.458
[10/0062] | train_loss:1.4052 val_acc:83.1409 val_loss:0.4707
[10/0063] | train_loss:1.3351 val_acc:82.679 val_loss:0.57
[10/0064] | train_loss:1.3857 val_acc:80.1386 val_loss:0.4628
[10/0065] | train_loss:1.3071 val_acc:80.8314 val_loss:0.4755
[10/0066] | train_loss:1.2935 val_acc:81.7552 val_loss:0.4946
[10/0067] | train_loss:1.324 val_acc:80.3695 val_loss:0.4471
[10/0068] | train_loss:1.3122 val_acc:81.5242 val_loss:0.4682
[10/0069] | train_loss:1.3103 val_acc:81.2933 val_loss:0.4693
[10/0070] | train_loss:1.2698 val_acc:80.3695 val_loss:0.4807
[10/0071] | train_loss:1.3331 val_acc:81.9861 val_loss:0.5264
[10/0072] | train_loss:1.3309 val_acc:82.2171 val_loss:0.4956
[10/0073] | train_loss:1.3101 val_acc:81.5242 val_loss:0.4955
[10/0074] | train_loss:1.3294 val_acc:82.9099 val_loss:0.4537
[10/0075] | train_loss:1.3657 val_acc:79.9076 val_loss:0.5488
[10/0076] | train_loss:1.2969 val_acc:82.2171 val_loss:0.474
[10/0077] | train_loss:1.2403 val_acc:81.2933 val_loss:0.5254
[10/0078] | train_loss:1.2091 val_acc:80.8314 val_loss:0.5101
[10/0079] | train_loss:1.2746 val_acc:80.3695 val_loss:0.5466
[10/0080] | train_loss:1.3016 val_acc:81.0624 val_loss:0.5046
[10/0081] | train_loss:1.2864 val_acc:78.7529 val_loss:0.6093
[10/0082] | train_loss:1.3175 val_acc:80.1386 val_loss:0.494
[10/0083] | train_loss:1.3446 val_acc:82.9099 val_loss:0.4585
[10/0084] | train_loss:1.3091 val_acc:80.1386 val_loss:0.4746
[10/0085] | train_loss:1.2354 val_acc:80.6005 val_loss:0.4907
[10/0086] | train_loss:1.1733 val_acc:84.0647 val_loss:0.4443
model is saved at epoch 86!![10/0087] | train_loss:1.2127 val_acc:81.5242 val_loss:0.4989
[10/0088] | train_loss:1.2481 val_acc:79.9076 val_loss:0.4708
[10/0089] | train_loss:1.1659 val_acc:81.5242 val_loss:0.5205
[10/0090] | train_loss:1.159 val_acc:82.2171 val_loss:0.4972
[10/0091] | train_loss:1.1068 val_acc:79.9076 val_loss:0.586
[10/0092] | train_loss:1.1285 val_acc:81.5242 val_loss:0.4879
[10/0093] | train_loss:1.1613 val_acc:82.2171 val_loss:0.4734
[10/0094] | train_loss:1.1183 val_acc:81.7552 val_loss:0.5687
[10/0095] | train_loss:1.1562 val_acc:81.0624 val_loss:0.4988
[10/0096] | train_loss:1.1683 val_acc:82.448 val_loss:0.5288
[10/0097] | train_loss:1.0867 val_acc:81.5242 val_loss:0.5388
[10/0098] | train_loss:1.1257 val_acc:80.8314 val_loss:0.5406
[10/0099] | train_loss:1.0959 val_acc:80.3695 val_loss:0.6052
[10/0100] | train_loss:1.0545 val_acc:81.7552 val_loss:0.5709
[10/0101] | train_loss:1.035 val_acc:80.3695 val_loss:0.6274
[10/0102] | train_loss:1.0767 val_acc:80.1386 val_loss:0.5477
[10/0103] | train_loss:1.0675 val_acc:81.2933 val_loss:0.5452
[10/0104] | train_loss:1.0366 val_acc:80.6005 val_loss:0.6042
[10/0105] | train_loss:1.1776 val_acc:79.4457 val_loss:0.5808
[10/0106] | train_loss:1.1674 val_acc:81.2933 val_loss:0.556
[10/0107] | train_loss:1.2575 val_acc:82.9099 val_loss:0.432
[10/0108] | train_loss:1.1233 val_acc:81.7552 val_loss:0.5196
[10/0109] | train_loss:1.044 val_acc:81.7552 val_loss:0.5258
[10/0110] | train_loss:1.0208 val_acc:81.0624 val_loss:0.5361
[10/0111] | train_loss:1.107 val_acc:82.9099 val_loss:0.6186
[10/0112] | train_loss:1.0612 val_acc:80.6005 val_loss:0.5208
[10/0113] | train_loss:1.1038 val_acc:83.1409 val_loss:0.5171
[10/0114] | train_loss:1.0618 val_acc:79.6767 val_loss:0.6353
[10/0115] | train_loss:0.9815 val_acc:80.1386 val_loss:0.6505
[10/0116] | train_loss:0.9058 val_acc:80.8314 val_loss:0.7206
[10/0117] | train_loss:0.9009 val_acc:80.3695 val_loss:0.6443
[10/0118] | train_loss:0.9432 val_acc:81.0624 val_loss:0.6107
[10/0119] | train_loss:1.0053 val_acc:81.5242 val_loss:0.5981
[10/0120] | train_loss:1.0592 val_acc:81.5242 val_loss:0.5466
[10/0121] | train_loss:1.0179 val_acc:80.6005 val_loss:0.5534
[10/0122] | train_loss:1.0339 val_acc:81.7552 val_loss:0.6147
[10/0123] | train_loss:0.9792 val_acc:80.8314 val_loss:0.5521
[10/0124] | train_loss:0.9757 val_acc:79.9076 val_loss:0.5784
[10/0125] | train_loss:0.975 val_acc:80.1386 val_loss:0.6338
[10/0126] | train_loss:1.01 val_acc:81.5242 val_loss:0.627
[10/0127] | train_loss:0.9664 val_acc:80.3695 val_loss:0.5734
[10/0128] | train_loss:0.9458 val_acc:79.2148 val_loss:0.6183
[10/0129] | train_loss:1.0163 val_acc:80.1386 val_loss:0.6343
[10/0130] | train_loss:1.0804 val_acc:82.679 val_loss:0.5062
[10/0131] | train_loss:0.9865 val_acc:81.5242 val_loss:0.5523
[10/0132] | train_loss:0.9611 val_acc:82.679 val_loss:0.5774
[10/0133] | train_loss:0.9529 val_acc:81.9861 val_loss:0.611
[10/0134] | train_loss:0.9651 val_acc:80.1386 val_loss:0.653
[10/0135] | train_loss:0.9877 val_acc:81.2933 val_loss:0.6016
[10/0136] | train_loss:0.9812 val_acc:81.2933 val_loss:0.658
[10/0137] | train_loss:0.9427 val_acc:80.1386 val_loss:0.7076
Fold: [10/10] Test is finish !! 
 Test Metrics are: test_acc:81.9861 test_loss:0.5811
all fold acc is: 
[85.02303957939148, 83.64055156707764, 84.56221222877502, 79.26267385482788, 83.41013789176941, 84.79262590408325, 83.41013789176941, 83.602774143219, 82.67898559570312, 81.98614120483398] 
Test is finish !! 
 Test Metrics are: acc_mean:83.2369 acc_std:1.5935