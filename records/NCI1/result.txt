Dataset: NCI1,
Model Name: EnsNet
net_params={'num_layers': 3, 'hidden': 64, 'dropout': 0.1, 'ds_name': 'NCI1', 'model_type': 'ThreeModel', 'temperature': 1, 'beta': 2, 'gama': 2, 'yta': 200, 'alpha': 1, 'in_channels': 37, 'out_channels': 2, 'device': 'cuda:3'}
train_config={'epochs': 300, 'batch_size': 128, 'seed': 8971, 'patience': 50, 'lr': 0.005, 'weight_decay': 1e-05}
EnsNet(
  (model1): SAGPool(
    (convs): ModuleList(
      (0): SAGEConv(37, 64)
      (1): SAGEConv(64, 64)
      (2): SAGEConv(64, 64)
    )
    (pools): ModuleList(
      (0): SAGPooling(
        (score_layer): GCNConv(192, 1)
      )
    )
    (lin1): Linear(in_features=384, out_features=64, bias=True)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
      (2): BatchNorm(64)
    )
    (lin2): Linear(in_features=64, out_features=32, bias=True)
    (lin3): Linear(in_features=32, out_features=2, bias=True)
  )
  (model2): SAGPool(
    (convs): ModuleList(
      (0): SAGEConv(37, 64)
      (1): SAGEConv(64, 64)
      (2): SAGEConv(64, 64)
    )
    (pools): ModuleList(
      (0): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
      (1): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
      (2): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
    )
    (lin1): Linear(in_features=128, out_features=64, bias=True)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
      (2): BatchNorm(64)
    )
    (lin2): Linear(in_features=64, out_features=32, bias=True)
    (lin3): Linear(in_features=32, out_features=2, bias=True)
  )
  (model3): GlobalAttentionNet(
    (convs): ModuleList(
      (0): SAGEConv(37, 64)
      (1): SAGEConv(64, 64)
      (2): SAGEConv(64, 64)
    )
    (att): GlobalAttention(gate_nn=Linear(in_features=64, out_features=1, bias=True), nn=None)
    (dropout): Dropout(p=0.3, inplace=False)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
      (2): BatchNorm(64)
    )
    (lin1): Linear(in_features=64, out_features=64, bias=True)
    (lin2): Linear(in_features=64, out_features=2, bias=True)
  )
  (confiNet): Sequential(
    (0): Linear(in_features=66, out_features=33, bias=True)
    (1): ReLU()
    (2): Linear(in_features=33, out_features=1, bias=True)
    (3): Sigmoid()
  )
)

fold [0/10] is start!!
[01/0001] | train_loss:2.6734 val_acc:48.9051 val_loss:0.6742
model is saved at epoch 1!![01/0002] | train_loss:2.5378 val_acc:49.635 val_loss:0.6691
model is saved at epoch 2!![01/0003] | train_loss:2.4941 val_acc:63.9903 val_loss:0.5856
model is saved at epoch 3!![01/0004] | train_loss:2.4391 val_acc:70.5596 val_loss:0.5538
model is saved at epoch 4!![01/0005] | train_loss:2.3739 val_acc:73.236 val_loss:0.5814
model is saved at epoch 5!![01/0006] | train_loss:2.3568 val_acc:72.0195 val_loss:0.5266
[01/0007] | train_loss:2.33 val_acc:75.1825 val_loss:0.5498
model is saved at epoch 7!![01/0008] | train_loss:2.2855 val_acc:78.3455 val_loss:0.5163
model is saved at epoch 8!![01/0009] | train_loss:2.2472 val_acc:76.8856 val_loss:0.5043
[01/0010] | train_loss:2.2114 val_acc:77.3723 val_loss:0.5113
[01/0011] | train_loss:2.2076 val_acc:75.6691 val_loss:0.5058
[01/0012] | train_loss:2.16 val_acc:77.3723 val_loss:0.4922
[01/0013] | train_loss:2.1184 val_acc:75.6691 val_loss:0.5413
[01/0014] | train_loss:2.1027 val_acc:73.4793 val_loss:0.4817
[01/0015] | train_loss:2.1133 val_acc:75.9124 val_loss:0.4794
[01/0016] | train_loss:2.0395 val_acc:78.1022 val_loss:0.4909
[01/0017] | train_loss:2.0763 val_acc:77.8589 val_loss:0.4711
[01/0018] | train_loss:2.0221 val_acc:77.6156 val_loss:0.4752
[01/0019] | train_loss:2.0431 val_acc:78.3455 val_loss:0.5037
[01/0020] | train_loss:1.9819 val_acc:79.0754 val_loss:0.49
model is saved at epoch 20!![01/0021] | train_loss:1.9529 val_acc:78.5888 val_loss:0.4442
[01/0022] | train_loss:1.9459 val_acc:78.8321 val_loss:0.4528
[01/0023] | train_loss:1.8731 val_acc:80.292 val_loss:0.4887
model is saved at epoch 23!![01/0024] | train_loss:1.9305 val_acc:78.1022 val_loss:0.4971
[01/0025] | train_loss:1.8988 val_acc:78.5888 val_loss:0.4682
[01/0026] | train_loss:1.8288 val_acc:78.5888 val_loss:0.488
[01/0027] | train_loss:1.8416 val_acc:79.0754 val_loss:0.5234
[01/0028] | train_loss:1.8569 val_acc:80.0487 val_loss:0.4747
[01/0029] | train_loss:1.7769 val_acc:77.6156 val_loss:0.4564
[01/0030] | train_loss:1.7472 val_acc:81.2652 val_loss:0.4405
model is saved at epoch 30!![01/0031] | train_loss:1.7708 val_acc:80.292 val_loss:0.4566
[01/0032] | train_loss:1.7752 val_acc:76.399 val_loss:0.4396
[01/0033] | train_loss:1.7618 val_acc:80.292 val_loss:0.4374
[01/0034] | train_loss:1.763 val_acc:77.3723 val_loss:0.46
[01/0035] | train_loss:1.7229 val_acc:80.292 val_loss:0.4679
[01/0036] | train_loss:1.7207 val_acc:79.562 val_loss:0.4631
[01/0037] | train_loss:1.6544 val_acc:80.292 val_loss:0.4603
[01/0038] | train_loss:1.6483 val_acc:80.292 val_loss:0.501
[01/0039] | train_loss:1.7041 val_acc:81.0219 val_loss:0.4605
[01/0040] | train_loss:1.6716 val_acc:80.0487 val_loss:0.483
[01/0041] | train_loss:1.5746 val_acc:80.5353 val_loss:0.47
[01/0042] | train_loss:1.6127 val_acc:78.3455 val_loss:0.4964
[01/0043] | train_loss:1.6146 val_acc:80.5353 val_loss:0.489
[01/0044] | train_loss:1.595 val_acc:80.0487 val_loss:0.4812
[01/0045] | train_loss:1.5681 val_acc:81.7518 val_loss:0.464
model is saved at epoch 45!![01/0046] | train_loss:1.5631 val_acc:80.0487 val_loss:0.4908
[01/0047] | train_loss:1.5364 val_acc:82.4818 val_loss:0.4583
model is saved at epoch 47!![01/0048] | train_loss:1.5554 val_acc:80.0487 val_loss:0.4477
[01/0049] | train_loss:1.5417 val_acc:79.8054 val_loss:0.4985
[01/0050] | train_loss:1.6045 val_acc:78.8321 val_loss:0.4645
[01/0051] | train_loss:1.5432 val_acc:78.8321 val_loss:0.5049
[01/0052] | train_loss:1.5149 val_acc:80.7786 val_loss:0.4669
[01/0053] | train_loss:1.4836 val_acc:80.5353 val_loss:0.4556
[01/0054] | train_loss:1.4726 val_acc:78.5888 val_loss:0.5008
[01/0055] | train_loss:1.4547 val_acc:81.0219 val_loss:0.498
[01/0056] | train_loss:1.3748 val_acc:81.2652 val_loss:0.4707
[01/0057] | train_loss:1.3986 val_acc:78.5888 val_loss:0.4786
[01/0058] | train_loss:1.4463 val_acc:79.562 val_loss:0.4667
[01/0059] | train_loss:1.417 val_acc:81.7518 val_loss:0.4574
[01/0060] | train_loss:1.4078 val_acc:81.5085 val_loss:0.4863
[01/0061] | train_loss:1.3522 val_acc:82.4818 val_loss:0.4658
[01/0062] | train_loss:1.363 val_acc:79.562 val_loss:0.4945
[01/0063] | train_loss:1.3361 val_acc:83.2117 val_loss:0.4859
model is saved at epoch 63!![01/0064] | train_loss:1.359 val_acc:79.0754 val_loss:0.5061
[01/0065] | train_loss:1.4072 val_acc:79.8054 val_loss:0.5156
[01/0066] | train_loss:1.3643 val_acc:81.9951 val_loss:0.436
[01/0067] | train_loss:1.3566 val_acc:80.0487 val_loss:0.4598
[01/0068] | train_loss:1.3154 val_acc:82.2384 val_loss:0.5053
[01/0069] | train_loss:1.2876 val_acc:80.0487 val_loss:0.5059
[01/0070] | train_loss:1.3301 val_acc:81.5085 val_loss:0.5149
[01/0071] | train_loss:1.2908 val_acc:79.562 val_loss:0.5564
[01/0072] | train_loss:1.2848 val_acc:81.7518 val_loss:0.5502
[01/0073] | train_loss:1.2549 val_acc:82.2384 val_loss:0.5103
[01/0074] | train_loss:1.3191 val_acc:78.5888 val_loss:0.5282
[01/0075] | train_loss:1.2708 val_acc:81.9951 val_loss:0.4763
[01/0076] | train_loss:1.2192 val_acc:81.2652 val_loss:0.5055
[01/0077] | train_loss:1.2706 val_acc:81.7518 val_loss:0.4998
[01/0078] | train_loss:1.2344 val_acc:82.4818 val_loss:0.5301
[01/0079] | train_loss:1.1658 val_acc:81.7518 val_loss:0.5556
[01/0080] | train_loss:1.196 val_acc:81.5085 val_loss:0.5175
[01/0081] | train_loss:1.1846 val_acc:80.5353 val_loss:0.4896
[01/0082] | train_loss:1.1861 val_acc:82.2384 val_loss:0.4759
[01/0083] | train_loss:1.1778 val_acc:81.2652 val_loss:0.5188
[01/0084] | train_loss:1.1114 val_acc:82.2384 val_loss:0.5001
[01/0085] | train_loss:1.1579 val_acc:81.9951 val_loss:0.5308
[01/0086] | train_loss:1.1827 val_acc:82.7251 val_loss:0.4668
[01/0087] | train_loss:1.1641 val_acc:81.5085 val_loss:0.568
[01/0088] | train_loss:1.1596 val_acc:81.9951 val_loss:0.5
[01/0089] | train_loss:1.133 val_acc:82.2384 val_loss:0.5405
[01/0090] | train_loss:1.0906 val_acc:82.7251 val_loss:0.5012
[01/0091] | train_loss:1.0605 val_acc:84.1849 val_loss:0.4974
model is saved at epoch 91!![01/0092] | train_loss:1.064 val_acc:82.7251 val_loss:0.5227
[01/0093] | train_loss:1.0867 val_acc:82.4818 val_loss:0.5311
[01/0094] | train_loss:1.0505 val_acc:83.2117 val_loss:0.5293
[01/0095] | train_loss:1.0292 val_acc:82.9684 val_loss:0.5859
[01/0096] | train_loss:1.0407 val_acc:82.9684 val_loss:0.5117
[01/0097] | train_loss:1.0472 val_acc:82.9684 val_loss:0.5168
[01/0098] | train_loss:1.1163 val_acc:81.5085 val_loss:0.5528
[01/0099] | train_loss:1.1001 val_acc:81.5085 val_loss:0.5379
[01/0100] | train_loss:1.0524 val_acc:82.4818 val_loss:0.5651
[01/0101] | train_loss:0.9855 val_acc:82.4818 val_loss:0.5385
[01/0102] | train_loss:0.9737 val_acc:84.4282 val_loss:0.583
model is saved at epoch 102!![01/0103] | train_loss:0.9986 val_acc:82.2384 val_loss:0.5683
[01/0104] | train_loss:0.9778 val_acc:81.9951 val_loss:0.5759
[01/0105] | train_loss:0.9595 val_acc:82.4818 val_loss:0.5508
[01/0106] | train_loss:0.9922 val_acc:82.9684 val_loss:0.6308
[01/0107] | train_loss:0.9184 val_acc:81.9951 val_loss:0.6016
[01/0108] | train_loss:0.9643 val_acc:81.2652 val_loss:0.5748
[01/0109] | train_loss:0.9585 val_acc:83.455 val_loss:0.5976
[01/0110] | train_loss:0.9627 val_acc:81.2652 val_loss:0.6189
[01/0111] | train_loss:0.9961 val_acc:85.4015 val_loss:0.5622
model is saved at epoch 111!![01/0112] | train_loss:0.9148 val_acc:83.455 val_loss:0.6127
[01/0113] | train_loss:0.9549 val_acc:82.7251 val_loss:0.5464
[01/0114] | train_loss:0.9926 val_acc:82.7251 val_loss:0.6147
[01/0115] | train_loss:0.918 val_acc:83.455 val_loss:0.6188
[01/0116] | train_loss:0.9639 val_acc:82.7251 val_loss:0.6808
[01/0117] | train_loss:0.9387 val_acc:83.9416 val_loss:0.5248
[01/0118] | train_loss:0.8851 val_acc:84.4282 val_loss:0.5612
[01/0119] | train_loss:0.909 val_acc:82.2384 val_loss:0.6154
[01/0120] | train_loss:0.8931 val_acc:82.9684 val_loss:0.5709
[01/0121] | train_loss:0.8517 val_acc:81.7518 val_loss:0.6369
[01/0122] | train_loss:0.8533 val_acc:81.5085 val_loss:0.7216
[01/0123] | train_loss:0.8471 val_acc:83.2117 val_loss:0.6652
[01/0124] | train_loss:0.898 val_acc:82.7251 val_loss:0.6396
[01/0125] | train_loss:0.815 val_acc:84.6715 val_loss:0.628
[01/0126] | train_loss:0.8606 val_acc:84.4282 val_loss:0.5747
[01/0127] | train_loss:0.8562 val_acc:83.9416 val_loss:0.5463
[01/0128] | train_loss:0.8371 val_acc:83.455 val_loss:0.6275
[01/0129] | train_loss:0.8063 val_acc:83.455 val_loss:0.6508
[01/0130] | train_loss:0.7922 val_acc:82.7251 val_loss:0.6682
[01/0131] | train_loss:0.9529 val_acc:82.9684 val_loss:0.6123
[01/0132] | train_loss:0.8859 val_acc:81.5085 val_loss:0.6251
[01/0133] | train_loss:0.8373 val_acc:82.9684 val_loss:0.5547
[01/0134] | train_loss:0.8219 val_acc:84.6715 val_loss:0.6695
[01/0135] | train_loss:0.7511 val_acc:84.6715 val_loss:0.6031
[01/0136] | train_loss:0.7572 val_acc:84.6715 val_loss:0.6438
[01/0137] | train_loss:0.7674 val_acc:84.1849 val_loss:0.6763
[01/0138] | train_loss:0.8443 val_acc:83.2117 val_loss:0.6204
[01/0139] | train_loss:0.862 val_acc:84.6715 val_loss:0.5763
[01/0140] | train_loss:0.8018 val_acc:82.2384 val_loss:0.5555
[01/0141] | train_loss:0.8081 val_acc:83.6983 val_loss:0.6775
[01/0142] | train_loss:0.7607 val_acc:83.2117 val_loss:0.7046
[01/0143] | train_loss:0.7475 val_acc:82.2384 val_loss:0.6703
[01/0144] | train_loss:0.7746 val_acc:83.455 val_loss:0.6903
[01/0145] | train_loss:0.8541 val_acc:81.9951 val_loss:0.6919
[01/0146] | train_loss:0.8506 val_acc:82.7251 val_loss:0.662
[01/0147] | train_loss:0.746 val_acc:83.455 val_loss:0.6448
[01/0148] | train_loss:0.7657 val_acc:83.9416 val_loss:0.6314
[01/0149] | train_loss:0.6952 val_acc:84.6715 val_loss:0.6619
[01/0150] | train_loss:0.7484 val_acc:81.7518 val_loss:0.6795
[01/0151] | train_loss:0.7327 val_acc:84.4282 val_loss:0.7105
[01/0152] | train_loss:0.7496 val_acc:83.9416 val_loss:0.7266
[01/0153] | train_loss:0.7657 val_acc:82.9684 val_loss:0.7583
[01/0154] | train_loss:0.7508 val_acc:84.4282 val_loss:0.6865
[01/0155] | train_loss:0.7513 val_acc:85.1582 val_loss:0.6491
[01/0156] | train_loss:0.785 val_acc:82.9684 val_loss:0.6494
[01/0157] | train_loss:0.7323 val_acc:82.4818 val_loss:0.639
[01/0158] | train_loss:0.6678 val_acc:81.9951 val_loss:0.7315
[01/0159] | train_loss:0.8327 val_acc:83.6983 val_loss:0.6525
[01/0160] | train_loss:0.7311 val_acc:83.9416 val_loss:0.6279
[01/0161] | train_loss:0.7407 val_acc:82.9684 val_loss:0.6461
[01/0162] | train_loss:0.6977 val_acc:85.4015 val_loss:0.6108
Fold: [1/10] Test is finish !! 
 Test Metrics are: test_acc:85.6448 test_loss:0.5758fold [1/10] is start!!
[02/0001] | train_loss:2.6335 val_acc:45.7421 val_loss:0.6826
model is saved at epoch 1!![02/0002] | train_loss:2.4956 val_acc:45.7421 val_loss:0.6813
[02/0003] | train_loss:2.4207 val_acc:62.7737 val_loss:0.595
model is saved at epoch 3!![02/0004] | train_loss:2.3738 val_acc:72.2628 val_loss:0.5799
model is saved at epoch 4!![02/0005] | train_loss:2.3281 val_acc:72.0195 val_loss:0.5194
[02/0006] | train_loss:2.2824 val_acc:72.7494 val_loss:0.5252
model is saved at epoch 6!![02/0007] | train_loss:2.241 val_acc:74.9392 val_loss:0.5247
model is saved at epoch 7!![02/0008] | train_loss:2.1885 val_acc:75.4258 val_loss:0.5266
model is saved at epoch 8!![02/0009] | train_loss:2.1708 val_acc:76.1557 val_loss:0.4913
model is saved at epoch 9!![02/0010] | train_loss:2.1568 val_acc:76.399 val_loss:0.4901
model is saved at epoch 10!![02/0011] | train_loss:2.1297 val_acc:76.6423 val_loss:0.4953
model is saved at epoch 11!![02/0012] | train_loss:2.1105 val_acc:76.1557 val_loss:0.4879
[02/0013] | train_loss:2.0765 val_acc:80.0487 val_loss:0.4833
model is saved at epoch 13!![02/0014] | train_loss:2.0437 val_acc:78.5888 val_loss:0.4805
[02/0015] | train_loss:2.0235 val_acc:78.8321 val_loss:0.4669
[02/0016] | train_loss:2.0035 val_acc:80.5353 val_loss:0.4564
model is saved at epoch 16!![02/0017] | train_loss:1.9572 val_acc:77.8589 val_loss:0.4993
[02/0018] | train_loss:1.9557 val_acc:82.4818 val_loss:0.501
model is saved at epoch 18!![02/0019] | train_loss:1.9806 val_acc:78.5888 val_loss:0.4824
[02/0020] | train_loss:1.9727 val_acc:80.292 val_loss:0.4648
[02/0021] | train_loss:1.9202 val_acc:82.4818 val_loss:0.4418
[02/0022] | train_loss:1.9305 val_acc:81.0219 val_loss:0.4635
[02/0023] | train_loss:1.903 val_acc:79.3187 val_loss:0.4587
[02/0024] | train_loss:1.9035 val_acc:78.5888 val_loss:0.479
[02/0025] | train_loss:1.8832 val_acc:81.7518 val_loss:0.4997
[02/0026] | train_loss:1.8547 val_acc:80.0487 val_loss:0.4776
[02/0027] | train_loss:1.8702 val_acc:80.0487 val_loss:0.4901
[02/0028] | train_loss:1.8512 val_acc:81.5085 val_loss:0.4392
[02/0029] | train_loss:1.8107 val_acc:81.7518 val_loss:0.4284
[02/0030] | train_loss:1.7595 val_acc:82.4818 val_loss:0.4305
[02/0031] | train_loss:1.7641 val_acc:82.9684 val_loss:0.4686
model is saved at epoch 31!![02/0032] | train_loss:1.7326 val_acc:82.2384 val_loss:0.4434
[02/0033] | train_loss:1.7105 val_acc:81.2652 val_loss:0.4287
[02/0034] | train_loss:1.7205 val_acc:83.455 val_loss:0.4424
model is saved at epoch 34!![02/0035] | train_loss:1.7007 val_acc:82.4818 val_loss:0.4267
[02/0036] | train_loss:1.6429 val_acc:83.455 val_loss:0.429
[02/0037] | train_loss:1.7067 val_acc:81.5085 val_loss:0.464
[02/0038] | train_loss:1.7004 val_acc:81.5085 val_loss:0.4896
[02/0039] | train_loss:1.6965 val_acc:80.0487 val_loss:0.4762
[02/0040] | train_loss:1.6595 val_acc:81.0219 val_loss:0.4665
[02/0041] | train_loss:1.6303 val_acc:81.2652 val_loss:0.4303
[02/0042] | train_loss:1.6127 val_acc:82.7251 val_loss:0.4915
[02/0043] | train_loss:1.6146 val_acc:79.8054 val_loss:0.5175
[02/0044] | train_loss:1.5699 val_acc:82.9684 val_loss:0.4583
[02/0045] | train_loss:1.5737 val_acc:82.2384 val_loss:0.4793
[02/0046] | train_loss:1.5727 val_acc:83.9416 val_loss:0.4377
model is saved at epoch 46!![02/0047] | train_loss:1.5608 val_acc:83.6983 val_loss:0.4617
[02/0048] | train_loss:1.5145 val_acc:83.455 val_loss:0.4982
[02/0049] | train_loss:1.5301 val_acc:82.2384 val_loss:0.4735
[02/0050] | train_loss:1.5425 val_acc:80.5353 val_loss:0.4676
[02/0051] | train_loss:1.5863 val_acc:82.2384 val_loss:0.484
[02/0052] | train_loss:1.5645 val_acc:83.2117 val_loss:0.5266
[02/0053] | train_loss:1.5047 val_acc:84.1849 val_loss:0.4689
model is saved at epoch 53!![02/0054] | train_loss:1.5229 val_acc:84.1849 val_loss:0.4674
[02/0055] | train_loss:1.4881 val_acc:83.9416 val_loss:0.5089
[02/0056] | train_loss:1.4468 val_acc:82.7251 val_loss:0.4832
[02/0057] | train_loss:1.4572 val_acc:84.1849 val_loss:0.4564
[02/0058] | train_loss:1.4268 val_acc:83.9416 val_loss:0.4568
[02/0059] | train_loss:1.4361 val_acc:84.9148 val_loss:0.4956
model is saved at epoch 59!![02/0060] | train_loss:1.4303 val_acc:84.6715 val_loss:0.4608
[02/0061] | train_loss:1.4823 val_acc:85.4015 val_loss:0.5133
model is saved at epoch 61!![02/0062] | train_loss:1.5399 val_acc:82.7251 val_loss:0.483
[02/0063] | train_loss:1.528 val_acc:84.9148 val_loss:0.481
[02/0064] | train_loss:1.5123 val_acc:83.9416 val_loss:0.4771
[02/0065] | train_loss:1.5182 val_acc:83.455 val_loss:0.4693
[02/0066] | train_loss:1.5042 val_acc:84.1849 val_loss:0.4742
[02/0067] | train_loss:1.4544 val_acc:84.1849 val_loss:0.4905
[02/0068] | train_loss:1.4587 val_acc:83.9416 val_loss:0.4357
[02/0069] | train_loss:1.4053 val_acc:84.9148 val_loss:0.4764
[02/0070] | train_loss:1.4039 val_acc:84.1849 val_loss:0.4689
[02/0071] | train_loss:1.3825 val_acc:83.9416 val_loss:0.4535
[02/0072] | train_loss:1.396 val_acc:83.9416 val_loss:0.4731
[02/0073] | train_loss:1.3405 val_acc:80.5353 val_loss:0.4879
[02/0074] | train_loss:1.3804 val_acc:84.1849 val_loss:0.47
[02/0075] | train_loss:1.3491 val_acc:85.6448 val_loss:0.5222
model is saved at epoch 75!![02/0076] | train_loss:1.2855 val_acc:86.1314 val_loss:0.513
model is saved at epoch 76!![02/0077] | train_loss:1.3178 val_acc:81.7518 val_loss:0.4848
[02/0078] | train_loss:1.2931 val_acc:83.6983 val_loss:0.5225
[02/0079] | train_loss:1.2628 val_acc:83.6983 val_loss:0.45
[02/0080] | train_loss:1.2568 val_acc:81.7518 val_loss:0.5129
[02/0081] | train_loss:1.2324 val_acc:82.9684 val_loss:0.5221
[02/0082] | train_loss:1.293 val_acc:85.1582 val_loss:0.5328
[02/0083] | train_loss:1.2804 val_acc:82.2384 val_loss:0.5406
[02/0084] | train_loss:1.2409 val_acc:83.6983 val_loss:0.5009
[02/0085] | train_loss:1.166 val_acc:83.9416 val_loss:0.5356
[02/0086] | train_loss:1.1982 val_acc:84.6715 val_loss:0.5417
[02/0087] | train_loss:1.2581 val_acc:83.2117 val_loss:0.533
[02/0088] | train_loss:1.2684 val_acc:86.1314 val_loss:0.6089
[02/0089] | train_loss:1.2544 val_acc:84.6715 val_loss:0.5308
[02/0090] | train_loss:1.2276 val_acc:83.455 val_loss:0.6238
[02/0091] | train_loss:1.2014 val_acc:83.9416 val_loss:0.632
[02/0092] | train_loss:1.1644 val_acc:83.6983 val_loss:0.565
[02/0093] | train_loss:1.1743 val_acc:85.6448 val_loss:0.5281
[02/0094] | train_loss:1.1808 val_acc:84.1849 val_loss:0.5648
[02/0095] | train_loss:1.1615 val_acc:84.9148 val_loss:0.5415
[02/0096] | train_loss:1.1616 val_acc:85.1582 val_loss:0.583
[02/0097] | train_loss:1.1194 val_acc:85.8881 val_loss:0.627
[02/0098] | train_loss:1.1484 val_acc:83.6983 val_loss:0.5255
[02/0099] | train_loss:1.1352 val_acc:86.3747 val_loss:0.5782
model is saved at epoch 99!![02/0100] | train_loss:1.0869 val_acc:85.8881 val_loss:0.5466
[02/0101] | train_loss:1.1141 val_acc:84.4282 val_loss:0.4836
[02/0102] | train_loss:1.0559 val_acc:85.1582 val_loss:0.5545
[02/0103] | train_loss:1.062 val_acc:85.6448 val_loss:0.5443
[02/0104] | train_loss:1.1107 val_acc:84.1849 val_loss:0.5097
[02/0105] | train_loss:1.0773 val_acc:84.1849 val_loss:0.6774
[02/0106] | train_loss:1.0852 val_acc:84.4282 val_loss:0.6171
[02/0107] | train_loss:1.0561 val_acc:84.6715 val_loss:0.6007
[02/0108] | train_loss:0.9859 val_acc:82.9684 val_loss:0.6399
[02/0109] | train_loss:1.0256 val_acc:84.4282 val_loss:0.6031
[02/0110] | train_loss:1.1576 val_acc:84.4282 val_loss:0.579
[02/0111] | train_loss:1.1299 val_acc:82.9684 val_loss:0.617
[02/0112] | train_loss:1.0711 val_acc:82.9684 val_loss:0.591
[02/0113] | train_loss:1.0484 val_acc:84.1849 val_loss:0.5766
[02/0114] | train_loss:1.0327 val_acc:84.6715 val_loss:0.5554
[02/0115] | train_loss:1.1116 val_acc:84.6715 val_loss:0.5478
[02/0116] | train_loss:1.1417 val_acc:85.6448 val_loss:0.526
[02/0117] | train_loss:1.0483 val_acc:85.1582 val_loss:0.5471
[02/0118] | train_loss:1.0121 val_acc:82.9684 val_loss:0.5276
[02/0119] | train_loss:0.9927 val_acc:84.1849 val_loss:0.5642
[02/0120] | train_loss:0.9967 val_acc:82.4818 val_loss:0.4819
[02/0121] | train_loss:1.0208 val_acc:84.6715 val_loss:0.6064
[02/0122] | train_loss:0.9814 val_acc:81.2652 val_loss:0.5801
[02/0123] | train_loss:1.0811 val_acc:82.9684 val_loss:0.5627
[02/0124] | train_loss:1.0226 val_acc:84.1849 val_loss:0.5817
[02/0125] | train_loss:1.0366 val_acc:84.9148 val_loss:0.5377
[02/0126] | train_loss:1.0173 val_acc:84.6715 val_loss:0.5897
[02/0127] | train_loss:0.9912 val_acc:85.1582 val_loss:0.5736
[02/0128] | train_loss:0.9282 val_acc:83.455 val_loss:0.6248
[02/0129] | train_loss:0.9769 val_acc:82.7251 val_loss:0.5082
[02/0130] | train_loss:0.9951 val_acc:84.6715 val_loss:0.5318
[02/0131] | train_loss:0.972 val_acc:82.2384 val_loss:0.57
[02/0132] | train_loss:1.0272 val_acc:83.9416 val_loss:0.5931
[02/0133] | train_loss:1.1294 val_acc:81.9951 val_loss:0.584
[02/0134] | train_loss:1.1988 val_acc:82.9684 val_loss:0.5397
[02/0135] | train_loss:1.1494 val_acc:84.1849 val_loss:0.5996
[02/0136] | train_loss:1.1217 val_acc:84.1849 val_loss:0.4969
[02/0137] | train_loss:1.0854 val_acc:84.9148 val_loss:0.5954
[02/0138] | train_loss:1.0804 val_acc:84.6715 val_loss:0.5498
[02/0139] | train_loss:1.0761 val_acc:84.1849 val_loss:0.5914
[02/0140] | train_loss:1.0739 val_acc:83.6983 val_loss:0.5766
[02/0141] | train_loss:1.0594 val_acc:83.2117 val_loss:0.6602
[02/0142] | train_loss:1.0762 val_acc:83.455 val_loss:0.6962
[02/0143] | train_loss:1.0167 val_acc:85.1582 val_loss:0.6973
[02/0144] | train_loss:0.9954 val_acc:84.6715 val_loss:0.6625
[02/0145] | train_loss:1.0258 val_acc:84.1849 val_loss:0.5384
[02/0146] | train_loss:0.9756 val_acc:82.9684 val_loss:0.6405
[02/0147] | train_loss:0.9926 val_acc:83.6983 val_loss:0.5699
[02/0148] | train_loss:0.9811 val_acc:82.9684 val_loss:0.6376
[02/0149] | train_loss:0.9948 val_acc:84.6715 val_loss:0.6576
[02/0150] | train_loss:0.9837 val_acc:83.9416 val_loss:0.5404
Fold: [2/10] Test is finish !! 
 Test Metrics are: test_acc:81.2652 test_loss:0.6934fold [2/10] is start!!
[03/0001] | train_loss:2.6654 val_acc:53.528 val_loss:0.6577
model is saved at epoch 1!![03/0002] | train_loss:2.507 val_acc:53.7713 val_loss:0.6723
model is saved at epoch 2!![03/0003] | train_loss:2.3968 val_acc:68.6131 val_loss:0.56
model is saved at epoch 3!![03/0004] | train_loss:2.3379 val_acc:71.5328 val_loss:0.5336
model is saved at epoch 4!![03/0005] | train_loss:2.288 val_acc:72.5061 val_loss:0.5324
model is saved at epoch 5!![03/0006] | train_loss:2.2461 val_acc:72.2628 val_loss:0.5147
[03/0007] | train_loss:2.2121 val_acc:72.7494 val_loss:0.5265
model is saved at epoch 7!![03/0008] | train_loss:2.1723 val_acc:69.3431 val_loss:0.5024
[03/0009] | train_loss:2.1537 val_acc:73.4793 val_loss:0.5113
model is saved at epoch 9!![03/0010] | train_loss:2.0856 val_acc:76.8856 val_loss:0.5064
model is saved at epoch 10!![03/0011] | train_loss:2.0994 val_acc:72.0195 val_loss:0.5009
[03/0012] | train_loss:2.0567 val_acc:74.2092 val_loss:0.4991
[03/0013] | train_loss:2.0226 val_acc:73.4793 val_loss:0.5203
[03/0014] | train_loss:2.0289 val_acc:74.6959 val_loss:0.5149
[03/0015] | train_loss:1.9958 val_acc:75.4258 val_loss:0.5062
[03/0016] | train_loss:1.9382 val_acc:74.6959 val_loss:0.5231
[03/0017] | train_loss:1.9598 val_acc:73.9659 val_loss:0.5048
[03/0018] | train_loss:1.9082 val_acc:76.399 val_loss:0.4981
[03/0019] | train_loss:1.9046 val_acc:76.6423 val_loss:0.5104
[03/0020] | train_loss:1.8877 val_acc:75.1825 val_loss:0.5165
[03/0021] | train_loss:1.9177 val_acc:77.129 val_loss:0.5387
model is saved at epoch 21!![03/0022] | train_loss:1.8862 val_acc:77.129 val_loss:0.4775
[03/0023] | train_loss:1.8839 val_acc:76.1557 val_loss:0.4856
[03/0024] | train_loss:1.7812 val_acc:76.6423 val_loss:0.4882
[03/0025] | train_loss:1.8184 val_acc:75.1825 val_loss:0.5093
[03/0026] | train_loss:1.8352 val_acc:77.129 val_loss:0.4904
[03/0027] | train_loss:1.803 val_acc:78.5888 val_loss:0.4922
model is saved at epoch 27!![03/0028] | train_loss:1.7476 val_acc:77.6156 val_loss:0.481
[03/0029] | train_loss:1.7536 val_acc:75.6691 val_loss:0.5428
[03/0030] | train_loss:1.7272 val_acc:77.8589 val_loss:0.4775
[03/0031] | train_loss:1.7105 val_acc:78.5888 val_loss:0.4794
[03/0032] | train_loss:1.6562 val_acc:78.1022 val_loss:0.5006
[03/0033] | train_loss:1.6654 val_acc:77.129 val_loss:0.5511
[03/0034] | train_loss:1.645 val_acc:76.6423 val_loss:0.5383
[03/0035] | train_loss:1.6606 val_acc:78.8321 val_loss:0.5242
model is saved at epoch 35!![03/0036] | train_loss:1.6225 val_acc:78.5888 val_loss:0.4629
[03/0037] | train_loss:1.6564 val_acc:78.1022 val_loss:0.4607
[03/0038] | train_loss:1.6244 val_acc:78.1022 val_loss:0.4807
[03/0039] | train_loss:1.591 val_acc:78.8321 val_loss:0.4909
[03/0040] | train_loss:1.5859 val_acc:78.3455 val_loss:0.4946
[03/0041] | train_loss:1.5911 val_acc:78.1022 val_loss:0.5355
[03/0042] | train_loss:1.5299 val_acc:78.8321 val_loss:0.5027
[03/0043] | train_loss:1.4949 val_acc:79.8054 val_loss:0.5029
model is saved at epoch 43!![03/0044] | train_loss:1.5651 val_acc:77.8589 val_loss:0.541
[03/0045] | train_loss:1.5239 val_acc:79.3187 val_loss:0.5491
[03/0046] | train_loss:1.5474 val_acc:80.5353 val_loss:0.4797
model is saved at epoch 46!![03/0047] | train_loss:1.5014 val_acc:77.8589 val_loss:0.5073
[03/0048] | train_loss:1.4855 val_acc:80.0487 val_loss:0.4717
[03/0049] | train_loss:1.4579 val_acc:79.3187 val_loss:0.5124
[03/0050] | train_loss:1.4708 val_acc:76.1557 val_loss:0.5354
[03/0051] | train_loss:1.4384 val_acc:77.8589 val_loss:0.4727
[03/0052] | train_loss:1.421 val_acc:77.6156 val_loss:0.5698
[03/0053] | train_loss:1.3658 val_acc:78.5888 val_loss:0.515
[03/0054] | train_loss:1.436 val_acc:78.3455 val_loss:0.5286
[03/0055] | train_loss:1.4432 val_acc:78.3455 val_loss:0.5068
[03/0056] | train_loss:1.3479 val_acc:79.3187 val_loss:0.5457
[03/0057] | train_loss:1.3657 val_acc:77.3723 val_loss:0.5352
[03/0058] | train_loss:1.3777 val_acc:79.562 val_loss:0.5365
[03/0059] | train_loss:1.3711 val_acc:78.1022 val_loss:0.5957
[03/0060] | train_loss:1.3762 val_acc:78.5888 val_loss:0.5494
[03/0061] | train_loss:1.2746 val_acc:81.0219 val_loss:0.5746
model is saved at epoch 61!![03/0062] | train_loss:1.2857 val_acc:79.8054 val_loss:0.5538
[03/0063] | train_loss:1.3069 val_acc:79.562 val_loss:0.5543
[03/0064] | train_loss:1.2766 val_acc:77.6156 val_loss:0.6287
[03/0065] | train_loss:1.3082 val_acc:77.3723 val_loss:0.621
[03/0066] | train_loss:1.2798 val_acc:78.1022 val_loss:0.5681
[03/0067] | train_loss:1.2664 val_acc:79.0754 val_loss:0.5236
[03/0068] | train_loss:1.2538 val_acc:79.0754 val_loss:0.5981
[03/0069] | train_loss:1.2309 val_acc:76.8856 val_loss:0.5748
[03/0070] | train_loss:1.2367 val_acc:78.8321 val_loss:0.603
[03/0071] | train_loss:1.1776 val_acc:79.3187 val_loss:0.6101
[03/0072] | train_loss:1.2141 val_acc:78.3455 val_loss:0.5508
[03/0073] | train_loss:1.2098 val_acc:80.5353 val_loss:0.6196
[03/0074] | train_loss:1.2096 val_acc:77.6156 val_loss:0.6103
[03/0075] | train_loss:1.1595 val_acc:81.2652 val_loss:0.5747
model is saved at epoch 75!![03/0076] | train_loss:1.1734 val_acc:77.8589 val_loss:0.6174
[03/0077] | train_loss:1.3701 val_acc:79.8054 val_loss:0.6461
[03/0078] | train_loss:1.331 val_acc:76.6423 val_loss:0.6292
[03/0079] | train_loss:1.2952 val_acc:79.8054 val_loss:0.5856
[03/0080] | train_loss:1.2679 val_acc:78.5888 val_loss:0.6376
[03/0081] | train_loss:1.2433 val_acc:77.6156 val_loss:0.6455
[03/0082] | train_loss:1.2343 val_acc:78.8321 val_loss:0.5878
[03/0083] | train_loss:1.2391 val_acc:78.8321 val_loss:0.64
[03/0084] | train_loss:1.2309 val_acc:78.8321 val_loss:0.6362
[03/0085] | train_loss:1.1899 val_acc:78.5888 val_loss:0.5878
[03/0086] | train_loss:1.1274 val_acc:78.3455 val_loss:0.5762
[03/0087] | train_loss:1.1231 val_acc:78.5888 val_loss:0.5514
[03/0088] | train_loss:1.1084 val_acc:80.0487 val_loss:0.5493
[03/0089] | train_loss:1.1429 val_acc:78.5888 val_loss:0.6142
[03/0090] | train_loss:1.0949 val_acc:77.6156 val_loss:0.539
[03/0091] | train_loss:1.0522 val_acc:78.3455 val_loss:0.63
[03/0092] | train_loss:1.0727 val_acc:78.8321 val_loss:0.6289
[03/0093] | train_loss:1.0919 val_acc:78.3455 val_loss:0.6355
[03/0094] | train_loss:1.0318 val_acc:80.0487 val_loss:0.652
[03/0095] | train_loss:1.0378 val_acc:77.6156 val_loss:0.6759
[03/0096] | train_loss:1.1155 val_acc:77.8589 val_loss:0.6054
[03/0097] | train_loss:1.0224 val_acc:79.562 val_loss:0.5812
[03/0098] | train_loss:0.9617 val_acc:78.3455 val_loss:0.6845
[03/0099] | train_loss:0.9706 val_acc:81.0219 val_loss:0.6653
[03/0100] | train_loss:0.9828 val_acc:80.0487 val_loss:0.7013
[03/0101] | train_loss:0.9779 val_acc:78.8321 val_loss:0.6139
[03/0102] | train_loss:0.9724 val_acc:81.2652 val_loss:0.6991
[03/0103] | train_loss:0.9849 val_acc:80.0487 val_loss:0.6046
[03/0104] | train_loss:0.895 val_acc:79.8054 val_loss:0.6548
[03/0105] | train_loss:0.9542 val_acc:80.292 val_loss:0.6809
[03/0106] | train_loss:0.9423 val_acc:80.5353 val_loss:0.6375
[03/0107] | train_loss:0.932 val_acc:76.8856 val_loss:0.8118
[03/0108] | train_loss:0.97 val_acc:79.8054 val_loss:0.7056
[03/0109] | train_loss:0.9613 val_acc:78.5888 val_loss:0.7189
[03/0110] | train_loss:0.9763 val_acc:79.0754 val_loss:0.8336
[03/0111] | train_loss:0.9554 val_acc:81.9951 val_loss:0.709
model is saved at epoch 111!![03/0112] | train_loss:1.0373 val_acc:78.8321 val_loss:0.6568
[03/0113] | train_loss:0.9726 val_acc:79.8054 val_loss:0.6255
[03/0114] | train_loss:0.9121 val_acc:79.3187 val_loss:0.7346
[03/0115] | train_loss:0.8893 val_acc:80.0487 val_loss:0.7032
[03/0116] | train_loss:0.8954 val_acc:79.0754 val_loss:0.7211
[03/0117] | train_loss:0.9408 val_acc:79.0754 val_loss:0.6421
[03/0118] | train_loss:1.0556 val_acc:79.8054 val_loss:0.6811
[03/0119] | train_loss:0.9768 val_acc:80.5353 val_loss:0.6846
[03/0120] | train_loss:0.9711 val_acc:80.5353 val_loss:0.6655
[03/0121] | train_loss:0.9361 val_acc:79.8054 val_loss:0.81
[03/0122] | train_loss:0.8925 val_acc:81.5085 val_loss:0.6902
[03/0123] | train_loss:0.8633 val_acc:79.562 val_loss:0.7327
[03/0124] | train_loss:0.7964 val_acc:79.8054 val_loss:0.786
[03/0125] | train_loss:0.8111 val_acc:79.3187 val_loss:0.7412
[03/0126] | train_loss:0.834 val_acc:80.5353 val_loss:0.7582
[03/0127] | train_loss:0.8185 val_acc:77.8589 val_loss:0.7034
[03/0128] | train_loss:0.8809 val_acc:80.5353 val_loss:0.6942
[03/0129] | train_loss:0.8799 val_acc:79.562 val_loss:0.7481
[03/0130] | train_loss:0.8492 val_acc:77.3723 val_loss:0.79
[03/0131] | train_loss:0.8418 val_acc:80.0487 val_loss:0.7335
[03/0132] | train_loss:0.8172 val_acc:80.7786 val_loss:0.7506
[03/0133] | train_loss:0.8112 val_acc:79.562 val_loss:0.7958
[03/0134] | train_loss:0.854 val_acc:80.5353 val_loss:0.8197
[03/0135] | train_loss:0.7943 val_acc:79.562 val_loss:0.8347
[03/0136] | train_loss:0.8042 val_acc:80.7786 val_loss:0.836
[03/0137] | train_loss:0.7854 val_acc:80.292 val_loss:0.809
[03/0138] | train_loss:0.8014 val_acc:79.3187 val_loss:0.7637
[03/0139] | train_loss:0.7809 val_acc:80.0487 val_loss:0.8761
[03/0140] | train_loss:0.7974 val_acc:79.0754 val_loss:0.8113
[03/0141] | train_loss:0.7997 val_acc:80.7786 val_loss:0.8065
[03/0142] | train_loss:0.8397 val_acc:80.292 val_loss:0.8607
[03/0143] | train_loss:0.7996 val_acc:77.8589 val_loss:0.8155
[03/0144] | train_loss:0.8224 val_acc:80.7786 val_loss:0.8489
[03/0145] | train_loss:0.771 val_acc:79.0754 val_loss:0.8226
[03/0146] | train_loss:0.7506 val_acc:78.3455 val_loss:0.837
[03/0147] | train_loss:0.8094 val_acc:81.5085 val_loss:0.8158
[03/0148] | train_loss:0.7802 val_acc:79.8054 val_loss:0.8052
[03/0149] | train_loss:0.8122 val_acc:79.562 val_loss:0.7007
[03/0150] | train_loss:0.7525 val_acc:79.8054 val_loss:0.7828
[03/0151] | train_loss:0.7885 val_acc:78.3455 val_loss:0.787
[03/0152] | train_loss:0.8214 val_acc:80.5353 val_loss:0.7723
[03/0153] | train_loss:0.8096 val_acc:81.0219 val_loss:0.7575
[03/0154] | train_loss:0.8253 val_acc:79.562 val_loss:0.7953
[03/0155] | train_loss:0.7542 val_acc:79.0754 val_loss:0.7535
[03/0156] | train_loss:0.7631 val_acc:80.0487 val_loss:0.8136
[03/0157] | train_loss:0.7488 val_acc:79.3187 val_loss:0.86
[03/0158] | train_loss:0.7181 val_acc:79.3187 val_loss:0.8137
[03/0159] | train_loss:0.7468 val_acc:80.292 val_loss:0.7606
[03/0160] | train_loss:0.7117 val_acc:80.5353 val_loss:0.8298
[03/0161] | train_loss:0.7152 val_acc:81.2652 val_loss:0.9007
[03/0162] | train_loss:0.6989 val_acc:79.0754 val_loss:0.9282
Fold: [3/10] Test is finish !! 
 Test Metrics are: test_acc:83.455 test_loss:0.6292fold [3/10] is start!!
[04/0001] | train_loss:2.6428 val_acc:49.1484 val_loss:0.6733
model is saved at epoch 1!![04/0002] | train_loss:2.4437 val_acc:49.8783 val_loss:0.644
model is saved at epoch 2!![04/0003] | train_loss:2.3517 val_acc:58.8808 val_loss:0.5945
model is saved at epoch 3!![04/0004] | train_loss:2.2737 val_acc:72.9927 val_loss:0.5552
model is saved at epoch 4!![04/0005] | train_loss:2.2634 val_acc:70.8029 val_loss:0.5729
[04/0006] | train_loss:2.221 val_acc:75.6691 val_loss:0.5521
model is saved at epoch 6!![04/0007] | train_loss:2.1698 val_acc:75.4258 val_loss:0.5278
[04/0008] | train_loss:2.1365 val_acc:74.6959 val_loss:0.5499
[04/0009] | train_loss:2.0811 val_acc:75.6691 val_loss:0.5173
[04/0010] | train_loss:2.043 val_acc:74.2092 val_loss:0.496
[04/0011] | train_loss:2.0394 val_acc:72.2628 val_loss:0.5927
[04/0012] | train_loss:2.0448 val_acc:77.6156 val_loss:0.4997
model is saved at epoch 12!![04/0013] | train_loss:2.0197 val_acc:75.1825 val_loss:0.483
[04/0014] | train_loss:1.9872 val_acc:76.8856 val_loss:0.4716
[04/0015] | train_loss:1.896 val_acc:74.9392 val_loss:0.4936
[04/0016] | train_loss:1.9162 val_acc:79.562 val_loss:0.4971
model is saved at epoch 16!![04/0017] | train_loss:1.904 val_acc:77.3723 val_loss:0.48
[04/0018] | train_loss:1.8424 val_acc:77.6156 val_loss:0.4768
[04/0019] | train_loss:1.8452 val_acc:76.399 val_loss:0.51
[04/0020] | train_loss:1.903 val_acc:78.5888 val_loss:0.4829
[04/0021] | train_loss:1.7825 val_acc:78.1022 val_loss:0.4741
[04/0022] | train_loss:1.7614 val_acc:76.8856 val_loss:0.5146
[04/0023] | train_loss:1.7859 val_acc:78.1022 val_loss:0.4815
[04/0024] | train_loss:1.776 val_acc:77.129 val_loss:0.4848
[04/0025] | train_loss:1.7245 val_acc:80.5353 val_loss:0.4656
model is saved at epoch 25!![04/0026] | train_loss:1.7092 val_acc:77.8589 val_loss:0.4981
[04/0027] | train_loss:1.7278 val_acc:78.1022 val_loss:0.4964
[04/0028] | train_loss:1.6958 val_acc:76.8856 val_loss:0.496
[04/0029] | train_loss:1.7567 val_acc:76.6423 val_loss:0.4972
[04/0030] | train_loss:1.7486 val_acc:80.7786 val_loss:0.4987
model is saved at epoch 30!![04/0031] | train_loss:1.6909 val_acc:79.8054 val_loss:0.491
[04/0032] | train_loss:1.6159 val_acc:76.1557 val_loss:0.4779
[04/0033] | train_loss:1.6471 val_acc:79.8054 val_loss:0.4866
[04/0034] | train_loss:1.6387 val_acc:77.6156 val_loss:0.5004
[04/0035] | train_loss:1.6144 val_acc:80.292 val_loss:0.4749
[04/0036] | train_loss:1.5744 val_acc:80.7786 val_loss:0.4878
[04/0037] | train_loss:1.552 val_acc:79.0754 val_loss:0.4621
[04/0038] | train_loss:1.5396 val_acc:79.8054 val_loss:0.4893
[04/0039] | train_loss:1.5108 val_acc:78.5888 val_loss:0.4523
[04/0040] | train_loss:1.5235 val_acc:78.1022 val_loss:0.4596
[04/0041] | train_loss:1.4698 val_acc:81.0219 val_loss:0.4694
model is saved at epoch 41!![04/0042] | train_loss:1.4545 val_acc:79.562 val_loss:0.5322
[04/0043] | train_loss:1.5321 val_acc:81.2652 val_loss:0.4853
model is saved at epoch 43!![04/0044] | train_loss:1.4748 val_acc:79.0754 val_loss:0.4591
[04/0045] | train_loss:1.4615 val_acc:80.292 val_loss:0.483
[04/0046] | train_loss:1.4253 val_acc:81.0219 val_loss:0.4923
[04/0047] | train_loss:1.427 val_acc:78.5888 val_loss:0.504
[04/0048] | train_loss:1.4265 val_acc:77.6156 val_loss:0.486
[04/0049] | train_loss:1.3885 val_acc:79.3187 val_loss:0.4709
[04/0050] | train_loss:1.3522 val_acc:79.8054 val_loss:0.5032
[04/0051] | train_loss:1.3623 val_acc:80.292 val_loss:0.4568
[04/0052] | train_loss:1.3937 val_acc:80.0487 val_loss:0.5061
[04/0053] | train_loss:1.3683 val_acc:79.562 val_loss:0.5243
[04/0054] | train_loss:1.3652 val_acc:80.292 val_loss:0.5044
[04/0055] | train_loss:1.3691 val_acc:79.0754 val_loss:0.486
[04/0056] | train_loss:1.2955 val_acc:78.1022 val_loss:0.5183
[04/0057] | train_loss:1.3706 val_acc:79.562 val_loss:0.5379
[04/0058] | train_loss:1.4539 val_acc:81.9951 val_loss:0.4896
model is saved at epoch 58!![04/0059] | train_loss:1.3157 val_acc:79.562 val_loss:0.5112
[04/0060] | train_loss:1.2703 val_acc:81.9951 val_loss:0.4714
[04/0061] | train_loss:1.2524 val_acc:80.7786 val_loss:0.4983
[04/0062] | train_loss:1.2185 val_acc:81.9951 val_loss:0.5224
[04/0063] | train_loss:1.2168 val_acc:81.9951 val_loss:0.5097
[04/0064] | train_loss:1.2332 val_acc:81.0219 val_loss:0.529
[04/0065] | train_loss:1.2632 val_acc:81.0219 val_loss:0.5094
[04/0066] | train_loss:1.265 val_acc:79.0754 val_loss:0.5183
[04/0067] | train_loss:1.2347 val_acc:78.8321 val_loss:0.5506
[04/0068] | train_loss:1.223 val_acc:78.8321 val_loss:0.5476
[04/0069] | train_loss:1.1773 val_acc:80.292 val_loss:0.6043
[04/0070] | train_loss:1.1848 val_acc:79.8054 val_loss:0.5884
[04/0071] | train_loss:1.1624 val_acc:80.292 val_loss:0.5459
[04/0072] | train_loss:1.1336 val_acc:83.2117 val_loss:0.5993
model is saved at epoch 72!![04/0073] | train_loss:1.1595 val_acc:80.292 val_loss:0.595
[04/0074] | train_loss:1.1169 val_acc:83.2117 val_loss:0.6195
[04/0075] | train_loss:1.1982 val_acc:79.562 val_loss:0.5915
[04/0076] | train_loss:1.2174 val_acc:81.7518 val_loss:0.5035
[04/0077] | train_loss:1.1202 val_acc:81.7518 val_loss:0.5489
[04/0078] | train_loss:1.0937 val_acc:81.2652 val_loss:0.5192
[04/0079] | train_loss:1.1658 val_acc:80.5353 val_loss:0.5623
[04/0080] | train_loss:1.0876 val_acc:81.2652 val_loss:0.5715
[04/0081] | train_loss:1.0921 val_acc:81.0219 val_loss:0.5677
[04/0082] | train_loss:1.0414 val_acc:81.9951 val_loss:0.6094
[04/0083] | train_loss:1.0278 val_acc:81.5085 val_loss:0.633
[04/0084] | train_loss:1.0462 val_acc:81.9951 val_loss:0.6908
[04/0085] | train_loss:1.0325 val_acc:81.0219 val_loss:0.5772
[04/0086] | train_loss:1.0712 val_acc:80.7786 val_loss:0.5611
[04/0087] | train_loss:1.0459 val_acc:82.2384 val_loss:0.6436
[04/0088] | train_loss:1.1507 val_acc:79.8054 val_loss:0.6555
[04/0089] | train_loss:1.1073 val_acc:80.0487 val_loss:0.5943
[04/0090] | train_loss:1.1983 val_acc:80.7786 val_loss:0.5324
[04/0091] | train_loss:1.1531 val_acc:83.2117 val_loss:0.5424
[04/0092] | train_loss:1.1228 val_acc:82.4818 val_loss:0.6272
[04/0093] | train_loss:1.1039 val_acc:80.292 val_loss:0.6046
[04/0094] | train_loss:1.1101 val_acc:82.4818 val_loss:0.5727
[04/0095] | train_loss:1.0991 val_acc:82.9684 val_loss:0.6408
[04/0096] | train_loss:1.0582 val_acc:81.5085 val_loss:0.5361
[04/0097] | train_loss:1.0467 val_acc:81.0219 val_loss:0.5856
[04/0098] | train_loss:1.0131 val_acc:81.7518 val_loss:0.6052
[04/0099] | train_loss:1.0659 val_acc:81.9951 val_loss:0.5725
[04/0100] | train_loss:1.0431 val_acc:79.8054 val_loss:0.6352
[04/0101] | train_loss:1.0429 val_acc:79.562 val_loss:0.532
[04/0102] | train_loss:1.0831 val_acc:81.0219 val_loss:0.7032
[04/0103] | train_loss:1.0462 val_acc:80.5353 val_loss:0.6059
[04/0104] | train_loss:1.0168 val_acc:80.7786 val_loss:0.6482
[04/0105] | train_loss:1.0252 val_acc:82.2384 val_loss:0.5774
[04/0106] | train_loss:1.0673 val_acc:80.7786 val_loss:0.6859
[04/0107] | train_loss:1.0712 val_acc:82.4818 val_loss:0.5654
[04/0108] | train_loss:1.0092 val_acc:81.5085 val_loss:0.6808
[04/0109] | train_loss:1.0445 val_acc:81.2652 val_loss:0.6026
[04/0110] | train_loss:1.0272 val_acc:82.2384 val_loss:0.5765
[04/0111] | train_loss:0.9758 val_acc:82.7251 val_loss:0.6883
[04/0112] | train_loss:0.9217 val_acc:81.9951 val_loss:0.6537
[04/0113] | train_loss:1.0089 val_acc:80.7786 val_loss:0.6362
[04/0114] | train_loss:0.9721 val_acc:82.2384 val_loss:0.6864
[04/0115] | train_loss:0.9545 val_acc:81.5085 val_loss:0.7391
[04/0116] | train_loss:0.9583 val_acc:81.9951 val_loss:0.7337
[04/0117] | train_loss:0.9894 val_acc:81.7518 val_loss:0.6631
[04/0118] | train_loss:0.9856 val_acc:81.9951 val_loss:0.6792
[04/0119] | train_loss:0.93 val_acc:81.5085 val_loss:0.6839
[04/0120] | train_loss:0.9218 val_acc:82.9684 val_loss:0.7439
[04/0121] | train_loss:0.9068 val_acc:78.8321 val_loss:0.6886
[04/0122] | train_loss:0.9655 val_acc:83.2117 val_loss:0.6574
[04/0123] | train_loss:0.9847 val_acc:79.562 val_loss:0.7802
Fold: [4/10] Test is finish !! 
 Test Metrics are: test_acc:80.292 test_loss:0.5384fold [4/10] is start!!
[05/0001] | train_loss:2.6124 val_acc:49.3917 val_loss:0.6707
model is saved at epoch 1!![05/0002] | train_loss:2.4958 val_acc:49.8783 val_loss:0.6585
model is saved at epoch 2!![05/0003] | train_loss:2.4113 val_acc:69.3431 val_loss:0.6247
model is saved at epoch 3!![05/0004] | train_loss:2.3585 val_acc:71.2895 val_loss:0.62
model is saved at epoch 4!![05/0005] | train_loss:2.2982 val_acc:73.236 val_loss:0.573
model is saved at epoch 5!![05/0006] | train_loss:2.263 val_acc:71.5328 val_loss:0.5907
[05/0007] | train_loss:2.2311 val_acc:68.1265 val_loss:0.6474
[05/0008] | train_loss:2.1685 val_acc:71.0462 val_loss:0.6377
[05/0009] | train_loss:2.1438 val_acc:73.9659 val_loss:0.5333
model is saved at epoch 9!![05/0010] | train_loss:2.1389 val_acc:75.6691 val_loss:0.6105
model is saved at epoch 10!![05/0011] | train_loss:2.1704 val_acc:73.4793 val_loss:0.5276
[05/0012] | train_loss:2.0843 val_acc:75.1825 val_loss:0.582
[05/0013] | train_loss:2.1088 val_acc:74.2092 val_loss:0.5109
[05/0014] | train_loss:2.0426 val_acc:72.5061 val_loss:0.5836
[05/0015] | train_loss:2.0268 val_acc:72.5061 val_loss:0.5054
[05/0016] | train_loss:2.0427 val_acc:74.9392 val_loss:0.5443
[05/0017] | train_loss:2.0156 val_acc:73.7226 val_loss:0.4997
[05/0018] | train_loss:1.9411 val_acc:76.8856 val_loss:0.4937
model is saved at epoch 18!![05/0019] | train_loss:1.9357 val_acc:77.129 val_loss:0.4783
model is saved at epoch 19!![05/0020] | train_loss:1.939 val_acc:76.6423 val_loss:0.4898
[05/0021] | train_loss:1.8812 val_acc:75.4258 val_loss:0.4974
[05/0022] | train_loss:1.9189 val_acc:76.8856 val_loss:0.4622
[05/0023] | train_loss:1.9278 val_acc:74.6959 val_loss:0.4736
[05/0024] | train_loss:1.8588 val_acc:77.6156 val_loss:0.4591
model is saved at epoch 24!![05/0025] | train_loss:1.8731 val_acc:76.1557 val_loss:0.5182
[05/0026] | train_loss:1.8857 val_acc:78.1022 val_loss:0.4671
model is saved at epoch 26!![05/0027] | train_loss:1.803 val_acc:77.129 val_loss:0.4831
[05/0028] | train_loss:1.8008 val_acc:79.3187 val_loss:0.4562
model is saved at epoch 28!![05/0029] | train_loss:1.7846 val_acc:77.6156 val_loss:0.4811
[05/0030] | train_loss:1.7398 val_acc:77.8589 val_loss:0.4739
[05/0031] | train_loss:1.7913 val_acc:75.4258 val_loss:0.518
[05/0032] | train_loss:1.7801 val_acc:77.3723 val_loss:0.4787
[05/0033] | train_loss:1.7332 val_acc:75.4258 val_loss:0.5076
[05/0034] | train_loss:1.8095 val_acc:76.6423 val_loss:0.5017
[05/0035] | train_loss:1.8296 val_acc:74.6959 val_loss:0.4757
[05/0036] | train_loss:1.8103 val_acc:73.236 val_loss:0.5764
[05/0037] | train_loss:1.8522 val_acc:77.3723 val_loss:0.4859
[05/0038] | train_loss:1.7289 val_acc:76.6423 val_loss:0.4712
[05/0039] | train_loss:1.7241 val_acc:79.0754 val_loss:0.4587
[05/0040] | train_loss:1.6949 val_acc:79.8054 val_loss:0.4724
model is saved at epoch 40!![05/0041] | train_loss:1.6326 val_acc:77.3723 val_loss:0.4709
[05/0042] | train_loss:1.6501 val_acc:77.8589 val_loss:0.4735
[05/0043] | train_loss:1.6476 val_acc:76.8856 val_loss:0.4847
[05/0044] | train_loss:1.6629 val_acc:79.562 val_loss:0.4707
[05/0045] | train_loss:1.6375 val_acc:78.1022 val_loss:0.4845
[05/0046] | train_loss:1.6231 val_acc:78.8321 val_loss:0.4604
[05/0047] | train_loss:1.633 val_acc:77.8589 val_loss:0.4751
[05/0048] | train_loss:1.6077 val_acc:77.8589 val_loss:0.5463
[05/0049] | train_loss:1.6618 val_acc:76.6423 val_loss:0.4971
[05/0050] | train_loss:1.6168 val_acc:79.0754 val_loss:0.4829
[05/0051] | train_loss:1.5566 val_acc:77.6156 val_loss:0.4676
[05/0052] | train_loss:1.5246 val_acc:79.3187 val_loss:0.4971
[05/0053] | train_loss:1.5502 val_acc:79.562 val_loss:0.4585
[05/0054] | train_loss:1.5095 val_acc:77.3723 val_loss:0.4986
[05/0055] | train_loss:1.4731 val_acc:76.1557 val_loss:0.5191
[05/0056] | train_loss:1.4759 val_acc:76.8856 val_loss:0.5074
[05/0057] | train_loss:1.5322 val_acc:79.0754 val_loss:0.5132
[05/0058] | train_loss:1.449 val_acc:79.0754 val_loss:0.4861
[05/0059] | train_loss:1.4806 val_acc:78.8321 val_loss:0.4806
[05/0060] | train_loss:1.3909 val_acc:78.8321 val_loss:0.5183
[05/0061] | train_loss:1.4114 val_acc:78.1022 val_loss:0.5828
[05/0062] | train_loss:1.4391 val_acc:80.7786 val_loss:0.5585
model is saved at epoch 62!![05/0063] | train_loss:1.3852 val_acc:77.3723 val_loss:0.55
[05/0064] | train_loss:1.3577 val_acc:78.3455 val_loss:0.617
[05/0065] | train_loss:1.4059 val_acc:78.3455 val_loss:0.6159
[05/0066] | train_loss:1.4029 val_acc:78.8321 val_loss:0.5547
[05/0067] | train_loss:1.3693 val_acc:78.3455 val_loss:0.5775
[05/0068] | train_loss:1.3758 val_acc:75.4258 val_loss:0.5977
[05/0069] | train_loss:1.4637 val_acc:77.6156 val_loss:0.5441
[05/0070] | train_loss:1.3811 val_acc:78.1022 val_loss:0.5712
[05/0071] | train_loss:1.3083 val_acc:77.6156 val_loss:0.5959
[05/0072] | train_loss:1.3312 val_acc:79.3187 val_loss:0.5272
[05/0073] | train_loss:1.2994 val_acc:79.0754 val_loss:0.4933
[05/0074] | train_loss:1.3285 val_acc:76.8856 val_loss:0.5013
[05/0075] | train_loss:1.3234 val_acc:79.562 val_loss:0.6072
[05/0076] | train_loss:1.332 val_acc:79.562 val_loss:0.5358
[05/0077] | train_loss:1.4192 val_acc:78.3455 val_loss:0.7002
[05/0078] | train_loss:1.3775 val_acc:81.2652 val_loss:0.951
model is saved at epoch 78!![05/0079] | train_loss:1.3322 val_acc:77.6156 val_loss:0.561
[05/0080] | train_loss:1.4586 val_acc:79.0754 val_loss:0.615
[05/0081] | train_loss:1.3786 val_acc:80.0487 val_loss:0.5638
[05/0082] | train_loss:1.3103 val_acc:80.5353 val_loss:0.4998
[05/0083] | train_loss:1.2986 val_acc:77.8589 val_loss:0.7217
[05/0084] | train_loss:1.3073 val_acc:79.562 val_loss:0.6777
[05/0085] | train_loss:1.1952 val_acc:79.8054 val_loss:0.6873
[05/0086] | train_loss:1.1835 val_acc:79.562 val_loss:0.5821
[05/0087] | train_loss:1.1373 val_acc:79.3187 val_loss:0.6745
[05/0088] | train_loss:1.2063 val_acc:79.8054 val_loss:0.5656
[05/0089] | train_loss:1.2065 val_acc:79.8054 val_loss:0.5396
[05/0090] | train_loss:1.1597 val_acc:78.3455 val_loss:0.5784
[05/0091] | train_loss:1.1377 val_acc:77.8589 val_loss:0.565
[05/0092] | train_loss:1.1529 val_acc:79.0754 val_loss:0.6498
[05/0093] | train_loss:1.0971 val_acc:80.0487 val_loss:0.5635
[05/0094] | train_loss:1.1578 val_acc:79.8054 val_loss:0.6
[05/0095] | train_loss:1.2256 val_acc:81.0219 val_loss:0.5807
[05/0096] | train_loss:1.3367 val_acc:79.3187 val_loss:0.5585
[05/0097] | train_loss:1.2753 val_acc:79.562 val_loss:0.5229
[05/0098] | train_loss:1.2429 val_acc:76.8856 val_loss:0.5547
[05/0099] | train_loss:1.2648 val_acc:79.8054 val_loss:0.5199
[05/0100] | train_loss:1.2082 val_acc:78.3455 val_loss:0.5157
[05/0101] | train_loss:1.1402 val_acc:80.5353 val_loss:0.5556
[05/0102] | train_loss:1.1862 val_acc:78.5888 val_loss:0.5942
[05/0103] | train_loss:1.2131 val_acc:80.0487 val_loss:0.531
[05/0104] | train_loss:1.2313 val_acc:78.3455 val_loss:0.5802
[05/0105] | train_loss:1.1649 val_acc:78.5888 val_loss:0.568
[05/0106] | train_loss:1.1443 val_acc:79.8054 val_loss:0.5311
[05/0107] | train_loss:1.1035 val_acc:81.2652 val_loss:0.5979
[05/0108] | train_loss:1.0569 val_acc:80.292 val_loss:0.5559
[05/0109] | train_loss:1.0911 val_acc:80.0487 val_loss:0.5688
[05/0110] | train_loss:1.0592 val_acc:80.292 val_loss:0.5329
[05/0111] | train_loss:1.0349 val_acc:80.0487 val_loss:0.5866
[05/0112] | train_loss:1.0898 val_acc:78.5888 val_loss:0.5984
[05/0113] | train_loss:1.0396 val_acc:81.0219 val_loss:0.587
[05/0114] | train_loss:1.0578 val_acc:79.0754 val_loss:0.59
[05/0115] | train_loss:1.0623 val_acc:79.562 val_loss:0.6023
[05/0116] | train_loss:1.0505 val_acc:77.8589 val_loss:0.5691
[05/0117] | train_loss:1.065 val_acc:81.2652 val_loss:0.5899
[05/0118] | train_loss:1.0194 val_acc:78.5888 val_loss:0.6388
[05/0119] | train_loss:1.0293 val_acc:82.2384 val_loss:0.6384
model is saved at epoch 119!![05/0120] | train_loss:1.0225 val_acc:80.5353 val_loss:0.5667
[05/0121] | train_loss:0.9551 val_acc:80.5353 val_loss:0.5458
[05/0122] | train_loss:0.9664 val_acc:79.8054 val_loss:0.5849
[05/0123] | train_loss:1.0472 val_acc:81.9951 val_loss:0.5888
[05/0124] | train_loss:1.0287 val_acc:82.2384 val_loss:0.6123
[05/0125] | train_loss:1.0021 val_acc:81.2652 val_loss:0.6416
[05/0126] | train_loss:1.0239 val_acc:80.7786 val_loss:0.5864
[05/0127] | train_loss:0.9609 val_acc:81.9951 val_loss:0.5632
[05/0128] | train_loss:0.9723 val_acc:80.292 val_loss:0.5785
[05/0129] | train_loss:0.9304 val_acc:81.2652 val_loss:0.6167
[05/0130] | train_loss:0.9258 val_acc:82.4818 val_loss:0.6694
model is saved at epoch 130!![05/0131] | train_loss:0.8657 val_acc:80.292 val_loss:0.6054
[05/0132] | train_loss:0.8717 val_acc:81.0219 val_loss:0.6066
[05/0133] | train_loss:0.8569 val_acc:81.0219 val_loss:0.6436
[05/0134] | train_loss:0.8792 val_acc:82.4818 val_loss:0.7395
[05/0135] | train_loss:0.8931 val_acc:80.292 val_loss:0.591
[05/0136] | train_loss:0.871 val_acc:81.9951 val_loss:0.5722
[05/0137] | train_loss:0.885 val_acc:82.2384 val_loss:0.6242
[05/0138] | train_loss:0.9578 val_acc:80.0487 val_loss:0.6064
[05/0139] | train_loss:0.897 val_acc:80.292 val_loss:0.6111
[05/0140] | train_loss:0.9349 val_acc:80.0487 val_loss:0.5698
[05/0141] | train_loss:0.9197 val_acc:82.2384 val_loss:0.6199
[05/0142] | train_loss:0.9037 val_acc:80.0487 val_loss:0.6612
[05/0143] | train_loss:0.9034 val_acc:80.292 val_loss:0.6153
[05/0144] | train_loss:0.8964 val_acc:82.9684 val_loss:0.6428
model is saved at epoch 144!![05/0145] | train_loss:0.8472 val_acc:80.5353 val_loss:0.6249
[05/0146] | train_loss:0.8319 val_acc:82.2384 val_loss:0.6675
[05/0147] | train_loss:0.8078 val_acc:81.7518 val_loss:0.7161
[05/0148] | train_loss:0.8729 val_acc:80.7786 val_loss:0.6037
[05/0149] | train_loss:0.8736 val_acc:81.2652 val_loss:0.6442
[05/0150] | train_loss:0.804 val_acc:79.8054 val_loss:0.6691
[05/0151] | train_loss:0.8237 val_acc:80.7786 val_loss:0.7209
[05/0152] | train_loss:0.8351 val_acc:81.5085 val_loss:0.6708
[05/0153] | train_loss:0.8338 val_acc:82.2384 val_loss:0.7082
[05/0154] | train_loss:0.8289 val_acc:80.292 val_loss:0.6797
[05/0155] | train_loss:0.8036 val_acc:80.0487 val_loss:0.7241
[05/0156] | train_loss:0.8544 val_acc:81.2652 val_loss:0.69
[05/0157] | train_loss:0.8073 val_acc:80.5353 val_loss:0.7492
[05/0158] | train_loss:0.7967 val_acc:81.9951 val_loss:0.8042
[05/0159] | train_loss:0.8035 val_acc:81.5085 val_loss:0.8255
[05/0160] | train_loss:0.7964 val_acc:80.5353 val_loss:0.7573
[05/0161] | train_loss:0.8519 val_acc:80.292 val_loss:0.6913
[05/0162] | train_loss:0.853 val_acc:80.5353 val_loss:0.6818
[05/0163] | train_loss:0.8853 val_acc:78.1022 val_loss:0.6946
[05/0164] | train_loss:0.9538 val_acc:79.8054 val_loss:0.7507
[05/0165] | train_loss:0.9462 val_acc:80.292 val_loss:0.5807
[05/0166] | train_loss:0.8778 val_acc:80.7786 val_loss:0.6488
[05/0167] | train_loss:0.9152 val_acc:80.7786 val_loss:0.7217
[05/0168] | train_loss:0.8915 val_acc:83.2117 val_loss:0.6994
model is saved at epoch 168!![05/0169] | train_loss:0.8427 val_acc:79.8054 val_loss:0.6871
[05/0170] | train_loss:0.7871 val_acc:81.9951 val_loss:0.7019
[05/0171] | train_loss:0.8568 val_acc:81.2652 val_loss:0.6625
[05/0172] | train_loss:0.823 val_acc:79.3187 val_loss:0.6979
[05/0173] | train_loss:0.7879 val_acc:80.5353 val_loss:0.7578
[05/0174] | train_loss:0.8243 val_acc:79.562 val_loss:0.6671
[05/0175] | train_loss:0.8539 val_acc:81.2652 val_loss:0.7545
[05/0176] | train_loss:0.79 val_acc:81.9951 val_loss:0.7366
[05/0177] | train_loss:0.7236 val_acc:80.0487 val_loss:0.7311
[05/0178] | train_loss:0.7632 val_acc:81.9951 val_loss:0.8889
[05/0179] | train_loss:0.746 val_acc:80.0487 val_loss:0.8501
[05/0180] | train_loss:0.8543 val_acc:81.9951 val_loss:0.7205
[05/0181] | train_loss:0.8012 val_acc:79.8054 val_loss:0.706
[05/0182] | train_loss:0.7978 val_acc:80.0487 val_loss:0.6939
[05/0183] | train_loss:0.7452 val_acc:80.7786 val_loss:0.7337
[05/0184] | train_loss:0.7604 val_acc:82.7251 val_loss:0.7127
[05/0185] | train_loss:0.7338 val_acc:81.5085 val_loss:0.7846
[05/0186] | train_loss:0.7603 val_acc:80.0487 val_loss:0.7872
[05/0187] | train_loss:0.7557 val_acc:79.8054 val_loss:0.6533
[05/0188] | train_loss:0.7474 val_acc:81.2652 val_loss:0.7244
[05/0189] | train_loss:0.6643 val_acc:80.7786 val_loss:0.7791
[05/0190] | train_loss:0.7102 val_acc:81.2652 val_loss:0.829
[05/0191] | train_loss:0.7014 val_acc:81.0219 val_loss:0.7579
[05/0192] | train_loss:0.7359 val_acc:81.7518 val_loss:0.8171
[05/0193] | train_loss:0.6903 val_acc:80.5353 val_loss:0.7148
[05/0194] | train_loss:0.8636 val_acc:80.292 val_loss:0.8554
[05/0195] | train_loss:0.8472 val_acc:79.562 val_loss:0.7499
[05/0196] | train_loss:0.8036 val_acc:82.2384 val_loss:0.6975
[05/0197] | train_loss:0.7541 val_acc:80.5353 val_loss:0.7004
[05/0198] | train_loss:0.7239 val_acc:78.1022 val_loss:0.784
[05/0199] | train_loss:0.7691 val_acc:81.2652 val_loss:0.7035
[05/0200] | train_loss:0.7654 val_acc:79.8054 val_loss:0.7228
[05/0201] | train_loss:0.8055 val_acc:79.0754 val_loss:0.6794
[05/0202] | train_loss:0.7217 val_acc:81.7518 val_loss:0.7135
[05/0203] | train_loss:0.6972 val_acc:81.5085 val_loss:0.7218
[05/0204] | train_loss:0.6667 val_acc:81.0219 val_loss:0.7282
[05/0205] | train_loss:0.6512 val_acc:81.2652 val_loss:0.8079
[05/0206] | train_loss:0.7719 val_acc:79.0754 val_loss:0.7491
[05/0207] | train_loss:0.8109 val_acc:79.8054 val_loss:0.7652
[05/0208] | train_loss:0.753 val_acc:81.0219 val_loss:0.7111
[05/0209] | train_loss:0.7416 val_acc:81.5085 val_loss:0.719
[05/0210] | train_loss:0.7356 val_acc:80.7786 val_loss:0.7926
[05/0211] | train_loss:0.6902 val_acc:81.7518 val_loss:0.7343
[05/0212] | train_loss:0.6509 val_acc:81.7518 val_loss:0.8264
[05/0213] | train_loss:0.7316 val_acc:80.7786 val_loss:0.7893
[05/0214] | train_loss:0.7443 val_acc:80.292 val_loss:0.811
[05/0215] | train_loss:0.7648 val_acc:79.8054 val_loss:0.7517
[05/0216] | train_loss:0.6497 val_acc:81.0219 val_loss:0.7065
[05/0217] | train_loss:0.6402 val_acc:79.8054 val_loss:0.8323
[05/0218] | train_loss:0.6412 val_acc:80.0487 val_loss:0.8279
[05/0219] | train_loss:0.615 val_acc:81.0219 val_loss:0.9077
Fold: [5/10] Test is finish !! 
 Test Metrics are: test_acc:83.455 test_loss:0.6593fold [5/10] is start!!
[06/0001] | train_loss:2.6801 val_acc:51.0949 val_loss:0.6681
model is saved at epoch 1!![06/0002] | train_loss:2.5335 val_acc:51.0949 val_loss:0.7764
[06/0003] | train_loss:2.4508 val_acc:64.7202 val_loss:0.5675
model is saved at epoch 3!![06/0004] | train_loss:2.3938 val_acc:73.236 val_loss:0.536
model is saved at epoch 4!![06/0005] | train_loss:2.3498 val_acc:74.4526 val_loss:0.5077
model is saved at epoch 5!![06/0006] | train_loss:2.3006 val_acc:76.8856 val_loss:0.5352
model is saved at epoch 6!![06/0007] | train_loss:2.282 val_acc:74.9392 val_loss:0.5382
[06/0008] | train_loss:2.2187 val_acc:78.1022 val_loss:0.4873
model is saved at epoch 8!![06/0009] | train_loss:2.2063 val_acc:77.6156 val_loss:0.4747
[06/0010] | train_loss:2.2093 val_acc:77.129 val_loss:0.4912
[06/0011] | train_loss:2.1608 val_acc:74.6959 val_loss:0.53
[06/0012] | train_loss:2.1351 val_acc:79.3187 val_loss:0.4775
model is saved at epoch 12!![06/0013] | train_loss:2.1134 val_acc:76.8856 val_loss:0.479
[06/0014] | train_loss:2.1012 val_acc:78.1022 val_loss:0.4688
[06/0015] | train_loss:2.1017 val_acc:78.3455 val_loss:0.4523
[06/0016] | train_loss:2.0574 val_acc:78.5888 val_loss:0.4536
[06/0017] | train_loss:2.0185 val_acc:77.6156 val_loss:0.4533
[06/0018] | train_loss:2.0315 val_acc:79.3187 val_loss:0.4344
[06/0019] | train_loss:1.9955 val_acc:76.8856 val_loss:0.472
[06/0020] | train_loss:1.993 val_acc:75.6691 val_loss:0.5067
[06/0021] | train_loss:1.9698 val_acc:77.6156 val_loss:0.4521
[06/0022] | train_loss:1.9308 val_acc:78.5888 val_loss:0.4351
[06/0023] | train_loss:1.9422 val_acc:79.3187 val_loss:0.4327
[06/0024] | train_loss:1.9485 val_acc:78.5888 val_loss:0.4539
[06/0025] | train_loss:1.9393 val_acc:75.6691 val_loss:0.4747
[06/0026] | train_loss:1.9583 val_acc:76.8856 val_loss:0.442
[06/0027] | train_loss:1.9401 val_acc:79.3187 val_loss:0.4161
[06/0028] | train_loss:1.9295 val_acc:80.0487 val_loss:0.4164
model is saved at epoch 28!![06/0029] | train_loss:1.8953 val_acc:80.7786 val_loss:0.4206
model is saved at epoch 29!![06/0030] | train_loss:1.9177 val_acc:79.8054 val_loss:0.4313
[06/0031] | train_loss:1.8251 val_acc:81.2652 val_loss:0.4035
model is saved at epoch 31!![06/0032] | train_loss:1.7965 val_acc:79.8054 val_loss:0.4395
[06/0033] | train_loss:1.8454 val_acc:81.2652 val_loss:0.4086
[06/0034] | train_loss:1.7845 val_acc:80.7786 val_loss:0.4077
[06/0035] | train_loss:1.763 val_acc:81.7518 val_loss:0.4058
model is saved at epoch 35!![06/0036] | train_loss:1.7683 val_acc:82.4818 val_loss:0.4004
model is saved at epoch 36!![06/0037] | train_loss:1.7169 val_acc:80.0487 val_loss:0.3991
[06/0038] | train_loss:1.6655 val_acc:81.9951 val_loss:0.4006
[06/0039] | train_loss:1.7087 val_acc:81.7518 val_loss:0.3999
[06/0040] | train_loss:1.6376 val_acc:81.2652 val_loss:0.4059
[06/0041] | train_loss:1.6816 val_acc:83.6983 val_loss:0.3953
model is saved at epoch 41!![06/0042] | train_loss:1.676 val_acc:80.7786 val_loss:0.4298
[06/0043] | train_loss:1.6688 val_acc:81.9951 val_loss:0.4295
[06/0044] | train_loss:1.6104 val_acc:79.0754 val_loss:0.4407
[06/0045] | train_loss:1.6676 val_acc:81.7518 val_loss:0.4105
[06/0046] | train_loss:1.6188 val_acc:80.5353 val_loss:0.4146
[06/0047] | train_loss:1.6205 val_acc:81.9951 val_loss:0.4023
[06/0048] | train_loss:1.5697 val_acc:82.9684 val_loss:0.3941
[06/0049] | train_loss:1.535 val_acc:82.9684 val_loss:0.4164
[06/0050] | train_loss:1.5817 val_acc:81.9951 val_loss:0.3874
[06/0051] | train_loss:1.5143 val_acc:82.4818 val_loss:0.377
[06/0052] | train_loss:1.5239 val_acc:81.5085 val_loss:0.4295
[06/0053] | train_loss:1.4777 val_acc:81.5085 val_loss:0.3997
[06/0054] | train_loss:1.4882 val_acc:83.9416 val_loss:0.3893
model is saved at epoch 54!![06/0055] | train_loss:1.4879 val_acc:82.4818 val_loss:0.3955
[06/0056] | train_loss:1.5554 val_acc:81.7518 val_loss:0.3808
[06/0057] | train_loss:1.5285 val_acc:82.9684 val_loss:0.393
[06/0058] | train_loss:1.4427 val_acc:82.9684 val_loss:0.3933
[06/0059] | train_loss:1.437 val_acc:81.9951 val_loss:0.4269
[06/0060] | train_loss:1.4764 val_acc:79.8054 val_loss:0.4136
[06/0061] | train_loss:1.4538 val_acc:80.5353 val_loss:0.4699
[06/0062] | train_loss:1.4166 val_acc:82.9684 val_loss:0.4128
[06/0063] | train_loss:1.415 val_acc:83.6983 val_loss:0.377
[06/0064] | train_loss:1.3862 val_acc:81.2652 val_loss:0.4009
[06/0065] | train_loss:1.311 val_acc:82.2384 val_loss:0.4204
[06/0066] | train_loss:1.2898 val_acc:82.7251 val_loss:0.3978
[06/0067] | train_loss:1.2976 val_acc:83.455 val_loss:0.4104
[06/0068] | train_loss:1.3185 val_acc:79.8054 val_loss:0.4337
[06/0069] | train_loss:1.3548 val_acc:81.9951 val_loss:0.4135
[06/0070] | train_loss:1.315 val_acc:82.4818 val_loss:0.4037
[06/0071] | train_loss:1.2586 val_acc:82.2384 val_loss:0.4104
[06/0072] | train_loss:1.2743 val_acc:82.7251 val_loss:0.3921
[06/0073] | train_loss:1.2688 val_acc:83.6983 val_loss:0.4084
[06/0074] | train_loss:1.3282 val_acc:80.5353 val_loss:0.4402
[06/0075] | train_loss:1.294 val_acc:81.7518 val_loss:0.4386
[06/0076] | train_loss:1.2373 val_acc:79.8054 val_loss:0.4548
[06/0077] | train_loss:1.2514 val_acc:82.2384 val_loss:0.4005
[06/0078] | train_loss:1.2064 val_acc:82.4818 val_loss:0.3982
[06/0079] | train_loss:1.2305 val_acc:84.1849 val_loss:0.4331
model is saved at epoch 79!![06/0080] | train_loss:1.2332 val_acc:81.2652 val_loss:0.453
[06/0081] | train_loss:1.2993 val_acc:80.5353 val_loss:0.4852
[06/0082] | train_loss:1.2305 val_acc:83.455 val_loss:0.4079
[06/0083] | train_loss:1.1726 val_acc:81.0219 val_loss:0.4458
[06/0084] | train_loss:1.2636 val_acc:81.9951 val_loss:0.4944
[06/0085] | train_loss:1.232 val_acc:81.7518 val_loss:0.4456
[06/0086] | train_loss:1.2248 val_acc:82.9684 val_loss:0.4351
[06/0087] | train_loss:1.1582 val_acc:81.2652 val_loss:0.4686
[06/0088] | train_loss:1.1749 val_acc:82.2384 val_loss:0.4485
[06/0089] | train_loss:1.1599 val_acc:82.9684 val_loss:0.4353
[06/0090] | train_loss:1.0938 val_acc:80.292 val_loss:0.5158
[06/0091] | train_loss:1.1261 val_acc:82.2384 val_loss:0.4562
[06/0092] | train_loss:1.1258 val_acc:83.6983 val_loss:0.4476
[06/0093] | train_loss:1.1335 val_acc:80.292 val_loss:0.5251
[06/0094] | train_loss:1.1532 val_acc:83.2117 val_loss:0.4487
[06/0095] | train_loss:1.1257 val_acc:81.9951 val_loss:0.4694
[06/0096] | train_loss:1.0945 val_acc:81.9951 val_loss:0.4672
[06/0097] | train_loss:1.1125 val_acc:83.9416 val_loss:0.3999
[06/0098] | train_loss:1.0658 val_acc:83.6983 val_loss:0.4357
[06/0099] | train_loss:1.0487 val_acc:83.2117 val_loss:0.5086
[06/0100] | train_loss:1.0902 val_acc:81.0219 val_loss:0.4702
[06/0101] | train_loss:1.0894 val_acc:82.2384 val_loss:0.452
[06/0102] | train_loss:1.0682 val_acc:81.9951 val_loss:0.4859
[06/0103] | train_loss:0.9826 val_acc:82.7251 val_loss:0.5427
[06/0104] | train_loss:1.0676 val_acc:81.0219 val_loss:0.468
[06/0105] | train_loss:1.0601 val_acc:82.7251 val_loss:0.4745
[06/0106] | train_loss:0.9965 val_acc:82.7251 val_loss:0.4972
[06/0107] | train_loss:1.1541 val_acc:83.6983 val_loss:0.5416
[06/0108] | train_loss:1.0975 val_acc:83.455 val_loss:0.4715
[06/0109] | train_loss:1.071 val_acc:83.2117 val_loss:0.4595
[06/0110] | train_loss:1.0936 val_acc:84.9148 val_loss:0.4518
model is saved at epoch 110!![06/0111] | train_loss:1.0489 val_acc:82.7251 val_loss:0.488
[06/0112] | train_loss:1.0951 val_acc:82.2384 val_loss:0.4918
[06/0113] | train_loss:1.0165 val_acc:82.9684 val_loss:0.4847
[06/0114] | train_loss:1.0028 val_acc:82.9684 val_loss:0.5165
[06/0115] | train_loss:1.0287 val_acc:83.455 val_loss:0.5058
[06/0116] | train_loss:1.0496 val_acc:80.7786 val_loss:0.5099
[06/0117] | train_loss:1.014 val_acc:81.5085 val_loss:0.5098
[06/0118] | train_loss:1.032 val_acc:82.4818 val_loss:0.5635
[06/0119] | train_loss:1.0066 val_acc:80.5353 val_loss:0.5907
[06/0120] | train_loss:0.9874 val_acc:81.7518 val_loss:0.5156
[06/0121] | train_loss:0.9564 val_acc:82.4818 val_loss:0.5359
[06/0122] | train_loss:1.0082 val_acc:82.9684 val_loss:0.5287
[06/0123] | train_loss:0.9651 val_acc:83.455 val_loss:0.529
[06/0124] | train_loss:1.0261 val_acc:81.9951 val_loss:0.4667
[06/0125] | train_loss:1.0845 val_acc:82.2384 val_loss:0.4741
[06/0126] | train_loss:1.0695 val_acc:81.9951 val_loss:0.5813
[06/0127] | train_loss:0.9658 val_acc:82.2384 val_loss:0.4911
[06/0128] | train_loss:1.032 val_acc:82.2384 val_loss:0.4938
[06/0129] | train_loss:1.0577 val_acc:83.455 val_loss:0.4732
[06/0130] | train_loss:0.9351 val_acc:83.6983 val_loss:0.5069
[06/0131] | train_loss:0.9521 val_acc:82.4818 val_loss:0.5286
[06/0132] | train_loss:0.9108 val_acc:84.9148 val_loss:0.5098
[06/0133] | train_loss:0.9981 val_acc:82.7251 val_loss:0.5103
[06/0134] | train_loss:0.863 val_acc:82.9684 val_loss:0.563
[06/0135] | train_loss:0.8772 val_acc:84.4282 val_loss:0.5468
[06/0136] | train_loss:0.8739 val_acc:83.6983 val_loss:0.5622
[06/0137] | train_loss:0.8613 val_acc:82.2384 val_loss:0.583
[06/0138] | train_loss:0.8304 val_acc:83.2117 val_loss:0.6192
[06/0139] | train_loss:0.885 val_acc:83.2117 val_loss:0.5474
[06/0140] | train_loss:0.9096 val_acc:82.9684 val_loss:0.5271
[06/0141] | train_loss:0.901 val_acc:81.9951 val_loss:0.5327
[06/0142] | train_loss:0.8786 val_acc:83.455 val_loss:0.5149
[06/0143] | train_loss:0.8547 val_acc:82.2384 val_loss:0.6036
[06/0144] | train_loss:0.94 val_acc:84.1849 val_loss:0.5186
[06/0145] | train_loss:0.8903 val_acc:85.1582 val_loss:0.4724
model is saved at epoch 145!![06/0146] | train_loss:0.8621 val_acc:85.1582 val_loss:0.5422
[06/0147] | train_loss:0.8566 val_acc:82.7251 val_loss:0.5121
[06/0148] | train_loss:0.8182 val_acc:84.6715 val_loss:0.5424
[06/0149] | train_loss:0.8047 val_acc:82.9684 val_loss:0.5766
[06/0150] | train_loss:0.7551 val_acc:82.2384 val_loss:0.6115
[06/0151] | train_loss:0.7647 val_acc:83.455 val_loss:0.5781
[06/0152] | train_loss:0.7657 val_acc:81.9951 val_loss:0.6207
[06/0153] | train_loss:0.8924 val_acc:82.4818 val_loss:0.5864
[06/0154] | train_loss:0.7945 val_acc:82.9684 val_loss:0.6366
[06/0155] | train_loss:0.76 val_acc:82.9684 val_loss:0.6558
[06/0156] | train_loss:0.7263 val_acc:84.1849 val_loss:0.5758
[06/0157] | train_loss:0.8069 val_acc:84.1849 val_loss:0.6375
[06/0158] | train_loss:0.7723 val_acc:85.1582 val_loss:0.582
[06/0159] | train_loss:0.7029 val_acc:84.6715 val_loss:0.57
[06/0160] | train_loss:0.7855 val_acc:83.6983 val_loss:0.6578
[06/0161] | train_loss:0.7412 val_acc:85.1582 val_loss:0.5297
[06/0162] | train_loss:0.6829 val_acc:86.1314 val_loss:0.5817
model is saved at epoch 162!![06/0163] | train_loss:0.6977 val_acc:85.6448 val_loss:0.5469
[06/0164] | train_loss:0.8092 val_acc:82.7251 val_loss:0.6497
[06/0165] | train_loss:0.8435 val_acc:82.2384 val_loss:0.6484
[06/0166] | train_loss:0.7774 val_acc:84.1849 val_loss:0.5237
[06/0167] | train_loss:0.7708 val_acc:82.2384 val_loss:0.6617
[06/0168] | train_loss:0.7481 val_acc:83.455 val_loss:0.6396
[06/0169] | train_loss:0.7806 val_acc:83.6983 val_loss:0.5414
[06/0170] | train_loss:0.7345 val_acc:83.9416 val_loss:0.6046
[06/0171] | train_loss:0.7418 val_acc:83.6983 val_loss:0.5948
[06/0172] | train_loss:0.7953 val_acc:80.7786 val_loss:0.6269
[06/0173] | train_loss:0.7708 val_acc:83.6983 val_loss:0.6138
[06/0174] | train_loss:0.7419 val_acc:83.6983 val_loss:0.5344
[06/0175] | train_loss:0.807 val_acc:82.9684 val_loss:0.56
[06/0176] | train_loss:0.7454 val_acc:83.2117 val_loss:0.7263
[06/0177] | train_loss:0.6732 val_acc:82.4818 val_loss:0.6653
[06/0178] | train_loss:0.7627 val_acc:83.2117 val_loss:0.5154
[06/0179] | train_loss:0.7864 val_acc:85.1582 val_loss:0.5848
[06/0180] | train_loss:0.7238 val_acc:83.455 val_loss:0.6151
[06/0181] | train_loss:0.69 val_acc:84.4282 val_loss:0.6216
[06/0182] | train_loss:0.6974 val_acc:84.6715 val_loss:0.5719
[06/0183] | train_loss:0.6938 val_acc:84.1849 val_loss:0.6538
[06/0184] | train_loss:0.7031 val_acc:83.9416 val_loss:0.6627
[06/0185] | train_loss:0.6952 val_acc:83.455 val_loss:0.6105
[06/0186] | train_loss:0.6835 val_acc:84.6715 val_loss:0.5449
[06/0187] | train_loss:0.683 val_acc:84.9148 val_loss:0.6012
[06/0188] | train_loss:0.6768 val_acc:84.4282 val_loss:0.5688
[06/0189] | train_loss:0.6962 val_acc:85.4015 val_loss:0.6168
[06/0190] | train_loss:0.6659 val_acc:84.9148 val_loss:0.6554
[06/0191] | train_loss:0.6717 val_acc:84.4282 val_loss:0.6326
[06/0192] | train_loss:0.6679 val_acc:85.6448 val_loss:0.5416
[06/0193] | train_loss:0.6421 val_acc:82.9684 val_loss:0.6318
[06/0194] | train_loss:0.6852 val_acc:85.1582 val_loss:0.6262
[06/0195] | train_loss:0.6986 val_acc:83.9416 val_loss:0.594
[06/0196] | train_loss:0.6604 val_acc:82.7251 val_loss:0.6715
[06/0197] | train_loss:0.6924 val_acc:82.7251 val_loss:0.6772
[06/0198] | train_loss:0.6526 val_acc:83.455 val_loss:0.579
[06/0199] | train_loss:0.6864 val_acc:82.4818 val_loss:0.6609
[06/0200] | train_loss:0.708 val_acc:82.7251 val_loss:0.6398
[06/0201] | train_loss:0.6783 val_acc:84.4282 val_loss:0.637
[06/0202] | train_loss:0.6491 val_acc:84.6715 val_loss:0.6376
[06/0203] | train_loss:0.6393 val_acc:84.1849 val_loss:0.6749
[06/0204] | train_loss:0.6642 val_acc:84.4282 val_loss:0.6534
[06/0205] | train_loss:0.6056 val_acc:83.455 val_loss:0.6659
[06/0206] | train_loss:0.5979 val_acc:82.2384 val_loss:0.6398
[06/0207] | train_loss:0.6134 val_acc:85.4015 val_loss:0.6851
[06/0208] | train_loss:0.7167 val_acc:83.455 val_loss:0.5814
[06/0209] | train_loss:0.6367 val_acc:84.6715 val_loss:0.7269
[06/0210] | train_loss:0.5738 val_acc:84.1849 val_loss:0.7191
[06/0211] | train_loss:0.6349 val_acc:83.9416 val_loss:0.6854
[06/0212] | train_loss:0.6027 val_acc:84.1849 val_loss:0.6207
[06/0213] | train_loss:0.6075 val_acc:85.6448 val_loss:0.6438
Fold: [6/10] Test is finish !! 
 Test Metrics are: test_acc:79.8054 test_loss:1.0128fold [6/10] is start!!
[07/0001] | train_loss:2.6943 val_acc:50.8516 val_loss:0.6682
model is saved at epoch 1!![07/0002] | train_loss:2.5346 val_acc:50.8516 val_loss:0.6575
[07/0003] | train_loss:2.4426 val_acc:67.6399 val_loss:0.5819
model is saved at epoch 3!![07/0004] | train_loss:2.4058 val_acc:71.0462 val_loss:0.5636
model is saved at epoch 4!![07/0005] | train_loss:2.3607 val_acc:70.5596 val_loss:0.6248
[07/0006] | train_loss:2.2814 val_acc:75.6691 val_loss:0.5403
model is saved at epoch 6!![07/0007] | train_loss:2.2564 val_acc:73.4793 val_loss:0.6177
[07/0008] | train_loss:2.2448 val_acc:73.7226 val_loss:0.5479
[07/0009] | train_loss:2.2158 val_acc:75.6691 val_loss:0.5307
[07/0010] | train_loss:2.1568 val_acc:76.399 val_loss:0.5161
model is saved at epoch 10!![07/0011] | train_loss:2.1872 val_acc:72.9927 val_loss:0.5942
[07/0012] | train_loss:2.153 val_acc:76.399 val_loss:0.5142
[07/0013] | train_loss:2.1031 val_acc:76.8856 val_loss:0.5615
model is saved at epoch 13!![07/0014] | train_loss:2.0612 val_acc:76.8856 val_loss:0.5277
[07/0015] | train_loss:2.0692 val_acc:77.8589 val_loss:0.5169
model is saved at epoch 15!![07/0016] | train_loss:2.009 val_acc:78.5888 val_loss:0.5195
model is saved at epoch 16!![07/0017] | train_loss:2.0516 val_acc:78.8321 val_loss:0.52
model is saved at epoch 17!![07/0018] | train_loss:2.0306 val_acc:78.5888 val_loss:0.5088
[07/0019] | train_loss:1.9713 val_acc:78.8321 val_loss:0.5288
[07/0020] | train_loss:1.9694 val_acc:77.8589 val_loss:0.5303
[07/0021] | train_loss:1.9436 val_acc:78.3455 val_loss:0.4943
[07/0022] | train_loss:1.8917 val_acc:79.562 val_loss:0.5046
model is saved at epoch 22!![07/0023] | train_loss:1.864 val_acc:77.8589 val_loss:0.5256
[07/0024] | train_loss:1.8925 val_acc:78.8321 val_loss:0.496
[07/0025] | train_loss:1.8716 val_acc:79.0754 val_loss:0.5267
[07/0026] | train_loss:1.8345 val_acc:79.3187 val_loss:0.5346
[07/0027] | train_loss:1.8228 val_acc:80.0487 val_loss:0.4875
model is saved at epoch 27!![07/0028] | train_loss:1.8655 val_acc:81.5085 val_loss:0.4727
model is saved at epoch 28!![07/0029] | train_loss:1.7853 val_acc:77.8589 val_loss:0.4968
[07/0030] | train_loss:1.7811 val_acc:79.562 val_loss:0.4543
[07/0031] | train_loss:1.7588 val_acc:78.1022 val_loss:0.4582
[07/0032] | train_loss:1.7486 val_acc:80.5353 val_loss:0.482
[07/0033] | train_loss:1.7685 val_acc:80.7786 val_loss:0.4609
[07/0034] | train_loss:1.7668 val_acc:81.0219 val_loss:0.4582
[07/0035] | train_loss:1.7214 val_acc:81.7518 val_loss:0.4407
model is saved at epoch 35!![07/0036] | train_loss:1.7005 val_acc:80.7786 val_loss:0.4714
[07/0037] | train_loss:1.675 val_acc:80.292 val_loss:0.427
[07/0038] | train_loss:1.6698 val_acc:81.0219 val_loss:0.4551
[07/0039] | train_loss:1.6524 val_acc:77.129 val_loss:0.467
[07/0040] | train_loss:1.663 val_acc:79.0754 val_loss:0.5002
[07/0041] | train_loss:1.6424 val_acc:81.2652 val_loss:0.4533
[07/0042] | train_loss:1.6581 val_acc:80.292 val_loss:0.5111
[07/0043] | train_loss:1.6089 val_acc:81.7518 val_loss:0.5126
[07/0044] | train_loss:1.5515 val_acc:81.7518 val_loss:0.5207
[07/0045] | train_loss:1.5427 val_acc:83.2117 val_loss:0.4532
model is saved at epoch 45!![07/0046] | train_loss:1.5002 val_acc:82.7251 val_loss:0.4717
[07/0047] | train_loss:1.5312 val_acc:80.5353 val_loss:0.5235
[07/0048] | train_loss:1.5339 val_acc:80.0487 val_loss:0.5421
[07/0049] | train_loss:1.5585 val_acc:82.2384 val_loss:0.4429
[07/0050] | train_loss:1.4779 val_acc:81.7518 val_loss:0.4102
[07/0051] | train_loss:1.4803 val_acc:82.2384 val_loss:0.4531
[07/0052] | train_loss:1.4365 val_acc:81.0219 val_loss:0.5354
[07/0053] | train_loss:1.4431 val_acc:81.5085 val_loss:0.4367
[07/0054] | train_loss:1.4447 val_acc:81.9951 val_loss:0.4892
[07/0055] | train_loss:1.4616 val_acc:81.0219 val_loss:0.6271
[07/0056] | train_loss:1.4089 val_acc:82.4818 val_loss:0.5939
[07/0057] | train_loss:1.392 val_acc:82.4818 val_loss:0.6093
[07/0058] | train_loss:1.4453 val_acc:82.9684 val_loss:0.6097
[07/0059] | train_loss:1.408 val_acc:81.2652 val_loss:0.6255
[07/0060] | train_loss:1.4372 val_acc:79.562 val_loss:0.517
[07/0061] | train_loss:1.427 val_acc:80.0487 val_loss:0.4638
[07/0062] | train_loss:1.3534 val_acc:81.0219 val_loss:0.5104
[07/0063] | train_loss:1.364 val_acc:81.5085 val_loss:0.5441
[07/0064] | train_loss:1.3461 val_acc:80.5353 val_loss:0.5665
[07/0065] | train_loss:1.351 val_acc:80.292 val_loss:0.5696
[07/0066] | train_loss:1.3459 val_acc:80.5353 val_loss:0.5112
[07/0067] | train_loss:1.3214 val_acc:80.7786 val_loss:0.6824
[07/0068] | train_loss:1.3387 val_acc:82.7251 val_loss:0.5789
[07/0069] | train_loss:1.3158 val_acc:80.0487 val_loss:0.6587
[07/0070] | train_loss:1.3188 val_acc:82.4818 val_loss:0.607
[07/0071] | train_loss:1.2575 val_acc:83.455 val_loss:0.5791
model is saved at epoch 71!![07/0072] | train_loss:1.3259 val_acc:79.562 val_loss:0.6502
[07/0073] | train_loss:1.2722 val_acc:82.4818 val_loss:0.6742
[07/0074] | train_loss:1.3048 val_acc:81.9951 val_loss:0.5051
[07/0075] | train_loss:1.3235 val_acc:79.562 val_loss:0.6586
[07/0076] | train_loss:1.32 val_acc:80.5353 val_loss:0.4808
[07/0077] | train_loss:1.2734 val_acc:82.2384 val_loss:0.4938
[07/0078] | train_loss:1.2767 val_acc:80.7786 val_loss:0.529
[07/0079] | train_loss:1.2313 val_acc:82.9684 val_loss:0.4534
[07/0080] | train_loss:1.2063 val_acc:80.7786 val_loss:0.5565
[07/0081] | train_loss:1.2149 val_acc:81.0219 val_loss:0.567
[07/0082] | train_loss:1.2561 val_acc:81.9951 val_loss:0.5164
[07/0083] | train_loss:1.2033 val_acc:79.0754 val_loss:0.5903
[07/0084] | train_loss:1.2266 val_acc:82.2384 val_loss:0.4875
[07/0085] | train_loss:1.2214 val_acc:81.7518 val_loss:0.515
[07/0086] | train_loss:1.1415 val_acc:81.9951 val_loss:0.5833
[07/0087] | train_loss:1.17 val_acc:81.5085 val_loss:0.514
[07/0088] | train_loss:1.149 val_acc:82.2384 val_loss:0.5503
[07/0089] | train_loss:1.1771 val_acc:80.292 val_loss:0.5272
[07/0090] | train_loss:1.1027 val_acc:82.2384 val_loss:0.5424
[07/0091] | train_loss:1.2444 val_acc:82.9684 val_loss:0.5509
[07/0092] | train_loss:1.1447 val_acc:81.9951 val_loss:0.5218
[07/0093] | train_loss:1.1448 val_acc:79.8054 val_loss:0.5695
[07/0094] | train_loss:1.1342 val_acc:80.7786 val_loss:0.575
[07/0095] | train_loss:1.1043 val_acc:82.2384 val_loss:0.7304
[07/0096] | train_loss:1.0878 val_acc:81.5085 val_loss:0.5652
[07/0097] | train_loss:1.0966 val_acc:82.2384 val_loss:0.5935
[07/0098] | train_loss:1.039 val_acc:82.7251 val_loss:0.6676
[07/0099] | train_loss:1.002 val_acc:82.7251 val_loss:0.6665
[07/0100] | train_loss:1.1099 val_acc:81.0219 val_loss:0.6578
[07/0101] | train_loss:1.1024 val_acc:81.2652 val_loss:0.6323
[07/0102] | train_loss:1.1143 val_acc:81.0219 val_loss:0.6274
[07/0103] | train_loss:1.1011 val_acc:80.292 val_loss:0.5864
[07/0104] | train_loss:1.0904 val_acc:80.292 val_loss:0.5487
[07/0105] | train_loss:1.0592 val_acc:81.2652 val_loss:0.6342
[07/0106] | train_loss:1.0051 val_acc:81.0219 val_loss:0.6215
[07/0107] | train_loss:1.0577 val_acc:82.2384 val_loss:0.5522
[07/0108] | train_loss:1.1246 val_acc:80.0487 val_loss:0.5934
[07/0109] | train_loss:1.0343 val_acc:81.2652 val_loss:0.5974
[07/0110] | train_loss:1.0662 val_acc:81.0219 val_loss:0.6092
[07/0111] | train_loss:1.0699 val_acc:82.9684 val_loss:0.6899
[07/0112] | train_loss:1.0444 val_acc:83.9416 val_loss:0.6269
model is saved at epoch 112!![07/0113] | train_loss:0.9913 val_acc:81.5085 val_loss:0.5446
[07/0114] | train_loss:1.0031 val_acc:82.2384 val_loss:0.6097
[07/0115] | train_loss:0.9755 val_acc:82.2384 val_loss:0.8082
[07/0116] | train_loss:0.9369 val_acc:81.2652 val_loss:0.7161
[07/0117] | train_loss:0.9884 val_acc:82.7251 val_loss:0.7436
[07/0118] | train_loss:0.9881 val_acc:82.4818 val_loss:0.7922
[07/0119] | train_loss:0.9261 val_acc:83.2117 val_loss:0.714
[07/0120] | train_loss:0.8757 val_acc:81.9951 val_loss:0.6908
[07/0121] | train_loss:0.8924 val_acc:83.455 val_loss:0.6872
[07/0122] | train_loss:0.9111 val_acc:81.5085 val_loss:0.7952
[07/0123] | train_loss:0.9562 val_acc:83.2117 val_loss:0.6639
[07/0124] | train_loss:0.9671 val_acc:81.9951 val_loss:0.8397
[07/0125] | train_loss:1.0504 val_acc:80.292 val_loss:0.7719
[07/0126] | train_loss:1.0336 val_acc:81.9951 val_loss:0.7093
[07/0127] | train_loss:0.9532 val_acc:80.0487 val_loss:0.8006
[07/0128] | train_loss:0.9325 val_acc:82.2384 val_loss:0.7506
[07/0129] | train_loss:0.9395 val_acc:83.6983 val_loss:0.8186
[07/0130] | train_loss:0.9346 val_acc:83.2117 val_loss:0.6839
[07/0131] | train_loss:0.9404 val_acc:83.455 val_loss:0.6845
[07/0132] | train_loss:0.8982 val_acc:81.5085 val_loss:0.789
[07/0133] | train_loss:0.8755 val_acc:82.7251 val_loss:0.7244
[07/0134] | train_loss:0.8711 val_acc:82.9684 val_loss:0.7266
[07/0135] | train_loss:0.8418 val_acc:80.292 val_loss:0.8299
[07/0136] | train_loss:0.8301 val_acc:82.2384 val_loss:0.8422
[07/0137] | train_loss:0.8452 val_acc:81.7518 val_loss:0.8316
[07/0138] | train_loss:0.932 val_acc:83.455 val_loss:0.7482
[07/0139] | train_loss:0.8959 val_acc:81.2652 val_loss:0.7804
[07/0140] | train_loss:0.8814 val_acc:83.6983 val_loss:0.7853
[07/0141] | train_loss:0.9257 val_acc:82.7251 val_loss:0.6926
[07/0142] | train_loss:0.8871 val_acc:82.4818 val_loss:0.8149
[07/0143] | train_loss:0.8935 val_acc:83.2117 val_loss:0.7885
[07/0144] | train_loss:0.8696 val_acc:82.4818 val_loss:0.717
[07/0145] | train_loss:0.828 val_acc:80.7786 val_loss:0.8404
[07/0146] | train_loss:0.8177 val_acc:81.9951 val_loss:0.8288
[07/0147] | train_loss:0.8144 val_acc:81.9951 val_loss:0.7962
[07/0148] | train_loss:0.9642 val_acc:81.7518 val_loss:0.8265
[07/0149] | train_loss:1.0376 val_acc:81.7518 val_loss:0.8912
[07/0150] | train_loss:0.9903 val_acc:81.0219 val_loss:0.7343
[07/0151] | train_loss:1.0382 val_acc:82.7251 val_loss:0.7703
[07/0152] | train_loss:0.9493 val_acc:82.4818 val_loss:0.7534
[07/0153] | train_loss:0.8856 val_acc:82.2384 val_loss:0.729
[07/0154] | train_loss:0.9255 val_acc:83.6983 val_loss:0.7152
[07/0155] | train_loss:0.8571 val_acc:83.2117 val_loss:0.8525
[07/0156] | train_loss:0.8197 val_acc:82.9684 val_loss:0.7507
[07/0157] | train_loss:0.8251 val_acc:83.2117 val_loss:0.7128
[07/0158] | train_loss:0.9038 val_acc:82.2384 val_loss:0.807
[07/0159] | train_loss:0.8557 val_acc:81.5085 val_loss:0.7562
[07/0160] | train_loss:0.7976 val_acc:81.9951 val_loss:0.7132
[07/0161] | train_loss:0.8653 val_acc:81.7518 val_loss:0.8601
[07/0162] | train_loss:0.8015 val_acc:81.9951 val_loss:0.8328
[07/0163] | train_loss:0.8241 val_acc:82.2384 val_loss:0.8269
Fold: [7/10] Test is finish !! 
 Test Metrics are: test_acc:78.3455 test_loss:0.9687fold [7/10] is start!!
[08/0001] | train_loss:2.6855 val_acc:49.3917 val_loss:0.6737
model is saved at epoch 1!![08/0002] | train_loss:2.5464 val_acc:49.3917 val_loss:0.7347
[08/0003] | train_loss:2.4382 val_acc:59.6107 val_loss:0.585
model is saved at epoch 3!![08/0004] | train_loss:2.3925 val_acc:71.7762 val_loss:0.5581
model is saved at epoch 4!![08/0005] | train_loss:2.3325 val_acc:72.2628 val_loss:0.567
model is saved at epoch 5!![08/0006] | train_loss:2.2693 val_acc:73.7226 val_loss:0.5534
model is saved at epoch 6!![08/0007] | train_loss:2.2378 val_acc:75.4258 val_loss:0.5444
model is saved at epoch 7!![08/0008] | train_loss:2.2242 val_acc:74.6959 val_loss:0.5407
[08/0009] | train_loss:2.1967 val_acc:75.9124 val_loss:0.5588
model is saved at epoch 9!![08/0010] | train_loss:2.1475 val_acc:72.5061 val_loss:0.5492
[08/0011] | train_loss:2.1369 val_acc:73.236 val_loss:0.579
[08/0012] | train_loss:2.1021 val_acc:74.6959 val_loss:0.5527
[08/0013] | train_loss:2.0539 val_acc:75.6691 val_loss:0.5324
[08/0014] | train_loss:2.0337 val_acc:75.1825 val_loss:0.5623
[08/0015] | train_loss:2.0312 val_acc:75.4258 val_loss:0.5428
[08/0016] | train_loss:2.0332 val_acc:76.399 val_loss:0.5503
model is saved at epoch 16!![08/0017] | train_loss:1.9884 val_acc:74.2092 val_loss:0.5725
[08/0018] | train_loss:1.955 val_acc:76.1557 val_loss:0.551
[08/0019] | train_loss:1.9506 val_acc:77.3723 val_loss:0.5072
model is saved at epoch 19!![08/0020] | train_loss:1.934 val_acc:74.9392 val_loss:0.5811
[08/0021] | train_loss:1.8955 val_acc:74.9392 val_loss:0.5811
[08/0022] | train_loss:1.8641 val_acc:76.399 val_loss:0.5272
[08/0023] | train_loss:1.828 val_acc:78.1022 val_loss:0.5224
model is saved at epoch 23!![08/0024] | train_loss:1.8173 val_acc:79.8054 val_loss:0.5063
model is saved at epoch 24!![08/0025] | train_loss:1.854 val_acc:78.1022 val_loss:0.5095
[08/0026] | train_loss:1.7826 val_acc:79.8054 val_loss:0.53
[08/0027] | train_loss:1.7543 val_acc:78.1022 val_loss:0.5374
[08/0028] | train_loss:1.719 val_acc:78.8321 val_loss:0.5767
[08/0029] | train_loss:1.791 val_acc:75.9124 val_loss:0.5075
[08/0030] | train_loss:1.7614 val_acc:78.1022 val_loss:0.5669
[08/0031] | train_loss:1.701 val_acc:79.3187 val_loss:0.5813
[08/0032] | train_loss:1.6849 val_acc:79.562 val_loss:0.5834
[08/0033] | train_loss:1.7184 val_acc:78.3455 val_loss:0.57
[08/0034] | train_loss:1.7086 val_acc:81.5085 val_loss:0.5934
model is saved at epoch 34!![08/0035] | train_loss:1.6636 val_acc:77.6156 val_loss:0.5951
[08/0036] | train_loss:1.6702 val_acc:80.292 val_loss:0.5386
[08/0037] | train_loss:1.6156 val_acc:79.562 val_loss:0.5769
[08/0038] | train_loss:1.6065 val_acc:77.8589 val_loss:0.511
[08/0039] | train_loss:1.5784 val_acc:79.8054 val_loss:0.597
[08/0040] | train_loss:1.5765 val_acc:79.0754 val_loss:0.5502
[08/0041] | train_loss:1.5581 val_acc:80.292 val_loss:0.5324
[08/0042] | train_loss:1.5214 val_acc:77.6156 val_loss:0.5949
[08/0043] | train_loss:1.5546 val_acc:80.292 val_loss:0.5361
[08/0044] | train_loss:1.4923 val_acc:77.8589 val_loss:0.6372
[08/0045] | train_loss:1.4443 val_acc:77.6156 val_loss:0.6149
[08/0046] | train_loss:1.4355 val_acc:78.8321 val_loss:0.6218
[08/0047] | train_loss:1.4801 val_acc:80.292 val_loss:0.6148
[08/0048] | train_loss:1.5143 val_acc:79.3187 val_loss:0.616
[08/0049] | train_loss:1.4156 val_acc:78.3455 val_loss:0.6851
[08/0050] | train_loss:1.407 val_acc:79.3187 val_loss:0.5707
[08/0051] | train_loss:1.3839 val_acc:79.0754 val_loss:0.6437
[08/0052] | train_loss:1.4039 val_acc:80.0487 val_loss:0.565
[08/0053] | train_loss:1.4197 val_acc:80.292 val_loss:0.5596
[08/0054] | train_loss:1.3976 val_acc:79.562 val_loss:0.5676
[08/0055] | train_loss:1.3338 val_acc:81.5085 val_loss:0.6447
[08/0056] | train_loss:1.3615 val_acc:80.5353 val_loss:0.6185
[08/0057] | train_loss:1.3587 val_acc:80.5353 val_loss:0.6632
[08/0058] | train_loss:1.35 val_acc:81.0219 val_loss:0.6239
[08/0059] | train_loss:1.3118 val_acc:80.7786 val_loss:0.603
[08/0060] | train_loss:1.2814 val_acc:81.0219 val_loss:0.6225
[08/0061] | train_loss:1.3137 val_acc:78.3455 val_loss:0.6708
[08/0062] | train_loss:1.3534 val_acc:80.292 val_loss:0.6834
[08/0063] | train_loss:1.2424 val_acc:79.3187 val_loss:0.5884
[08/0064] | train_loss:1.2093 val_acc:81.5085 val_loss:0.6395
[08/0065] | train_loss:1.2433 val_acc:80.7786 val_loss:0.6075
[08/0066] | train_loss:1.2774 val_acc:81.2652 val_loss:0.6288
[08/0067] | train_loss:1.2286 val_acc:80.0487 val_loss:0.6411
[08/0068] | train_loss:1.2565 val_acc:80.292 val_loss:0.6223
[08/0069] | train_loss:1.1978 val_acc:80.5353 val_loss:0.6511
[08/0070] | train_loss:1.2257 val_acc:80.5353 val_loss:0.6912
[08/0071] | train_loss:1.1963 val_acc:81.7518 val_loss:0.6586
model is saved at epoch 71!![08/0072] | train_loss:1.2011 val_acc:78.8321 val_loss:0.6475
[08/0073] | train_loss:1.1868 val_acc:80.5353 val_loss:0.6584
[08/0074] | train_loss:1.1548 val_acc:82.7251 val_loss:0.7075
model is saved at epoch 74!![08/0075] | train_loss:1.1588 val_acc:81.0219 val_loss:0.655
[08/0076] | train_loss:1.1653 val_acc:81.2652 val_loss:0.6073
[08/0077] | train_loss:1.1703 val_acc:81.5085 val_loss:0.724
[08/0078] | train_loss:1.119 val_acc:81.9951 val_loss:0.7075
[08/0079] | train_loss:1.1467 val_acc:81.9951 val_loss:0.5779
[08/0080] | train_loss:1.1423 val_acc:81.9951 val_loss:0.61
[08/0081] | train_loss:1.0845 val_acc:80.7786 val_loss:0.6775
[08/0082] | train_loss:1.0565 val_acc:81.9951 val_loss:0.7412
[08/0083] | train_loss:1.0785 val_acc:81.7518 val_loss:0.7931
[08/0084] | train_loss:1.1194 val_acc:82.4818 val_loss:0.6678
[08/0085] | train_loss:1.0941 val_acc:79.3187 val_loss:0.715
[08/0086] | train_loss:1.044 val_acc:80.5353 val_loss:0.7905
[08/0087] | train_loss:1.0592 val_acc:81.7518 val_loss:0.8141
[08/0088] | train_loss:1.0453 val_acc:81.0219 val_loss:0.7468
[08/0089] | train_loss:1.0267 val_acc:81.0219 val_loss:0.6419
[08/0090] | train_loss:1.1401 val_acc:80.7786 val_loss:0.7593
[08/0091] | train_loss:1.0281 val_acc:81.9951 val_loss:0.7185
[08/0092] | train_loss:1.0514 val_acc:81.5085 val_loss:0.6696
[08/0093] | train_loss:1.064 val_acc:81.7518 val_loss:0.7255
[08/0094] | train_loss:1.0211 val_acc:81.5085 val_loss:0.7777
[08/0095] | train_loss:1.1288 val_acc:81.2652 val_loss:0.7654
[08/0096] | train_loss:1.0551 val_acc:80.292 val_loss:0.6566
[08/0097] | train_loss:1.0453 val_acc:79.8054 val_loss:0.7531
[08/0098] | train_loss:0.9867 val_acc:81.9951 val_loss:0.7667
[08/0099] | train_loss:0.9108 val_acc:83.2117 val_loss:0.7701
model is saved at epoch 99!![08/0100] | train_loss:0.9677 val_acc:81.5085 val_loss:0.7455
[08/0101] | train_loss:0.934 val_acc:82.9684 val_loss:0.7456
[08/0102] | train_loss:0.9628 val_acc:80.7786 val_loss:0.8026
[08/0103] | train_loss:0.9652 val_acc:80.5353 val_loss:0.9051
[08/0104] | train_loss:0.9383 val_acc:80.5353 val_loss:0.7949
[08/0105] | train_loss:0.9667 val_acc:81.0219 val_loss:0.8414
[08/0106] | train_loss:0.9618 val_acc:81.7518 val_loss:0.7501
[08/0107] | train_loss:0.9202 val_acc:83.2117 val_loss:0.9079
[08/0108] | train_loss:0.9263 val_acc:81.5085 val_loss:0.7824
[08/0109] | train_loss:0.912 val_acc:83.9416 val_loss:0.7947
model is saved at epoch 109!![08/0110] | train_loss:0.8718 val_acc:82.4818 val_loss:0.8714
[08/0111] | train_loss:0.885 val_acc:82.2384 val_loss:0.7577
[08/0112] | train_loss:0.8227 val_acc:80.7786 val_loss:0.9165
[08/0113] | train_loss:0.8414 val_acc:82.7251 val_loss:0.8548
[08/0114] | train_loss:0.9184 val_acc:80.5353 val_loss:0.737
[08/0115] | train_loss:1.0027 val_acc:80.5353 val_loss:0.7982
[08/0116] | train_loss:0.9434 val_acc:82.2384 val_loss:0.7972
[08/0117] | train_loss:0.9093 val_acc:82.9684 val_loss:0.8045
[08/0118] | train_loss:0.9826 val_acc:80.292 val_loss:0.9007
[08/0119] | train_loss:0.8912 val_acc:82.2384 val_loss:0.8444
[08/0120] | train_loss:0.8721 val_acc:83.6983 val_loss:0.996
[08/0121] | train_loss:0.8803 val_acc:83.455 val_loss:0.8207
[08/0122] | train_loss:0.8529 val_acc:84.1849 val_loss:0.9064
model is saved at epoch 122!![08/0123] | train_loss:0.8605 val_acc:82.4818 val_loss:1.0154
[08/0124] | train_loss:1.0139 val_acc:81.9951 val_loss:0.7702
[08/0125] | train_loss:0.9112 val_acc:82.2384 val_loss:1.0885
[08/0126] | train_loss:0.7896 val_acc:82.4818 val_loss:0.8494
[08/0127] | train_loss:0.8136 val_acc:80.7786 val_loss:0.9589
[08/0128] | train_loss:0.9281 val_acc:81.5085 val_loss:1.1031
[08/0129] | train_loss:0.8289 val_acc:80.292 val_loss:0.9524
[08/0130] | train_loss:0.8393 val_acc:81.2652 val_loss:0.7736
[08/0131] | train_loss:0.876 val_acc:83.6983 val_loss:0.847
[08/0132] | train_loss:0.8606 val_acc:82.4818 val_loss:1.0625
[08/0133] | train_loss:0.8555 val_acc:81.5085 val_loss:0.9932
[08/0134] | train_loss:0.8226 val_acc:80.5353 val_loss:0.8654
[08/0135] | train_loss:0.7941 val_acc:81.0219 val_loss:0.9452
[08/0136] | train_loss:0.7116 val_acc:82.7251 val_loss:0.9611
[08/0137] | train_loss:0.7596 val_acc:81.2652 val_loss:0.98
[08/0138] | train_loss:0.8625 val_acc:82.7251 val_loss:0.8337
[08/0139] | train_loss:0.8991 val_acc:80.5353 val_loss:0.7812
[08/0140] | train_loss:0.8171 val_acc:80.292 val_loss:0.8098
[08/0141] | train_loss:0.8104 val_acc:79.8054 val_loss:0.8733
[08/0142] | train_loss:0.7913 val_acc:82.4818 val_loss:0.8351
[08/0143] | train_loss:0.7446 val_acc:81.7518 val_loss:0.8957
[08/0144] | train_loss:0.8323 val_acc:82.2384 val_loss:0.7705
[08/0145] | train_loss:0.7693 val_acc:81.0219 val_loss:1.0606
[08/0146] | train_loss:0.7382 val_acc:81.2652 val_loss:1.0538
[08/0147] | train_loss:0.6857 val_acc:81.2652 val_loss:1.025
[08/0148] | train_loss:0.8094 val_acc:80.7786 val_loss:0.9375
[08/0149] | train_loss:0.8877 val_acc:82.2384 val_loss:0.9568
[08/0150] | train_loss:0.7651 val_acc:81.9951 val_loss:0.978
[08/0151] | train_loss:0.7915 val_acc:81.7518 val_loss:1.0298
[08/0152] | train_loss:0.7233 val_acc:84.4282 val_loss:0.9087
model is saved at epoch 152!![08/0153] | train_loss:0.6978 val_acc:82.2384 val_loss:1.1052
[08/0154] | train_loss:0.719 val_acc:82.7251 val_loss:0.8833
[08/0155] | train_loss:0.7343 val_acc:80.5353 val_loss:1.0007
[08/0156] | train_loss:0.6994 val_acc:82.9684 val_loss:0.9672
[08/0157] | train_loss:0.6728 val_acc:82.2384 val_loss:0.9629
[08/0158] | train_loss:0.6778 val_acc:83.455 val_loss:1.002
[08/0159] | train_loss:0.6887 val_acc:82.9684 val_loss:1.0834
[08/0160] | train_loss:0.7299 val_acc:82.9684 val_loss:1.1692
[08/0161] | train_loss:0.6915 val_acc:82.9684 val_loss:1.104
[08/0162] | train_loss:0.6384 val_acc:85.1582 val_loss:1.2459
model is saved at epoch 162!![08/0163] | train_loss:0.6833 val_acc:81.2652 val_loss:1.2873
[08/0164] | train_loss:0.7154 val_acc:82.7251 val_loss:1.0305
[08/0165] | train_loss:0.7211 val_acc:81.9951 val_loss:1.0117
[08/0166] | train_loss:0.7301 val_acc:80.7786 val_loss:1.069
[08/0167] | train_loss:0.6876 val_acc:81.0219 val_loss:0.9239
[08/0168] | train_loss:0.6484 val_acc:83.9416 val_loss:1.0314
[08/0169] | train_loss:0.6987 val_acc:84.6715 val_loss:0.9048
[08/0170] | train_loss:0.7668 val_acc:79.8054 val_loss:1.1864
[08/0171] | train_loss:0.8266 val_acc:80.7786 val_loss:0.945
[08/0172] | train_loss:0.7427 val_acc:82.2384 val_loss:1.104
[08/0173] | train_loss:0.7453 val_acc:81.2652 val_loss:1.0218
[08/0174] | train_loss:0.6647 val_acc:82.9684 val_loss:0.9754
[08/0175] | train_loss:0.6265 val_acc:82.7251 val_loss:1.0501
[08/0176] | train_loss:0.6557 val_acc:81.5085 val_loss:0.8552
[08/0177] | train_loss:0.6307 val_acc:82.4818 val_loss:0.9772
[08/0178] | train_loss:0.7303 val_acc:81.9951 val_loss:1.0204
[08/0179] | train_loss:0.6749 val_acc:83.6983 val_loss:0.887
[08/0180] | train_loss:0.6628 val_acc:81.9951 val_loss:0.9182
[08/0181] | train_loss:0.6573 val_acc:81.7518 val_loss:1.1833
[08/0182] | train_loss:0.6579 val_acc:82.2384 val_loss:1.0371
[08/0183] | train_loss:0.6072 val_acc:83.6983 val_loss:0.951
[08/0184] | train_loss:0.5955 val_acc:83.9416 val_loss:1.0267
[08/0185] | train_loss:0.6152 val_acc:82.9684 val_loss:1.1189
[08/0186] | train_loss:0.6074 val_acc:82.2384 val_loss:1.0932
[08/0187] | train_loss:0.5657 val_acc:83.9416 val_loss:1.1397
[08/0188] | train_loss:0.5622 val_acc:81.5085 val_loss:1.227
[08/0189] | train_loss:0.5547 val_acc:82.2384 val_loss:1.2383
[08/0190] | train_loss:0.6463 val_acc:83.2117 val_loss:1.0705
[08/0191] | train_loss:0.6139 val_acc:82.7251 val_loss:1.2805
[08/0192] | train_loss:0.6152 val_acc:83.6983 val_loss:1.1422
[08/0193] | train_loss:0.6243 val_acc:81.7518 val_loss:1.1886
[08/0194] | train_loss:0.5845 val_acc:81.5085 val_loss:1.0469
[08/0195] | train_loss:0.5954 val_acc:83.2117 val_loss:1.0548
[08/0196] | train_loss:0.6554 val_acc:81.7518 val_loss:1.0398
[08/0197] | train_loss:0.7615 val_acc:82.2384 val_loss:1.0631
[08/0198] | train_loss:0.6835 val_acc:82.2384 val_loss:0.9402
[08/0199] | train_loss:0.6519 val_acc:83.2117 val_loss:1.0219
[08/0200] | train_loss:0.6087 val_acc:81.9951 val_loss:0.9536
[08/0201] | train_loss:0.5925 val_acc:80.5353 val_loss:1.098
[08/0202] | train_loss:0.6331 val_acc:82.7251 val_loss:1.2009
[08/0203] | train_loss:0.5512 val_acc:81.7518 val_loss:1.2271
[08/0204] | train_loss:0.6347 val_acc:82.9684 val_loss:1.1513
[08/0205] | train_loss:0.6049 val_acc:80.7786 val_loss:1.0837
[08/0206] | train_loss:0.6036 val_acc:82.2384 val_loss:0.9577
[08/0207] | train_loss:0.664 val_acc:79.562 val_loss:1.3398
[08/0208] | train_loss:0.653 val_acc:83.2117 val_loss:1.1782
[08/0209] | train_loss:0.5863 val_acc:82.4818 val_loss:1.289
[08/0210] | train_loss:0.641 val_acc:81.9951 val_loss:1.0053
[08/0211] | train_loss:0.6147 val_acc:81.7518 val_loss:1.0093
[08/0212] | train_loss:0.5151 val_acc:81.9951 val_loss:1.1343
[08/0213] | train_loss:0.5507 val_acc:81.7518 val_loss:1.0755
Fold: [8/10] Test is finish !! 
 Test Metrics are: test_acc:82.7251 test_loss:0.5757fold [8/10] is start!!
[09/0001] | train_loss:2.696 val_acc:53.2847 val_loss:0.6624
model is saved at epoch 1!![09/0002] | train_loss:2.5512 val_acc:53.528 val_loss:0.6682
model is saved at epoch 2!![09/0003] | train_loss:2.4651 val_acc:63.017 val_loss:0.5801
model is saved at epoch 3!![09/0004] | train_loss:2.4048 val_acc:72.7494 val_loss:0.5406
model is saved at epoch 4!![09/0005] | train_loss:2.3535 val_acc:72.5061 val_loss:0.5827
[09/0006] | train_loss:2.3481 val_acc:72.7494 val_loss:0.557
[09/0007] | train_loss:2.2968 val_acc:73.236 val_loss:0.5646
model is saved at epoch 7!![09/0008] | train_loss:2.2431 val_acc:77.3723 val_loss:0.5355
model is saved at epoch 8!![09/0009] | train_loss:2.2155 val_acc:75.1825 val_loss:0.5327
[09/0010] | train_loss:2.1996 val_acc:78.1022 val_loss:0.5269
model is saved at epoch 10!![09/0011] | train_loss:2.175 val_acc:78.8321 val_loss:0.5013
model is saved at epoch 11!![09/0012] | train_loss:2.1516 val_acc:74.6959 val_loss:0.4897
[09/0013] | train_loss:2.1404 val_acc:77.6156 val_loss:0.4821
[09/0014] | train_loss:2.0717 val_acc:76.8856 val_loss:0.5217
[09/0015] | train_loss:2.0738 val_acc:76.399 val_loss:0.5283
[09/0016] | train_loss:2.0897 val_acc:77.129 val_loss:0.5197
[09/0017] | train_loss:2.0713 val_acc:74.9392 val_loss:0.5516
[09/0018] | train_loss:2.0419 val_acc:79.0754 val_loss:0.4818
model is saved at epoch 18!![09/0019] | train_loss:2.0287 val_acc:78.1022 val_loss:0.4902
[09/0020] | train_loss:2.0217 val_acc:81.0219 val_loss:0.4678
model is saved at epoch 20!![09/0021] | train_loss:1.9739 val_acc:82.9684 val_loss:0.4415
model is saved at epoch 21!![09/0022] | train_loss:1.9549 val_acc:76.8856 val_loss:0.4959
[09/0023] | train_loss:1.9837 val_acc:79.3187 val_loss:0.4758
[09/0024] | train_loss:1.9103 val_acc:79.562 val_loss:0.464
[09/0025] | train_loss:1.9575 val_acc:81.9951 val_loss:0.454
[09/0026] | train_loss:1.9347 val_acc:79.562 val_loss:0.4685
[09/0027] | train_loss:1.8871 val_acc:81.0219 val_loss:0.4304
[09/0028] | train_loss:1.8085 val_acc:80.0487 val_loss:0.4465
[09/0029] | train_loss:1.8522 val_acc:78.5888 val_loss:0.4193
[09/0030] | train_loss:1.8499 val_acc:78.8321 val_loss:0.515
[09/0031] | train_loss:1.8298 val_acc:79.8054 val_loss:0.4604
[09/0032] | train_loss:1.8273 val_acc:82.9684 val_loss:0.4572
[09/0033] | train_loss:1.7943 val_acc:81.7518 val_loss:0.4256
[09/0034] | train_loss:1.7868 val_acc:80.7786 val_loss:0.4578
[09/0035] | train_loss:1.8014 val_acc:79.8054 val_loss:0.4433
[09/0036] | train_loss:1.7438 val_acc:81.0219 val_loss:0.4991
[09/0037] | train_loss:1.7481 val_acc:81.2652 val_loss:0.4316
[09/0038] | train_loss:1.7028 val_acc:80.7786 val_loss:0.4252
[09/0039] | train_loss:1.674 val_acc:83.2117 val_loss:0.4356
model is saved at epoch 39!![09/0040] | train_loss:1.6887 val_acc:81.0219 val_loss:0.4112
[09/0041] | train_loss:1.6823 val_acc:81.7518 val_loss:0.4073
[09/0042] | train_loss:1.7259 val_acc:78.8321 val_loss:0.4748
[09/0043] | train_loss:1.7342 val_acc:80.7786 val_loss:0.4151
[09/0044] | train_loss:1.6906 val_acc:81.7518 val_loss:0.4129
[09/0045] | train_loss:1.6396 val_acc:83.6983 val_loss:0.4043
model is saved at epoch 45!![09/0046] | train_loss:1.5916 val_acc:80.292 val_loss:0.5251
[09/0047] | train_loss:1.6538 val_acc:81.0219 val_loss:0.4504
[09/0048] | train_loss:1.6034 val_acc:82.4818 val_loss:0.4198
[09/0049] | train_loss:1.5795 val_acc:81.9951 val_loss:0.4424
[09/0050] | train_loss:1.5501 val_acc:82.9684 val_loss:0.3867
[09/0051] | train_loss:1.5526 val_acc:82.7251 val_loss:0.4365
[09/0052] | train_loss:1.4952 val_acc:81.5085 val_loss:0.4631
[09/0053] | train_loss:1.608 val_acc:81.7518 val_loss:0.4721
[09/0054] | train_loss:1.5417 val_acc:82.7251 val_loss:0.415
[09/0055] | train_loss:1.5548 val_acc:85.1582 val_loss:0.409
model is saved at epoch 55!![09/0056] | train_loss:1.5738 val_acc:81.5085 val_loss:0.4273
[09/0057] | train_loss:1.512 val_acc:81.2652 val_loss:0.435
[09/0058] | train_loss:1.4529 val_acc:81.2652 val_loss:0.4749
[09/0059] | train_loss:1.4858 val_acc:80.292 val_loss:0.4524
[09/0060] | train_loss:1.4835 val_acc:82.2384 val_loss:0.4246
[09/0061] | train_loss:1.4646 val_acc:79.562 val_loss:0.4088
[09/0062] | train_loss:1.4107 val_acc:83.2117 val_loss:0.4896
[09/0063] | train_loss:1.488 val_acc:81.7518 val_loss:0.4518
[09/0064] | train_loss:1.4242 val_acc:82.2384 val_loss:0.4667
[09/0065] | train_loss:1.3886 val_acc:81.7518 val_loss:0.4353
[09/0066] | train_loss:1.3804 val_acc:81.0219 val_loss:0.4414
[09/0067] | train_loss:1.3609 val_acc:81.7518 val_loss:0.4866
[09/0068] | train_loss:1.322 val_acc:80.7786 val_loss:0.4522
[09/0069] | train_loss:1.2732 val_acc:81.9951 val_loss:0.4968
[09/0070] | train_loss:1.3153 val_acc:81.0219 val_loss:0.4621
[09/0071] | train_loss:1.3443 val_acc:83.6983 val_loss:0.4703
[09/0072] | train_loss:1.2999 val_acc:81.2652 val_loss:0.4726
[09/0073] | train_loss:1.3458 val_acc:79.562 val_loss:0.5011
[09/0074] | train_loss:1.4027 val_acc:84.1849 val_loss:0.4222
[09/0075] | train_loss:1.3612 val_acc:81.9951 val_loss:0.4377
[09/0076] | train_loss:1.2765 val_acc:80.5353 val_loss:0.429
[09/0077] | train_loss:1.2552 val_acc:81.0219 val_loss:0.4559
[09/0078] | train_loss:1.2528 val_acc:80.7786 val_loss:0.4573
[09/0079] | train_loss:1.2132 val_acc:82.9684 val_loss:0.4334
[09/0080] | train_loss:1.2483 val_acc:83.9416 val_loss:0.4499
[09/0081] | train_loss:1.2324 val_acc:83.2117 val_loss:0.4658
[09/0082] | train_loss:1.1836 val_acc:80.7786 val_loss:0.5202
[09/0083] | train_loss:1.2779 val_acc:81.0219 val_loss:0.4938
[09/0084] | train_loss:1.2547 val_acc:83.2117 val_loss:0.427
[09/0085] | train_loss:1.2326 val_acc:81.2652 val_loss:0.5127
[09/0086] | train_loss:1.2712 val_acc:81.2652 val_loss:0.457
[09/0087] | train_loss:1.249 val_acc:81.5085 val_loss:0.5473
[09/0088] | train_loss:1.2192 val_acc:83.2117 val_loss:0.4879
[09/0089] | train_loss:1.1591 val_acc:82.4818 val_loss:0.4811
[09/0090] | train_loss:1.144 val_acc:83.6983 val_loss:0.5075
[09/0091] | train_loss:1.1344 val_acc:83.2117 val_loss:0.4393
[09/0092] | train_loss:1.1664 val_acc:83.6983 val_loss:0.5086
[09/0093] | train_loss:1.139 val_acc:80.5353 val_loss:0.6773
[09/0094] | train_loss:1.1407 val_acc:82.7251 val_loss:0.5989
[09/0095] | train_loss:1.1224 val_acc:81.7518 val_loss:0.565
[09/0096] | train_loss:1.1312 val_acc:85.8881 val_loss:0.4854
model is saved at epoch 96!![09/0097] | train_loss:1.2059 val_acc:81.2652 val_loss:0.5468
[09/0098] | train_loss:1.1706 val_acc:81.7518 val_loss:0.482
[09/0099] | train_loss:1.15 val_acc:81.9951 val_loss:0.4643
[09/0100] | train_loss:1.1641 val_acc:82.2384 val_loss:0.5178
[09/0101] | train_loss:1.0893 val_acc:82.9684 val_loss:0.5067
[09/0102] | train_loss:1.0368 val_acc:81.9951 val_loss:0.5996
[09/0103] | train_loss:1.0936 val_acc:82.2384 val_loss:0.5492
[09/0104] | train_loss:1.0851 val_acc:83.455 val_loss:0.5407
[09/0105] | train_loss:1.0998 val_acc:82.2384 val_loss:0.5528
[09/0106] | train_loss:1.0958 val_acc:81.5085 val_loss:0.435
[09/0107] | train_loss:1.1219 val_acc:84.1849 val_loss:0.485
[09/0108] | train_loss:1.0428 val_acc:82.9684 val_loss:0.5187
[09/0109] | train_loss:0.9955 val_acc:83.455 val_loss:0.5395
[09/0110] | train_loss:0.998 val_acc:82.7251 val_loss:0.5559
[09/0111] | train_loss:0.999 val_acc:83.9416 val_loss:0.4919
[09/0112] | train_loss:1.003 val_acc:83.2117 val_loss:0.5415
[09/0113] | train_loss:0.9833 val_acc:82.7251 val_loss:0.6006
[09/0114] | train_loss:1.0491 val_acc:81.9951 val_loss:0.5874
[09/0115] | train_loss:1.0461 val_acc:83.6983 val_loss:0.4825
[09/0116] | train_loss:1.0388 val_acc:82.9684 val_loss:0.4889
[09/0117] | train_loss:0.9645 val_acc:82.9684 val_loss:0.4913
[09/0118] | train_loss:1.1012 val_acc:82.2384 val_loss:0.5627
[09/0119] | train_loss:1.2498 val_acc:83.9416 val_loss:0.6162
[09/0120] | train_loss:1.2262 val_acc:80.7786 val_loss:0.5826
[09/0121] | train_loss:1.2284 val_acc:83.9416 val_loss:0.5869
[09/0122] | train_loss:1.2533 val_acc:81.7518 val_loss:0.487
[09/0123] | train_loss:1.1157 val_acc:82.2384 val_loss:0.5219
[09/0124] | train_loss:1.1074 val_acc:83.2117 val_loss:0.5487
[09/0125] | train_loss:1.1219 val_acc:81.2652 val_loss:0.557
[09/0126] | train_loss:1.1071 val_acc:83.9416 val_loss:0.5963
[09/0127] | train_loss:1.0861 val_acc:81.9951 val_loss:0.5935
[09/0128] | train_loss:1.094 val_acc:83.6983 val_loss:0.4439
[09/0129] | train_loss:1.0945 val_acc:82.2384 val_loss:0.5858
[09/0130] | train_loss:1.0431 val_acc:81.5085 val_loss:0.6396
[09/0131] | train_loss:1.1982 val_acc:81.9951 val_loss:0.6207
[09/0132] | train_loss:1.083 val_acc:81.0219 val_loss:0.5166
[09/0133] | train_loss:1.0075 val_acc:82.7251 val_loss:0.5367
[09/0134] | train_loss:1.034 val_acc:82.9684 val_loss:0.6076
[09/0135] | train_loss:1.0297 val_acc:83.9416 val_loss:0.5103
[09/0136] | train_loss:1.0083 val_acc:83.2117 val_loss:0.605
[09/0137] | train_loss:1.0141 val_acc:82.9684 val_loss:0.6146
[09/0138] | train_loss:0.9664 val_acc:84.4282 val_loss:0.5967
[09/0139] | train_loss:0.958 val_acc:84.1849 val_loss:0.6666
[09/0140] | train_loss:0.9959 val_acc:82.9684 val_loss:0.6321
[09/0141] | train_loss:0.9911 val_acc:84.1849 val_loss:0.6168
[09/0142] | train_loss:1.0178 val_acc:83.2117 val_loss:0.6177
[09/0143] | train_loss:1.0176 val_acc:84.1849 val_loss:0.6312
[09/0144] | train_loss:1.0299 val_acc:81.7518 val_loss:0.6522
[09/0145] | train_loss:0.976 val_acc:83.455 val_loss:0.6088
[09/0146] | train_loss:0.9078 val_acc:83.6983 val_loss:0.6584
[09/0147] | train_loss:0.9268 val_acc:84.6715 val_loss:0.6218
Fold: [9/10] Test is finish !! 
 Test Metrics are: test_acc:82.4818 test_loss:0.8624fold [9/10] is start!!
[10/0001] | train_loss:2.6945 val_acc:49.1484 val_loss:0.6747
model is saved at epoch 1!![10/0002] | train_loss:2.5232 val_acc:49.1484 val_loss:0.777
[10/0003] | train_loss:2.4388 val_acc:57.9075 val_loss:0.5948
model is saved at epoch 3!![10/0004] | train_loss:2.3691 val_acc:76.399 val_loss:0.5357
model is saved at epoch 4!![10/0005] | train_loss:2.3579 val_acc:70.073 val_loss:0.5591
[10/0006] | train_loss:2.3017 val_acc:69.5864 val_loss:0.608
[10/0007] | train_loss:2.2645 val_acc:73.9659 val_loss:0.5236
[10/0008] | train_loss:2.2178 val_acc:73.4793 val_loss:0.5306
[10/0009] | train_loss:2.1756 val_acc:76.8856 val_loss:0.546
model is saved at epoch 9!![10/0010] | train_loss:2.1599 val_acc:78.1022 val_loss:0.5391
model is saved at epoch 10!![10/0011] | train_loss:2.1314 val_acc:77.8589 val_loss:0.5195
[10/0012] | train_loss:2.0985 val_acc:75.4258 val_loss:0.5394
[10/0013] | train_loss:2.0834 val_acc:76.1557 val_loss:0.5358
[10/0014] | train_loss:2.0293 val_acc:75.9124 val_loss:0.5297
[10/0015] | train_loss:1.9951 val_acc:73.9659 val_loss:0.5382
[10/0016] | train_loss:1.9607 val_acc:74.2092 val_loss:0.5462
[10/0017] | train_loss:2.0211 val_acc:77.3723 val_loss:0.5114
[10/0018] | train_loss:1.9886 val_acc:75.6691 val_loss:0.5128
[10/0019] | train_loss:1.9322 val_acc:78.5888 val_loss:0.523
model is saved at epoch 19!![10/0020] | train_loss:1.8903 val_acc:75.6691 val_loss:0.541
[10/0021] | train_loss:1.8637 val_acc:75.6691 val_loss:0.5177
[10/0022] | train_loss:1.8468 val_acc:77.3723 val_loss:0.5616
[10/0023] | train_loss:1.8727 val_acc:78.5888 val_loss:0.5053
[10/0024] | train_loss:1.8192 val_acc:78.3455 val_loss:0.5488
[10/0025] | train_loss:1.7945 val_acc:76.8856 val_loss:0.5396
[10/0026] | train_loss:1.807 val_acc:74.9392 val_loss:0.5716
[10/0027] | train_loss:1.8414 val_acc:78.3455 val_loss:0.5392
[10/0028] | train_loss:1.8037 val_acc:79.0754 val_loss:0.5412
model is saved at epoch 28!![10/0029] | train_loss:1.8337 val_acc:76.8856 val_loss:0.5178
[10/0030] | train_loss:1.781 val_acc:78.8321 val_loss:0.5403
[10/0031] | train_loss:1.7563 val_acc:79.0754 val_loss:0.5376
[10/0032] | train_loss:1.7617 val_acc:78.5888 val_loss:0.5314
[10/0033] | train_loss:1.7688 val_acc:77.8589 val_loss:0.5429
[10/0034] | train_loss:1.7065 val_acc:81.5085 val_loss:0.5508
model is saved at epoch 34!![10/0035] | train_loss:1.7213 val_acc:80.292 val_loss:0.5594
[10/0036] | train_loss:1.6908 val_acc:80.0487 val_loss:0.4974
[10/0037] | train_loss:1.6736 val_acc:78.8321 val_loss:0.5391
[10/0038] | train_loss:1.6819 val_acc:78.5888 val_loss:0.561
[10/0039] | train_loss:1.7247 val_acc:78.5888 val_loss:0.5243
[10/0040] | train_loss:1.6513 val_acc:80.5353 val_loss:0.5368
[10/0041] | train_loss:1.6885 val_acc:78.8321 val_loss:0.5481
[10/0042] | train_loss:1.6129 val_acc:80.5353 val_loss:0.5469
[10/0043] | train_loss:1.5534 val_acc:81.2652 val_loss:0.548
[10/0044] | train_loss:1.5557 val_acc:81.7518 val_loss:0.5558
model is saved at epoch 44!![10/0045] | train_loss:1.5755 val_acc:79.3187 val_loss:0.589
[10/0046] | train_loss:1.5636 val_acc:79.3187 val_loss:0.5818
[10/0047] | train_loss:1.5543 val_acc:79.562 val_loss:0.5817
[10/0048] | train_loss:1.4681 val_acc:81.5085 val_loss:0.6081
[10/0049] | train_loss:1.5187 val_acc:79.562 val_loss:0.5809
[10/0050] | train_loss:1.4969 val_acc:79.3187 val_loss:0.5996
[10/0051] | train_loss:1.5003 val_acc:78.8321 val_loss:0.6167
[10/0052] | train_loss:1.4451 val_acc:80.5353 val_loss:0.5853
[10/0053] | train_loss:1.4849 val_acc:80.292 val_loss:0.622
[10/0054] | train_loss:1.4008 val_acc:81.2652 val_loss:0.6475
[10/0055] | train_loss:1.3872 val_acc:79.3187 val_loss:0.6618
[10/0056] | train_loss:1.4073 val_acc:80.0487 val_loss:0.6298
[10/0057] | train_loss:1.4795 val_acc:76.6423 val_loss:0.6613
[10/0058] | train_loss:1.4309 val_acc:80.7786 val_loss:0.7608
[10/0059] | train_loss:1.3716 val_acc:79.0754 val_loss:0.6984
[10/0060] | train_loss:1.3315 val_acc:79.562 val_loss:0.6998
[10/0061] | train_loss:1.3584 val_acc:81.7518 val_loss:0.656
[10/0062] | train_loss:1.3168 val_acc:80.0487 val_loss:0.6637
[10/0063] | train_loss:1.3323 val_acc:81.0219 val_loss:0.6507
[10/0064] | train_loss:1.2963 val_acc:80.292 val_loss:0.6952
[10/0065] | train_loss:1.2441 val_acc:80.5353 val_loss:0.6979
[10/0066] | train_loss:1.2369 val_acc:81.7518 val_loss:0.6381
[10/0067] | train_loss:1.2405 val_acc:81.0219 val_loss:0.707
[10/0068] | train_loss:1.2193 val_acc:80.0487 val_loss:0.7037
[10/0069] | train_loss:1.2165 val_acc:80.7786 val_loss:0.7341
[10/0070] | train_loss:1.2169 val_acc:78.8321 val_loss:0.756
[10/0071] | train_loss:1.3054 val_acc:81.9951 val_loss:0.5863
model is saved at epoch 71!![10/0072] | train_loss:1.2684 val_acc:78.8321 val_loss:0.7867
[10/0073] | train_loss:1.215 val_acc:76.8856 val_loss:0.6857
[10/0074] | train_loss:1.2277 val_acc:80.0487 val_loss:0.7403
[10/0075] | train_loss:1.2127 val_acc:79.3187 val_loss:0.7452
[10/0076] | train_loss:1.1861 val_acc:79.0754 val_loss:0.7389
[10/0077] | train_loss:1.1554 val_acc:79.562 val_loss:0.8322
[10/0078] | train_loss:1.1819 val_acc:80.5353 val_loss:0.7974
[10/0079] | train_loss:1.1278 val_acc:81.0219 val_loss:0.7937
[10/0080] | train_loss:1.1431 val_acc:77.6156 val_loss:0.7806
[10/0081] | train_loss:1.1303 val_acc:79.0754 val_loss:0.8044
[10/0082] | train_loss:1.1776 val_acc:79.0754 val_loss:0.7336
[10/0083] | train_loss:1.1349 val_acc:79.8054 val_loss:0.8203
[10/0084] | train_loss:1.1045 val_acc:81.9951 val_loss:0.8618
[10/0085] | train_loss:1.0688 val_acc:80.5353 val_loss:0.8371
[10/0086] | train_loss:1.0766 val_acc:81.5085 val_loss:0.9622
[10/0087] | train_loss:1.0727 val_acc:80.292 val_loss:0.948
[10/0088] | train_loss:1.0918 val_acc:81.7518 val_loss:0.8098
[10/0089] | train_loss:1.1331 val_acc:80.0487 val_loss:0.7886
[10/0090] | train_loss:1.1054 val_acc:80.292 val_loss:0.8179
[10/0091] | train_loss:1.0246 val_acc:79.3187 val_loss:0.9067
[10/0092] | train_loss:1.0703 val_acc:78.8321 val_loss:0.8704
[10/0093] | train_loss:1.0389 val_acc:80.7786 val_loss:0.9
[10/0094] | train_loss:1.0527 val_acc:78.5888 val_loss:0.8954
[10/0095] | train_loss:1.0055 val_acc:79.562 val_loss:0.9591
[10/0096] | train_loss:0.9875 val_acc:79.562 val_loss:0.8955
[10/0097] | train_loss:1.009 val_acc:82.2384 val_loss:0.9778
model is saved at epoch 97!![10/0098] | train_loss:1.1667 val_acc:79.562 val_loss:0.9259
[10/0099] | train_loss:1.1171 val_acc:81.2652 val_loss:0.7692
[10/0100] | train_loss:1.03 val_acc:79.8054 val_loss:0.7803
[10/0101] | train_loss:0.97 val_acc:79.8054 val_loss:0.8306
[10/0102] | train_loss:0.9046 val_acc:79.8054 val_loss:0.9041
[10/0103] | train_loss:1.0 val_acc:79.562 val_loss:0.8664
[10/0104] | train_loss:0.8992 val_acc:81.0219 val_loss:0.9191
[10/0105] | train_loss:0.9091 val_acc:80.5353 val_loss:0.923
[10/0106] | train_loss:0.9651 val_acc:79.562 val_loss:0.9162
[10/0107] | train_loss:0.961 val_acc:80.0487 val_loss:0.9379
[10/0108] | train_loss:0.93 val_acc:80.292 val_loss:1.0076
[10/0109] | train_loss:0.8768 val_acc:80.7786 val_loss:0.947
[10/0110] | train_loss:0.911 val_acc:80.292 val_loss:0.9357
[10/0111] | train_loss:0.9007 val_acc:79.0754 val_loss:1.0471
[10/0112] | train_loss:0.8952 val_acc:80.7786 val_loss:1.0379
[10/0113] | train_loss:0.8599 val_acc:82.7251 val_loss:1.0227
model is saved at epoch 113!![10/0114] | train_loss:0.9672 val_acc:81.7518 val_loss:0.9652
[10/0115] | train_loss:0.9834 val_acc:80.0487 val_loss:0.8722
[10/0116] | train_loss:0.9772 val_acc:80.0487 val_loss:0.7928
[10/0117] | train_loss:0.9119 val_acc:80.0487 val_loss:0.8937
[10/0118] | train_loss:0.8725 val_acc:82.2384 val_loss:0.9702
[10/0119] | train_loss:0.9104 val_acc:80.7786 val_loss:1.0757
[10/0120] | train_loss:0.8885 val_acc:81.0219 val_loss:1.1326
[10/0121] | train_loss:0.8096 val_acc:79.8054 val_loss:1.1084
[10/0122] | train_loss:0.7984 val_acc:81.7518 val_loss:1.18
[10/0123] | train_loss:0.8316 val_acc:80.7786 val_loss:1.1862
[10/0124] | train_loss:0.7632 val_acc:81.0219 val_loss:1.156
[10/0125] | train_loss:0.8024 val_acc:80.5353 val_loss:1.1612
[10/0126] | train_loss:0.8528 val_acc:79.0754 val_loss:1.0696
[10/0127] | train_loss:0.8888 val_acc:80.5353 val_loss:0.9255
[10/0128] | train_loss:0.8633 val_acc:81.5085 val_loss:0.9622
[10/0129] | train_loss:0.7742 val_acc:79.8054 val_loss:1.0507
[10/0130] | train_loss:0.8365 val_acc:82.7251 val_loss:1.0222
[10/0131] | train_loss:0.8021 val_acc:80.292 val_loss:0.9901
[10/0132] | train_loss:0.854 val_acc:81.0219 val_loss:0.9465
[10/0133] | train_loss:1.0103 val_acc:81.2652 val_loss:0.9183
[10/0134] | train_loss:1.0075 val_acc:80.0487 val_loss:1.0199
[10/0135] | train_loss:0.8663 val_acc:81.2652 val_loss:1.0234
[10/0136] | train_loss:0.8458 val_acc:81.2652 val_loss:1.1335
[10/0137] | train_loss:0.7506 val_acc:80.5353 val_loss:1.102
[10/0138] | train_loss:0.7643 val_acc:81.2652 val_loss:1.1188
[10/0139] | train_loss:0.8881 val_acc:81.0219 val_loss:1.0609
[10/0140] | train_loss:0.9153 val_acc:81.0219 val_loss:1.1213
[10/0141] | train_loss:0.8584 val_acc:81.7518 val_loss:1.0087
[10/0142] | train_loss:0.765 val_acc:81.7518 val_loss:1.1739
[10/0143] | train_loss:0.7279 val_acc:81.9951 val_loss:1.2206
[10/0144] | train_loss:0.7768 val_acc:81.9951 val_loss:0.9511
[10/0145] | train_loss:0.8605 val_acc:83.2117 val_loss:1.1133
model is saved at epoch 145!![10/0146] | train_loss:0.8319 val_acc:81.0219 val_loss:1.231
[10/0147] | train_loss:0.8426 val_acc:82.2384 val_loss:0.9835
[10/0148] | train_loss:0.8518 val_acc:81.5085 val_loss:0.9101
[10/0149] | train_loss:0.7518 val_acc:83.2117 val_loss:1.0312
[10/0150] | train_loss:0.7417 val_acc:81.9951 val_loss:1.1701
[10/0151] | train_loss:0.7717 val_acc:80.7786 val_loss:1.1362
[10/0152] | train_loss:0.7277 val_acc:80.5353 val_loss:1.204
[10/0153] | train_loss:0.6919 val_acc:80.292 val_loss:1.2275
[10/0154] | train_loss:0.6887 val_acc:80.5353 val_loss:1.2357
[10/0155] | train_loss:0.7245 val_acc:79.3187 val_loss:1.0239
[10/0156] | train_loss:0.7581 val_acc:80.7786 val_loss:1.1218
[10/0157] | train_loss:0.7765 val_acc:79.562 val_loss:1.044
[10/0158] | train_loss:0.6902 val_acc:80.5353 val_loss:1.196
[10/0159] | train_loss:0.6932 val_acc:82.4818 val_loss:1.2131
[10/0160] | train_loss:0.6507 val_acc:81.9951 val_loss:1.4914
[10/0161] | train_loss:0.6524 val_acc:80.5353 val_loss:1.2091
[10/0162] | train_loss:0.6883 val_acc:82.2384 val_loss:1.2621
[10/0163] | train_loss:0.6156 val_acc:81.5085 val_loss:1.2518
[10/0164] | train_loss:0.5933 val_acc:81.7518 val_loss:1.2828
[10/0165] | train_loss:0.634 val_acc:81.2652 val_loss:1.2657
[10/0166] | train_loss:0.6696 val_acc:80.5353 val_loss:1.3796
[10/0167] | train_loss:0.6488 val_acc:81.9951 val_loss:1.1622
[10/0168] | train_loss:0.6191 val_acc:81.0219 val_loss:1.3496
[10/0169] | train_loss:0.6148 val_acc:82.9684 val_loss:1.3639
[10/0170] | train_loss:0.6688 val_acc:81.9951 val_loss:1.2437
[10/0171] | train_loss:0.6598 val_acc:83.2117 val_loss:1.0892
[10/0172] | train_loss:0.6146 val_acc:81.2652 val_loss:1.3133
[10/0173] | train_loss:0.5907 val_acc:81.9951 val_loss:1.235
[10/0174] | train_loss:0.6011 val_acc:79.562 val_loss:1.2959
[10/0175] | train_loss:0.6006 val_acc:82.7251 val_loss:1.2696
[10/0176] | train_loss:0.5988 val_acc:81.2652 val_loss:1.3072
[10/0177] | train_loss:0.642 val_acc:80.292 val_loss:1.3341
[10/0178] | train_loss:0.6068 val_acc:81.5085 val_loss:1.4197
[10/0179] | train_loss:0.576 val_acc:80.292 val_loss:1.4245
[10/0180] | train_loss:0.5994 val_acc:81.0219 val_loss:1.4526
[10/0181] | train_loss:0.7095 val_acc:81.0219 val_loss:1.317
[10/0182] | train_loss:0.7056 val_acc:79.3187 val_loss:1.162
[10/0183] | train_loss:0.6008 val_acc:82.2384 val_loss:1.2876
[10/0184] | train_loss:0.5751 val_acc:81.0219 val_loss:1.4304
[10/0185] | train_loss:0.6005 val_acc:79.8054 val_loss:1.4437
[10/0186] | train_loss:0.614 val_acc:82.4818 val_loss:1.2907
[10/0187] | train_loss:0.6186 val_acc:81.2652 val_loss:1.2832
[10/0188] | train_loss:0.6224 val_acc:81.5085 val_loss:1.1962
[10/0189] | train_loss:0.6429 val_acc:81.7518 val_loss:1.2426
[10/0190] | train_loss:0.6193 val_acc:81.2652 val_loss:1.3021
[10/0191] | train_loss:0.6095 val_acc:81.2652 val_loss:1.3015
[10/0192] | train_loss:0.5666 val_acc:81.2652 val_loss:1.2285
[10/0193] | train_loss:0.6238 val_acc:80.292 val_loss:1.2529
[10/0194] | train_loss:0.5657 val_acc:82.9684 val_loss:1.4323
[10/0195] | train_loss:0.6754 val_acc:80.7786 val_loss:1.1936
[10/0196] | train_loss:0.6651 val_acc:80.5353 val_loss:1.3009
Fold: [10/10] Test is finish !! 
 Test Metrics are: test_acc:83.455 test_loss:0.7072
all fold acc is: 
[85.6447696685791, 81.2652051448822, 83.45499038696289, 80.29196858406067, 83.45499038696289, 79.8053503036499, 78.34550142288208, 82.7250599861145, 82.48175382614136, 83.45499038696289] 
Test is finish !! 
 Test Metrics are: acc_mean:82.0925 acc_std:2.0478