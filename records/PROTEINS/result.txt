Dataset: PROTEINS,
Model Name: EnsNet
net_params={'num_layers': 3, 'hidden': 64, 'dropout': 0.1, 'ds_name': 'PROTEINS', 'model_type': 'ThreeModel', 'temperature': 1, 'beta': 2, 'gama': 1, 'yta': 150, 'alpha': 1, 'in_channels': 3, 'out_channels': 2, 'device': 'cuda:3'}
train_config={'epochs': 300, 'batch_size': 64, 'seed': 8971, 'patience': 50, 'lr': 0.01, 'weight_decay': 1e-05}
EnsNet(
  (model1): SAGPool(
    (convs): ModuleList(
      (0): SAGEConv(3, 64)
      (1): SAGEConv(64, 64)
    )
    (pools): ModuleList(
      (0): SAGPooling(
        (score_layer): GCNConv(128, 1)
      )
    )
    (lin1): Linear(in_features=256, out_features=64, bias=True)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
    )
    (lin2): Linear(in_features=64, out_features=32, bias=True)
    (lin3): Linear(in_features=32, out_features=2, bias=True)
  )
  (model2): SAGPool(
    (convs): ModuleList(
      (0): SAGEConv(3, 64)
      (1): SAGEConv(64, 64)
    )
    (pools): ModuleList(
      (0): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
      (1): SAGPooling(
        (score_layer): GCNConv(64, 1)
      )
    )
    (lin1): Linear(in_features=128, out_features=64, bias=True)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
    )
    (lin2): Linear(in_features=64, out_features=32, bias=True)
    (lin3): Linear(in_features=32, out_features=2, bias=True)
  )
  (model3): GlobalAttentionNet(
    (convs): ModuleList(
      (0): SAGEConv(3, 64)
      (1): SAGEConv(64, 64)
    )
    (att): GlobalAttention(gate_nn=Linear(in_features=64, out_features=1, bias=True), nn=None)
    (dropout): Dropout(p=0.4, inplace=False)
    (gsnorm): GraphSizeNorm()
    (bns): ModuleList(
      (0): BatchNorm(64)
      (1): BatchNorm(64)
    )
    (lin1): Linear(in_features=64, out_features=64, bias=True)
    (lin2): Linear(in_features=64, out_features=2, bias=True)
  )
  (confiNet): Sequential(
    (0): Linear(in_features=66, out_features=33, bias=True)
    (1): ReLU()
    (2): Linear(in_features=33, out_features=1, bias=True)
    (3): Sigmoid()
  )
)

fold [0/10] is start!!
[01/0001] | train_loss:2.7342 val_acc:64.8649 val_loss:0.662
model is saved at epoch 1!![01/0002] | train_loss:2.5253 val_acc:70.2703 val_loss:0.6336
model is saved at epoch 2!![01/0003] | train_loss:2.3983 val_acc:72.0721 val_loss:0.6029
model is saved at epoch 3!![01/0004] | train_loss:2.3821 val_acc:72.973 val_loss:0.565
model is saved at epoch 4!![01/0005] | train_loss:2.3551 val_acc:75.6757 val_loss:0.5328
model is saved at epoch 5!![01/0006] | train_loss:2.3511 val_acc:74.7748 val_loss:0.4836
[01/0007] | train_loss:2.2498 val_acc:72.973 val_loss:0.506
[01/0008] | train_loss:2.2787 val_acc:75.6757 val_loss:0.4913
[01/0009] | train_loss:2.2895 val_acc:72.973 val_loss:0.5565
[01/0010] | train_loss:2.2534 val_acc:75.6757 val_loss:0.5114
[01/0011] | train_loss:2.2519 val_acc:74.7748 val_loss:0.5146
[01/0012] | train_loss:2.2804 val_acc:76.5766 val_loss:0.5409
model is saved at epoch 12!![01/0013] | train_loss:2.2785 val_acc:74.7748 val_loss:0.5368
[01/0014] | train_loss:2.2631 val_acc:73.8739 val_loss:0.5484
[01/0015] | train_loss:2.3225 val_acc:76.5766 val_loss:0.4815
[01/0016] | train_loss:2.2606 val_acc:78.3784 val_loss:0.499
model is saved at epoch 16!![01/0017] | train_loss:2.228 val_acc:74.7748 val_loss:0.5531
[01/0018] | train_loss:2.2431 val_acc:74.7748 val_loss:0.5679
[01/0019] | train_loss:2.3075 val_acc:75.6757 val_loss:0.5561
[01/0020] | train_loss:2.2413 val_acc:77.4775 val_loss:0.5179
[01/0021] | train_loss:2.2338 val_acc:76.5766 val_loss:0.5029
[01/0022] | train_loss:2.275 val_acc:75.6757 val_loss:0.5284
[01/0023] | train_loss:2.2666 val_acc:75.6757 val_loss:0.5081
[01/0024] | train_loss:2.2424 val_acc:74.7748 val_loss:0.5394
[01/0025] | train_loss:2.2302 val_acc:77.4775 val_loss:0.5023
[01/0026] | train_loss:2.2339 val_acc:76.5766 val_loss:0.4862
[01/0027] | train_loss:2.2948 val_acc:75.6757 val_loss:0.4959
[01/0028] | train_loss:2.2696 val_acc:76.5766 val_loss:0.4815
[01/0029] | train_loss:2.2423 val_acc:74.7748 val_loss:0.4845
[01/0030] | train_loss:2.2124 val_acc:77.4775 val_loss:0.4832
[01/0031] | train_loss:2.2059 val_acc:77.4775 val_loss:0.4823
[01/0032] | train_loss:2.2483 val_acc:76.5766 val_loss:0.4688
[01/0033] | train_loss:2.2244 val_acc:75.6757 val_loss:0.5105
[01/0034] | train_loss:2.1856 val_acc:78.3784 val_loss:0.4716
[01/0035] | train_loss:2.2353 val_acc:75.6757 val_loss:0.5093
[01/0036] | train_loss:2.2197 val_acc:80.1802 val_loss:0.4823
model is saved at epoch 36!![01/0037] | train_loss:2.231 val_acc:78.3784 val_loss:0.4874
[01/0038] | train_loss:2.2122 val_acc:78.3784 val_loss:0.4837
[01/0039] | train_loss:2.246 val_acc:77.4775 val_loss:0.4793
[01/0040] | train_loss:2.2161 val_acc:77.4775 val_loss:0.4832
[01/0041] | train_loss:2.2297 val_acc:78.3784 val_loss:0.5017
[01/0042] | train_loss:2.2654 val_acc:77.4775 val_loss:0.4835
[01/0043] | train_loss:2.3456 val_acc:73.8739 val_loss:0.505
[01/0044] | train_loss:2.2176 val_acc:75.6757 val_loss:0.5108
[01/0045] | train_loss:2.2377 val_acc:76.5766 val_loss:0.516
[01/0046] | train_loss:2.2043 val_acc:74.7748 val_loss:0.5
[01/0047] | train_loss:2.2335 val_acc:76.5766 val_loss:0.506
[01/0048] | train_loss:2.2136 val_acc:74.7748 val_loss:0.4741
[01/0049] | train_loss:2.2004 val_acc:76.5766 val_loss:0.4907
[01/0050] | train_loss:2.2157 val_acc:77.4775 val_loss:0.4825
[01/0051] | train_loss:2.2929 val_acc:78.3784 val_loss:0.4853
[01/0052] | train_loss:2.2558 val_acc:78.3784 val_loss:0.4867
[01/0053] | train_loss:2.2363 val_acc:77.4775 val_loss:0.4957
[01/0054] | train_loss:2.3004 val_acc:77.4775 val_loss:0.5026
[01/0055] | train_loss:2.2479 val_acc:77.4775 val_loss:0.482
[01/0056] | train_loss:2.2595 val_acc:76.5766 val_loss:0.4774
[01/0057] | train_loss:2.1803 val_acc:74.7748 val_loss:0.4811
[01/0058] | train_loss:2.281 val_acc:77.4775 val_loss:0.4792
[01/0059] | train_loss:2.1994 val_acc:78.3784 val_loss:0.4732
[01/0060] | train_loss:2.202 val_acc:80.1802 val_loss:0.4808
[01/0061] | train_loss:2.2166 val_acc:78.3784 val_loss:0.4874
[01/0062] | train_loss:2.1836 val_acc:77.4775 val_loss:0.4649
[01/0063] | train_loss:2.2337 val_acc:79.2793 val_loss:0.4921
[01/0064] | train_loss:2.2066 val_acc:76.5766 val_loss:0.459
[01/0065] | train_loss:2.2113 val_acc:78.3784 val_loss:0.4784
[01/0066] | train_loss:2.244 val_acc:74.7748 val_loss:0.4898
[01/0067] | train_loss:2.2306 val_acc:76.5766 val_loss:0.4824
[01/0068] | train_loss:2.1907 val_acc:77.4775 val_loss:0.4769
[01/0069] | train_loss:2.2325 val_acc:75.6757 val_loss:0.4928
[01/0070] | train_loss:2.1874 val_acc:73.8739 val_loss:0.476
[01/0071] | train_loss:2.2055 val_acc:76.5766 val_loss:0.4712
[01/0072] | train_loss:2.2042 val_acc:76.5766 val_loss:0.4609
[01/0073] | train_loss:2.19 val_acc:76.5766 val_loss:0.4698
[01/0074] | train_loss:2.2081 val_acc:79.2793 val_loss:0.4618
[01/0075] | train_loss:2.2591 val_acc:74.7748 val_loss:0.5134
[01/0076] | train_loss:2.2294 val_acc:76.5766 val_loss:0.4779
[01/0077] | train_loss:2.1755 val_acc:77.4775 val_loss:0.5136
[01/0078] | train_loss:2.178 val_acc:77.4775 val_loss:0.5242
[01/0079] | train_loss:2.2269 val_acc:79.2793 val_loss:0.4842
[01/0080] | train_loss:2.2225 val_acc:76.5766 val_loss:0.5142
[01/0081] | train_loss:2.2191 val_acc:74.7748 val_loss:0.5366
[01/0082] | train_loss:2.1714 val_acc:78.3784 val_loss:0.5276
[01/0083] | train_loss:2.1645 val_acc:76.5766 val_loss:0.5534
[01/0084] | train_loss:2.1874 val_acc:76.5766 val_loss:0.4876
[01/0085] | train_loss:2.1871 val_acc:79.2793 val_loss:0.5717
[01/0086] | train_loss:2.1124 val_acc:80.1802 val_loss:0.4915
[01/0087] | train_loss:2.2036 val_acc:78.3784 val_loss:0.583
Fold: [1/10] Test is finish !! 
 Test Metrics are: test_acc:77.6786 test_loss:0.5254fold [1/10] is start!!
[02/0001] | train_loss:2.6834 val_acc:57.1429 val_loss:0.6915
model is saved at epoch 1!![02/0002] | train_loss:2.4258 val_acc:59.8214 val_loss:0.6774
model is saved at epoch 2!![02/0003] | train_loss:2.3409 val_acc:65.1786 val_loss:0.6362
model is saved at epoch 3!![02/0004] | train_loss:2.3343 val_acc:71.4286 val_loss:0.57
model is saved at epoch 4!![02/0005] | train_loss:2.3245 val_acc:73.2143 val_loss:0.5562
model is saved at epoch 5!![02/0006] | train_loss:2.3209 val_acc:78.5714 val_loss:0.5129
model is saved at epoch 6!![02/0007] | train_loss:2.3109 val_acc:75.0 val_loss:0.5472
[02/0008] | train_loss:2.2762 val_acc:70.5357 val_loss:0.5396
[02/0009] | train_loss:2.2142 val_acc:80.3571 val_loss:0.5179
model is saved at epoch 9!![02/0010] | train_loss:2.2506 val_acc:71.4286 val_loss:0.5511
[02/0011] | train_loss:2.2587 val_acc:75.0 val_loss:0.5223
[02/0012] | train_loss:2.2462 val_acc:77.6786 val_loss:0.5195
[02/0013] | train_loss:2.278 val_acc:76.7857 val_loss:0.534
[02/0014] | train_loss:2.2533 val_acc:80.3571 val_loss:0.5154
[02/0015] | train_loss:2.245 val_acc:78.5714 val_loss:0.5256
[02/0016] | train_loss:2.2344 val_acc:77.6786 val_loss:0.5342
[02/0017] | train_loss:2.2173 val_acc:77.6786 val_loss:0.5148
[02/0018] | train_loss:2.1901 val_acc:76.7857 val_loss:0.505
[02/0019] | train_loss:2.2151 val_acc:75.8929 val_loss:0.5437
[02/0020] | train_loss:2.1826 val_acc:76.7857 val_loss:0.5622
[02/0021] | train_loss:2.2249 val_acc:76.7857 val_loss:0.5467
[02/0022] | train_loss:2.1587 val_acc:75.0 val_loss:0.5695
[02/0023] | train_loss:2.2072 val_acc:76.7857 val_loss:0.5204
[02/0024] | train_loss:2.1587 val_acc:75.0 val_loss:0.5139
[02/0025] | train_loss:2.1548 val_acc:76.7857 val_loss:0.5238
[02/0026] | train_loss:2.1326 val_acc:77.6786 val_loss:0.5217
[02/0027] | train_loss:2.1645 val_acc:75.8929 val_loss:0.5209
[02/0028] | train_loss:2.1287 val_acc:77.6786 val_loss:0.518
[02/0029] | train_loss:2.1607 val_acc:75.8929 val_loss:0.5507
[02/0030] | train_loss:2.2712 val_acc:75.0 val_loss:0.5293
[02/0031] | train_loss:2.1827 val_acc:77.6786 val_loss:0.5321
[02/0032] | train_loss:2.1665 val_acc:80.3571 val_loss:0.5189
[02/0033] | train_loss:2.2135 val_acc:78.5714 val_loss:0.5297
[02/0034] | train_loss:2.2116 val_acc:79.4643 val_loss:0.5124
[02/0035] | train_loss:2.1573 val_acc:75.8929 val_loss:0.5182
[02/0036] | train_loss:2.2156 val_acc:78.5714 val_loss:0.5131
[02/0037] | train_loss:2.2209 val_acc:76.7857 val_loss:0.5179
[02/0038] | train_loss:2.2397 val_acc:77.6786 val_loss:0.5218
[02/0039] | train_loss:2.2109 val_acc:75.0 val_loss:0.5328
[02/0040] | train_loss:2.2769 val_acc:77.6786 val_loss:0.5115
[02/0041] | train_loss:2.239 val_acc:78.5714 val_loss:0.5156
[02/0042] | train_loss:2.1535 val_acc:75.0 val_loss:0.4996
[02/0043] | train_loss:2.2624 val_acc:78.5714 val_loss:0.5392
[02/0044] | train_loss:2.2252 val_acc:77.6786 val_loss:0.5133
[02/0045] | train_loss:2.2202 val_acc:75.8929 val_loss:0.5152
[02/0046] | train_loss:2.1743 val_acc:77.6786 val_loss:0.5344
[02/0047] | train_loss:2.1782 val_acc:78.5714 val_loss:0.5145
[02/0048] | train_loss:2.1507 val_acc:77.6786 val_loss:0.5184
[02/0049] | train_loss:2.1571 val_acc:75.8929 val_loss:0.5275
[02/0050] | train_loss:2.1576 val_acc:76.7857 val_loss:0.4972
[02/0051] | train_loss:2.1423 val_acc:77.6786 val_loss:0.5118
[02/0052] | train_loss:2.1633 val_acc:76.7857 val_loss:0.525
[02/0053] | train_loss:2.1213 val_acc:79.4643 val_loss:0.5128
[02/0054] | train_loss:2.1362 val_acc:77.6786 val_loss:0.522
[02/0055] | train_loss:2.1241 val_acc:75.8929 val_loss:0.5226
[02/0056] | train_loss:2.1229 val_acc:76.7857 val_loss:0.5255
[02/0057] | train_loss:2.1285 val_acc:76.7857 val_loss:0.5329
[02/0058] | train_loss:2.1135 val_acc:77.6786 val_loss:0.5285
[02/0059] | train_loss:2.0819 val_acc:78.5714 val_loss:0.5317
[02/0060] | train_loss:2.1457 val_acc:77.6786 val_loss:0.5153
Fold: [2/10] Test is finish !! 
 Test Metrics are: test_acc:71.4286 test_loss:0.5511fold [2/10] is start!!
[03/0001] | train_loss:2.6197 val_acc:55.3571 val_loss:0.7076
model is saved at epoch 1!![03/0002] | train_loss:2.5243 val_acc:56.25 val_loss:0.6926
model is saved at epoch 2!![03/0003] | train_loss:2.443 val_acc:65.1786 val_loss:0.6491
model is saved at epoch 3!![03/0004] | train_loss:2.3483 val_acc:62.5 val_loss:0.6351
[03/0005] | train_loss:2.3195 val_acc:72.3214 val_loss:0.562
model is saved at epoch 5!![03/0006] | train_loss:2.2997 val_acc:65.1786 val_loss:0.6143
[03/0007] | train_loss:2.3082 val_acc:73.2143 val_loss:0.5898
model is saved at epoch 7!![03/0008] | train_loss:2.2726 val_acc:72.3214 val_loss:0.5604
[03/0009] | train_loss:2.2503 val_acc:70.5357 val_loss:0.5853
[03/0010] | train_loss:2.3019 val_acc:69.6429 val_loss:0.5913
[03/0011] | train_loss:2.3125 val_acc:72.3214 val_loss:0.5466
[03/0012] | train_loss:2.2612 val_acc:71.4286 val_loss:0.5733
[03/0013] | train_loss:2.2368 val_acc:71.4286 val_loss:0.5692
[03/0014] | train_loss:2.2234 val_acc:67.8571 val_loss:0.6418
[03/0015] | train_loss:2.3284 val_acc:70.5357 val_loss:0.5425
[03/0016] | train_loss:2.2931 val_acc:70.5357 val_loss:0.5599
[03/0017] | train_loss:2.2538 val_acc:67.8571 val_loss:0.5838
[03/0018] | train_loss:2.2369 val_acc:68.75 val_loss:0.5744
[03/0019] | train_loss:2.2931 val_acc:64.2857 val_loss:0.6105
[03/0020] | train_loss:2.2878 val_acc:71.4286 val_loss:0.5626
[03/0021] | train_loss:2.2127 val_acc:71.4286 val_loss:0.5777
[03/0022] | train_loss:2.2546 val_acc:70.5357 val_loss:0.5538
[03/0023] | train_loss:2.1931 val_acc:71.4286 val_loss:0.5771
[03/0024] | train_loss:2.1874 val_acc:67.8571 val_loss:0.5789
[03/0025] | train_loss:2.1925 val_acc:67.8571 val_loss:0.5706
[03/0026] | train_loss:2.2794 val_acc:69.6429 val_loss:0.5895
[03/0027] | train_loss:2.1872 val_acc:69.6429 val_loss:0.5566
[03/0028] | train_loss:2.1983 val_acc:70.5357 val_loss:0.6057
[03/0029] | train_loss:2.2159 val_acc:70.5357 val_loss:0.5504
[03/0030] | train_loss:2.2012 val_acc:69.6429 val_loss:0.5843
[03/0031] | train_loss:2.2113 val_acc:70.5357 val_loss:0.5694
[03/0032] | train_loss:2.2353 val_acc:67.8571 val_loss:0.5592
[03/0033] | train_loss:2.2461 val_acc:74.1071 val_loss:0.5371
model is saved at epoch 33!![03/0034] | train_loss:2.2075 val_acc:69.6429 val_loss:0.5728
[03/0035] | train_loss:2.2115 val_acc:74.1071 val_loss:0.528
[03/0036] | train_loss:2.2187 val_acc:69.6429 val_loss:0.6213
[03/0037] | train_loss:2.2507 val_acc:71.4286 val_loss:0.5949
[03/0038] | train_loss:2.1985 val_acc:67.8571 val_loss:0.6151
[03/0039] | train_loss:2.2834 val_acc:71.4286 val_loss:0.5819
[03/0040] | train_loss:2.2354 val_acc:70.5357 val_loss:0.5804
[03/0041] | train_loss:2.1992 val_acc:70.5357 val_loss:0.5693
[03/0042] | train_loss:2.1637 val_acc:68.75 val_loss:0.5961
[03/0043] | train_loss:2.1983 val_acc:74.1071 val_loss:0.5516
[03/0044] | train_loss:2.1515 val_acc:68.75 val_loss:0.6122
[03/0045] | train_loss:2.2071 val_acc:69.6429 val_loss:0.5917
[03/0046] | train_loss:2.1757 val_acc:67.8571 val_loss:0.5855
[03/0047] | train_loss:2.1558 val_acc:74.1071 val_loss:0.5533
[03/0048] | train_loss:2.2031 val_acc:72.3214 val_loss:0.5987
[03/0049] | train_loss:2.1768 val_acc:73.2143 val_loss:0.5433
[03/0050] | train_loss:2.1941 val_acc:72.3214 val_loss:0.6053
[03/0051] | train_loss:2.2412 val_acc:72.3214 val_loss:0.5846
[03/0052] | train_loss:2.1877 val_acc:71.4286 val_loss:0.5598
[03/0053] | train_loss:2.1337 val_acc:70.5357 val_loss:0.5899
[03/0054] | train_loss:2.2078 val_acc:71.4286 val_loss:0.5674
[03/0055] | train_loss:2.2065 val_acc:71.4286 val_loss:0.5652
[03/0056] | train_loss:2.154 val_acc:72.3214 val_loss:0.5638
[03/0057] | train_loss:2.1702 val_acc:69.6429 val_loss:0.5891
[03/0058] | train_loss:2.2779 val_acc:70.5357 val_loss:0.5639
[03/0059] | train_loss:2.202 val_acc:70.5357 val_loss:0.5702
[03/0060] | train_loss:2.2315 val_acc:71.4286 val_loss:0.5729
[03/0061] | train_loss:2.1637 val_acc:73.2143 val_loss:0.557
[03/0062] | train_loss:2.1957 val_acc:68.75 val_loss:0.5838
[03/0063] | train_loss:2.1874 val_acc:70.5357 val_loss:0.5465
[03/0064] | train_loss:2.1476 val_acc:71.4286 val_loss:0.5803
[03/0065] | train_loss:2.1336 val_acc:71.4286 val_loss:0.604
[03/0066] | train_loss:2.1763 val_acc:70.5357 val_loss:0.5937
[03/0067] | train_loss:2.1849 val_acc:73.2143 val_loss:0.5651
[03/0068] | train_loss:2.149 val_acc:70.5357 val_loss:0.5733
[03/0069] | train_loss:2.1913 val_acc:69.6429 val_loss:0.5895
[03/0070] | train_loss:2.1749 val_acc:71.4286 val_loss:0.5516
[03/0071] | train_loss:2.1168 val_acc:71.4286 val_loss:0.5753
[03/0072] | train_loss:2.1269 val_acc:70.5357 val_loss:0.5621
[03/0073] | train_loss:2.1105 val_acc:74.1071 val_loss:0.5583
[03/0074] | train_loss:2.1621 val_acc:69.6429 val_loss:0.6083
[03/0075] | train_loss:2.1331 val_acc:73.2143 val_loss:0.5741
[03/0076] | train_loss:2.1896 val_acc:71.4286 val_loss:0.5692
[03/0077] | train_loss:2.1339 val_acc:69.6429 val_loss:0.5966
[03/0078] | train_loss:2.1201 val_acc:72.3214 val_loss:0.557
[03/0079] | train_loss:2.1303 val_acc:68.75 val_loss:0.5654
[03/0080] | train_loss:2.1541 val_acc:71.4286 val_loss:0.5843
[03/0081] | train_loss:2.1859 val_acc:68.75 val_loss:0.6132
[03/0082] | train_loss:2.142 val_acc:70.5357 val_loss:0.5679
[03/0083] | train_loss:2.1218 val_acc:71.4286 val_loss:0.5711
[03/0084] | train_loss:2.1856 val_acc:70.5357 val_loss:0.6032
Fold: [3/10] Test is finish !! 
 Test Metrics are: test_acc:82.1429 test_loss:0.5349fold [3/10] is start!!
[04/0001] | train_loss:2.6498 val_acc:60.7143 val_loss:0.6787
model is saved at epoch 1!![04/0002] | train_loss:2.3956 val_acc:63.3929 val_loss:0.6428
model is saved at epoch 2!![04/0003] | train_loss:2.4161 val_acc:71.4286 val_loss:0.595
model is saved at epoch 3!![04/0004] | train_loss:2.4112 val_acc:74.1071 val_loss:0.5753
model is saved at epoch 4!![04/0005] | train_loss:2.3341 val_acc:80.3571 val_loss:0.5241
model is saved at epoch 5!![04/0006] | train_loss:2.3145 val_acc:80.3571 val_loss:0.5296
[04/0007] | train_loss:2.2789 val_acc:79.4643 val_loss:0.5013
[04/0008] | train_loss:2.311 val_acc:75.8929 val_loss:0.5292
[04/0009] | train_loss:2.2709 val_acc:74.1071 val_loss:0.5369
[04/0010] | train_loss:2.2287 val_acc:83.0357 val_loss:0.5203
model is saved at epoch 10!![04/0011] | train_loss:2.2692 val_acc:81.25 val_loss:0.5236
[04/0012] | train_loss:2.2906 val_acc:76.7857 val_loss:0.5094
[04/0013] | train_loss:2.2564 val_acc:79.4643 val_loss:0.5187
[04/0014] | train_loss:2.2667 val_acc:83.9286 val_loss:0.4868
model is saved at epoch 14!![04/0015] | train_loss:2.2898 val_acc:78.5714 val_loss:0.5067
[04/0016] | train_loss:2.2836 val_acc:79.4643 val_loss:0.5207
[04/0017] | train_loss:2.1903 val_acc:80.3571 val_loss:0.4889
[04/0018] | train_loss:2.2324 val_acc:81.25 val_loss:0.498
[04/0019] | train_loss:2.2188 val_acc:76.7857 val_loss:0.5133
[04/0020] | train_loss:2.187 val_acc:82.1429 val_loss:0.516
[04/0021] | train_loss:2.2208 val_acc:80.3571 val_loss:0.4994
[04/0022] | train_loss:2.1994 val_acc:79.4643 val_loss:0.5087
[04/0023] | train_loss:2.228 val_acc:81.25 val_loss:0.5125
[04/0024] | train_loss:2.2544 val_acc:84.8214 val_loss:0.5125
model is saved at epoch 24!![04/0025] | train_loss:2.2182 val_acc:76.7857 val_loss:0.5223
[04/0026] | train_loss:2.2083 val_acc:82.1429 val_loss:0.5118
[04/0027] | train_loss:2.1986 val_acc:77.6786 val_loss:0.523
[04/0028] | train_loss:2.1558 val_acc:81.25 val_loss:0.53
[04/0029] | train_loss:2.1538 val_acc:82.1429 val_loss:0.5013
[04/0030] | train_loss:2.1483 val_acc:80.3571 val_loss:0.4995
[04/0031] | train_loss:2.1939 val_acc:78.5714 val_loss:0.5007
[04/0032] | train_loss:2.2027 val_acc:82.1429 val_loss:0.4839
[04/0033] | train_loss:2.1814 val_acc:83.0357 val_loss:0.4991
[04/0034] | train_loss:2.187 val_acc:80.3571 val_loss:0.5067
[04/0035] | train_loss:2.1716 val_acc:82.1429 val_loss:0.4841
[04/0036] | train_loss:2.2123 val_acc:79.4643 val_loss:0.5138
[04/0037] | train_loss:2.233 val_acc:83.0357 val_loss:0.5157
[04/0038] | train_loss:2.1961 val_acc:78.5714 val_loss:0.4993
[04/0039] | train_loss:2.1692 val_acc:79.4643 val_loss:0.5014
[04/0040] | train_loss:2.1868 val_acc:79.4643 val_loss:0.5522
[04/0041] | train_loss:2.1625 val_acc:78.5714 val_loss:0.4961
[04/0042] | train_loss:2.2164 val_acc:77.6786 val_loss:0.5329
[04/0043] | train_loss:2.2351 val_acc:82.1429 val_loss:0.5129
[04/0044] | train_loss:2.2203 val_acc:80.3571 val_loss:0.5113
[04/0045] | train_loss:2.2237 val_acc:81.25 val_loss:0.5279
[04/0046] | train_loss:2.1509 val_acc:82.1429 val_loss:0.4947
[04/0047] | train_loss:2.2115 val_acc:79.4643 val_loss:0.5045
[04/0048] | train_loss:2.1471 val_acc:78.5714 val_loss:0.5119
[04/0049] | train_loss:2.2081 val_acc:77.6786 val_loss:0.5166
[04/0050] | train_loss:2.1532 val_acc:81.25 val_loss:0.5427
[04/0051] | train_loss:2.3506 val_acc:80.3571 val_loss:0.5304
[04/0052] | train_loss:2.2001 val_acc:81.25 val_loss:0.4759
[04/0053] | train_loss:2.1509 val_acc:80.3571 val_loss:0.4941
[04/0054] | train_loss:2.1798 val_acc:82.1429 val_loss:0.5475
[04/0055] | train_loss:2.2228 val_acc:79.4643 val_loss:0.4942
[04/0056] | train_loss:2.2244 val_acc:79.4643 val_loss:0.5152
[04/0057] | train_loss:2.1471 val_acc:76.7857 val_loss:0.5148
[04/0058] | train_loss:2.1635 val_acc:76.7857 val_loss:0.5069
[04/0059] | train_loss:2.1183 val_acc:80.3571 val_loss:0.5062
[04/0060] | train_loss:2.1491 val_acc:83.0357 val_loss:0.5134
[04/0061] | train_loss:2.1744 val_acc:78.5714 val_loss:0.5283
[04/0062] | train_loss:2.1353 val_acc:82.1429 val_loss:0.5014
[04/0063] | train_loss:2.1738 val_acc:80.3571 val_loss:0.4997
[04/0064] | train_loss:2.1647 val_acc:79.4643 val_loss:0.5013
[04/0065] | train_loss:2.1542 val_acc:79.4643 val_loss:0.49
[04/0066] | train_loss:2.1975 val_acc:80.3571 val_loss:0.4947
[04/0067] | train_loss:2.1817 val_acc:77.6786 val_loss:0.5082
[04/0068] | train_loss:2.1516 val_acc:78.5714 val_loss:0.5159
[04/0069] | train_loss:2.1939 val_acc:80.3571 val_loss:0.5151
[04/0070] | train_loss:2.1964 val_acc:82.1429 val_loss:0.4909
[04/0071] | train_loss:2.197 val_acc:80.3571 val_loss:0.5076
[04/0072] | train_loss:2.1589 val_acc:77.6786 val_loss:0.5209
[04/0073] | train_loss:2.2399 val_acc:78.5714 val_loss:0.5251
[04/0074] | train_loss:2.1531 val_acc:83.0357 val_loss:0.5186
[04/0075] | train_loss:2.1831 val_acc:74.1071 val_loss:0.5367
Fold: [4/10] Test is finish !! 
 Test Metrics are: test_acc:71.1712 test_loss:0.5549fold [4/10] is start!!
[05/0001] | train_loss:2.663 val_acc:63.0631 val_loss:0.6783
model is saved at epoch 1!![05/0002] | train_loss:2.4441 val_acc:63.0631 val_loss:0.7055
[05/0003] | train_loss:2.3256 val_acc:65.7658 val_loss:0.6734
model is saved at epoch 3!![05/0004] | train_loss:2.339 val_acc:67.5676 val_loss:0.6361
model is saved at epoch 4!![05/0005] | train_loss:2.2783 val_acc:69.3694 val_loss:0.5916
model is saved at epoch 5!![05/0006] | train_loss:2.2695 val_acc:68.4685 val_loss:0.5928
[05/0007] | train_loss:2.2606 val_acc:71.1712 val_loss:0.566
model is saved at epoch 7!![05/0008] | train_loss:2.2914 val_acc:71.1712 val_loss:0.5837
[05/0009] | train_loss:2.2777 val_acc:69.3694 val_loss:0.6201
[05/0010] | train_loss:2.2401 val_acc:70.2703 val_loss:0.5689
[05/0011] | train_loss:2.2502 val_acc:69.3694 val_loss:0.607
[05/0012] | train_loss:2.3541 val_acc:72.0721 val_loss:0.5895
model is saved at epoch 12!![05/0013] | train_loss:2.2403 val_acc:69.3694 val_loss:0.5847
[05/0014] | train_loss:2.2251 val_acc:70.2703 val_loss:0.5857
[05/0015] | train_loss:2.1906 val_acc:71.1712 val_loss:0.575
[05/0016] | train_loss:2.1993 val_acc:68.4685 val_loss:0.6114
[05/0017] | train_loss:2.3138 val_acc:70.2703 val_loss:0.5897
[05/0018] | train_loss:2.3469 val_acc:70.2703 val_loss:0.5889
[05/0019] | train_loss:2.3274 val_acc:69.3694 val_loss:0.5781
[05/0020] | train_loss:2.2729 val_acc:72.0721 val_loss:0.5702
[05/0021] | train_loss:2.2209 val_acc:71.1712 val_loss:0.5831
[05/0022] | train_loss:2.2345 val_acc:70.2703 val_loss:0.5726
[05/0023] | train_loss:2.2375 val_acc:71.1712 val_loss:0.5985
[05/0024] | train_loss:2.2365 val_acc:70.2703 val_loss:0.5792
[05/0025] | train_loss:2.2339 val_acc:71.1712 val_loss:0.5919
[05/0026] | train_loss:2.2015 val_acc:72.0721 val_loss:0.5786
[05/0027] | train_loss:2.2048 val_acc:70.2703 val_loss:0.5947
[05/0028] | train_loss:2.2353 val_acc:72.0721 val_loss:0.5783
[05/0029] | train_loss:2.2102 val_acc:72.973 val_loss:0.5864
model is saved at epoch 29!![05/0030] | train_loss:2.2122 val_acc:71.1712 val_loss:0.5995
[05/0031] | train_loss:2.1938 val_acc:71.1712 val_loss:0.567
[05/0032] | train_loss:2.213 val_acc:72.973 val_loss:0.5609
[05/0033] | train_loss:2.2089 val_acc:72.0721 val_loss:0.5864
[05/0034] | train_loss:2.1812 val_acc:72.0721 val_loss:0.5564
[05/0035] | train_loss:2.2873 val_acc:70.2703 val_loss:0.5726
[05/0036] | train_loss:2.2508 val_acc:69.3694 val_loss:0.5731
[05/0037] | train_loss:2.2543 val_acc:72.0721 val_loss:0.5622
[05/0038] | train_loss:2.1955 val_acc:72.973 val_loss:0.5512
[05/0039] | train_loss:2.1634 val_acc:72.0721 val_loss:0.601
[05/0040] | train_loss:2.1972 val_acc:68.4685 val_loss:0.6022
[05/0041] | train_loss:2.155 val_acc:72.0721 val_loss:0.5505
[05/0042] | train_loss:2.1776 val_acc:71.1712 val_loss:0.5609
[05/0043] | train_loss:2.1784 val_acc:69.3694 val_loss:0.6112
[05/0044] | train_loss:2.1717 val_acc:71.1712 val_loss:0.6059
[05/0045] | train_loss:2.1923 val_acc:70.2703 val_loss:0.7044
[05/0046] | train_loss:2.2094 val_acc:74.7748 val_loss:0.5661
model is saved at epoch 46!![05/0047] | train_loss:2.158 val_acc:69.3694 val_loss:0.5919
[05/0048] | train_loss:2.204 val_acc:70.2703 val_loss:0.5551
[05/0049] | train_loss:2.1816 val_acc:68.4685 val_loss:0.5855
[05/0050] | train_loss:2.1331 val_acc:72.0721 val_loss:0.5766
[05/0051] | train_loss:2.1743 val_acc:69.3694 val_loss:0.6061
[05/0052] | train_loss:2.1771 val_acc:72.0721 val_loss:0.58
[05/0053] | train_loss:2.1883 val_acc:71.1712 val_loss:0.5697
[05/0054] | train_loss:2.1415 val_acc:70.2703 val_loss:0.5951
[05/0055] | train_loss:2.166 val_acc:72.0721 val_loss:0.5805
[05/0056] | train_loss:2.1154 val_acc:69.3694 val_loss:0.5785
[05/0057] | train_loss:2.1506 val_acc:70.2703 val_loss:0.6336
[05/0058] | train_loss:2.2605 val_acc:74.7748 val_loss:0.5688
[05/0059] | train_loss:2.1604 val_acc:71.1712 val_loss:0.5988
[05/0060] | train_loss:2.1519 val_acc:72.0721 val_loss:0.6094
[05/0061] | train_loss:2.1438 val_acc:72.0721 val_loss:0.6068
[05/0062] | train_loss:2.1309 val_acc:72.0721 val_loss:0.5729
[05/0063] | train_loss:2.1059 val_acc:72.973 val_loss:0.6168
[05/0064] | train_loss:2.1786 val_acc:72.973 val_loss:0.589
[05/0065] | train_loss:2.0984 val_acc:70.2703 val_loss:0.6152
[05/0066] | train_loss:2.1617 val_acc:72.973 val_loss:0.5723
[05/0067] | train_loss:2.1238 val_acc:70.2703 val_loss:0.6036
[05/0068] | train_loss:2.1115 val_acc:72.0721 val_loss:0.5844
[05/0069] | train_loss:2.1063 val_acc:71.1712 val_loss:0.6128
[05/0070] | train_loss:2.1077 val_acc:74.7748 val_loss:0.5992
[05/0071] | train_loss:2.0854 val_acc:73.8739 val_loss:0.6201
[05/0072] | train_loss:2.1676 val_acc:74.7748 val_loss:0.5888
[05/0073] | train_loss:2.1312 val_acc:70.2703 val_loss:0.6149
[05/0074] | train_loss:2.1569 val_acc:70.2703 val_loss:0.6443
[05/0075] | train_loss:2.0803 val_acc:72.0721 val_loss:0.6248
[05/0076] | train_loss:2.181 val_acc:70.2703 val_loss:0.6151
[05/0077] | train_loss:2.1899 val_acc:69.3694 val_loss:0.6253
[05/0078] | train_loss:2.1698 val_acc:72.0721 val_loss:0.6263
[05/0079] | train_loss:2.2139 val_acc:69.3694 val_loss:0.5944
[05/0080] | train_loss:2.1776 val_acc:70.2703 val_loss:0.6231
[05/0081] | train_loss:2.2042 val_acc:71.1712 val_loss:0.6154
[05/0082] | train_loss:2.1402 val_acc:70.2703 val_loss:0.5982
[05/0083] | train_loss:2.1725 val_acc:69.3694 val_loss:0.5902
[05/0084] | train_loss:2.1583 val_acc:70.2703 val_loss:0.6257
[05/0085] | train_loss:2.0929 val_acc:71.1712 val_loss:0.5862
[05/0086] | train_loss:2.1472 val_acc:69.3694 val_loss:0.628
[05/0087] | train_loss:2.1118 val_acc:72.0721 val_loss:0.6179
[05/0088] | train_loss:2.09 val_acc:71.1712 val_loss:0.6024
[05/0089] | train_loss:2.0877 val_acc:72.973 val_loss:0.566
[05/0090] | train_loss:2.0579 val_acc:71.1712 val_loss:0.6082
[05/0091] | train_loss:2.0722 val_acc:72.0721 val_loss:0.607
[05/0092] | train_loss:2.0874 val_acc:72.973 val_loss:0.5882
[05/0093] | train_loss:2.1213 val_acc:72.973 val_loss:0.6043
[05/0094] | train_loss:2.1226 val_acc:72.0721 val_loss:0.5965
[05/0095] | train_loss:2.1379 val_acc:72.0721 val_loss:0.5871
[05/0096] | train_loss:2.1314 val_acc:73.8739 val_loss:0.5783
[05/0097] | train_loss:2.0755 val_acc:72.973 val_loss:0.6038
Fold: [5/10] Test is finish !! 
 Test Metrics are: test_acc:79.2793 test_loss:0.5825fold [5/10] is start!!
[06/0001] | train_loss:2.633 val_acc:55.8559 val_loss:0.6999
model is saved at epoch 1!![06/0002] | train_loss:2.4617 val_acc:55.8559 val_loss:0.6944
[06/0003] | train_loss:2.372 val_acc:59.4595 val_loss:0.6781
model is saved at epoch 3!![06/0004] | train_loss:2.2862 val_acc:70.2703 val_loss:0.605
model is saved at epoch 4!![06/0005] | train_loss:2.225 val_acc:70.2703 val_loss:0.5806
[06/0006] | train_loss:2.3054 val_acc:72.0721 val_loss:0.5872
model is saved at epoch 6!![06/0007] | train_loss:2.236 val_acc:73.8739 val_loss:0.5798
model is saved at epoch 7!![06/0008] | train_loss:2.2413 val_acc:69.3694 val_loss:0.6173
[06/0009] | train_loss:2.2462 val_acc:63.964 val_loss:0.6058
[06/0010] | train_loss:2.2163 val_acc:71.1712 val_loss:0.5918
[06/0011] | train_loss:2.1927 val_acc:67.5676 val_loss:0.6037
[06/0012] | train_loss:2.2544 val_acc:71.1712 val_loss:0.5956
[06/0013] | train_loss:2.2146 val_acc:69.3694 val_loss:0.6242
[06/0014] | train_loss:2.2081 val_acc:67.5676 val_loss:0.6473
[06/0015] | train_loss:2.226 val_acc:69.3694 val_loss:0.6043
[06/0016] | train_loss:2.1654 val_acc:72.973 val_loss:0.5423
[06/0017] | train_loss:2.1528 val_acc:63.0631 val_loss:0.7105
[06/0018] | train_loss:2.2614 val_acc:72.0721 val_loss:0.5957
[06/0019] | train_loss:2.1943 val_acc:71.1712 val_loss:0.5921
[06/0020] | train_loss:2.2243 val_acc:67.5676 val_loss:0.646
[06/0021] | train_loss:2.1978 val_acc:71.1712 val_loss:0.64
[06/0022] | train_loss:2.1902 val_acc:68.4685 val_loss:0.6177
[06/0023] | train_loss:2.1991 val_acc:73.8739 val_loss:0.5861
[06/0024] | train_loss:2.1652 val_acc:75.6757 val_loss:0.6288
model is saved at epoch 24!![06/0025] | train_loss:2.2032 val_acc:72.0721 val_loss:0.5616
[06/0026] | train_loss:2.2078 val_acc:68.4685 val_loss:0.6344
[06/0027] | train_loss:2.1411 val_acc:72.0721 val_loss:0.5995
[06/0028] | train_loss:2.1273 val_acc:73.8739 val_loss:0.5714
[06/0029] | train_loss:2.1843 val_acc:69.3694 val_loss:0.6465
[06/0030] | train_loss:2.1013 val_acc:71.1712 val_loss:0.5812
[06/0031] | train_loss:2.1464 val_acc:72.0721 val_loss:0.6198
[06/0032] | train_loss:2.2028 val_acc:71.1712 val_loss:0.6346
[06/0033] | train_loss:2.2892 val_acc:70.2703 val_loss:0.6009
[06/0034] | train_loss:2.169 val_acc:73.8739 val_loss:0.5822
[06/0035] | train_loss:2.1813 val_acc:72.973 val_loss:0.6198
[06/0036] | train_loss:2.1936 val_acc:72.0721 val_loss:0.5798
[06/0037] | train_loss:2.1254 val_acc:75.6757 val_loss:0.5583
[06/0038] | train_loss:2.1044 val_acc:77.4775 val_loss:0.5864
model is saved at epoch 38!![06/0039] | train_loss:2.1301 val_acc:74.7748 val_loss:0.5843
[06/0040] | train_loss:2.1187 val_acc:73.8739 val_loss:0.5853
[06/0041] | train_loss:2.1446 val_acc:75.6757 val_loss:0.5604
[06/0042] | train_loss:2.1557 val_acc:76.5766 val_loss:0.597
[06/0043] | train_loss:2.17 val_acc:73.8739 val_loss:0.5808
[06/0044] | train_loss:2.1212 val_acc:76.5766 val_loss:0.5527
[06/0045] | train_loss:2.1913 val_acc:72.0721 val_loss:0.6847
[06/0046] | train_loss:2.1691 val_acc:75.6757 val_loss:0.5478
[06/0047] | train_loss:2.2116 val_acc:73.8739 val_loss:0.6019
[06/0048] | train_loss:2.1849 val_acc:75.6757 val_loss:0.5698
[06/0049] | train_loss:2.1553 val_acc:76.5766 val_loss:0.5922
[06/0050] | train_loss:2.2081 val_acc:79.2793 val_loss:0.5483
model is saved at epoch 50!![06/0051] | train_loss:2.2009 val_acc:70.2703 val_loss:0.6167
[06/0052] | train_loss:2.129 val_acc:74.7748 val_loss:0.5908
[06/0053] | train_loss:2.1999 val_acc:73.8739 val_loss:0.5662
[06/0054] | train_loss:2.1545 val_acc:73.8739 val_loss:0.556
[06/0055] | train_loss:2.2013 val_acc:74.7748 val_loss:0.5879
[06/0056] | train_loss:2.1417 val_acc:76.5766 val_loss:0.5536
[06/0057] | train_loss:2.1941 val_acc:76.5766 val_loss:0.5829
[06/0058] | train_loss:2.1016 val_acc:78.3784 val_loss:0.5953
[06/0059] | train_loss:2.1958 val_acc:77.4775 val_loss:0.5778
[06/0060] | train_loss:2.1143 val_acc:72.973 val_loss:0.5783
[06/0061] | train_loss:2.1555 val_acc:75.6757 val_loss:0.5665
[06/0062] | train_loss:2.1039 val_acc:75.6757 val_loss:0.5958
[06/0063] | train_loss:2.1058 val_acc:73.8739 val_loss:0.589
[06/0064] | train_loss:2.1377 val_acc:74.7748 val_loss:0.5711
[06/0065] | train_loss:2.1481 val_acc:72.973 val_loss:0.57
[06/0066] | train_loss:2.1438 val_acc:76.5766 val_loss:0.5809
[06/0067] | train_loss:2.1305 val_acc:74.7748 val_loss:0.547
[06/0068] | train_loss:2.1652 val_acc:72.0721 val_loss:0.5735
[06/0069] | train_loss:2.078 val_acc:73.8739 val_loss:0.6016
[06/0070] | train_loss:2.0949 val_acc:75.6757 val_loss:0.5641
[06/0071] | train_loss:2.1246 val_acc:72.973 val_loss:0.568
[06/0072] | train_loss:2.0843 val_acc:77.4775 val_loss:0.5544
[06/0073] | train_loss:2.1001 val_acc:73.8739 val_loss:0.5631
[06/0074] | train_loss:2.1036 val_acc:77.4775 val_loss:0.5889
[06/0075] | train_loss:2.1259 val_acc:75.6757 val_loss:0.5548
[06/0076] | train_loss:2.1148 val_acc:73.8739 val_loss:0.5854
[06/0077] | train_loss:2.107 val_acc:76.5766 val_loss:0.5674
[06/0078] | train_loss:2.0944 val_acc:75.6757 val_loss:0.5994
[06/0079] | train_loss:2.0748 val_acc:73.8739 val_loss:0.634
[06/0080] | train_loss:2.1118 val_acc:73.8739 val_loss:0.5774
[06/0081] | train_loss:2.1462 val_acc:73.8739 val_loss:0.5621
[06/0082] | train_loss:2.1193 val_acc:72.973 val_loss:0.6355
[06/0083] | train_loss:2.1387 val_acc:74.7748 val_loss:0.5823
[06/0084] | train_loss:2.1159 val_acc:74.7748 val_loss:0.5998
[06/0085] | train_loss:2.0792 val_acc:77.4775 val_loss:0.5628
[06/0086] | train_loss:2.0977 val_acc:75.6757 val_loss:0.6079
[06/0087] | train_loss:2.0703 val_acc:72.973 val_loss:0.59
[06/0088] | train_loss:2.0945 val_acc:72.973 val_loss:0.6151
[06/0089] | train_loss:2.0702 val_acc:73.8739 val_loss:0.586
[06/0090] | train_loss:2.148 val_acc:74.7748 val_loss:0.5931
[06/0091] | train_loss:2.0812 val_acc:75.6757 val_loss:0.5757
[06/0092] | train_loss:2.1485 val_acc:73.8739 val_loss:0.5894
[06/0093] | train_loss:2.0922 val_acc:71.1712 val_loss:0.6191
[06/0094] | train_loss:2.1551 val_acc:75.6757 val_loss:0.5771
[06/0095] | train_loss:2.1055 val_acc:75.6757 val_loss:0.6108
[06/0096] | train_loss:2.0344 val_acc:74.7748 val_loss:0.5911
[06/0097] | train_loss:2.0586 val_acc:73.8739 val_loss:0.6016
[06/0098] | train_loss:2.088 val_acc:74.7748 val_loss:0.5932
[06/0099] | train_loss:2.1266 val_acc:72.973 val_loss:0.5975
[06/0100] | train_loss:2.0988 val_acc:72.0721 val_loss:0.6336
[06/0101] | train_loss:2.1047 val_acc:72.0721 val_loss:0.6503
Fold: [6/10] Test is finish !! 
 Test Metrics are: test_acc:71.1712 test_loss:0.5977fold [6/10] is start!!
[07/0001] | train_loss:2.6761 val_acc:60.3604 val_loss:0.6882
model is saved at epoch 1!![07/0002] | train_loss:2.4171 val_acc:60.3604 val_loss:0.6797
[07/0003] | train_loss:2.2725 val_acc:67.5676 val_loss:0.6511
model is saved at epoch 3!![07/0004] | train_loss:2.303 val_acc:70.2703 val_loss:0.602
model is saved at epoch 4!![07/0005] | train_loss:2.3379 val_acc:72.0721 val_loss:0.5875
model is saved at epoch 5!![07/0006] | train_loss:2.2537 val_acc:71.1712 val_loss:0.5929
[07/0007] | train_loss:2.2035 val_acc:70.2703 val_loss:0.5988
[07/0008] | train_loss:2.2283 val_acc:68.4685 val_loss:0.6097
[07/0009] | train_loss:2.1935 val_acc:70.2703 val_loss:0.58
[07/0010] | train_loss:2.194 val_acc:64.8649 val_loss:0.5961
[07/0011] | train_loss:2.2459 val_acc:68.4685 val_loss:0.5989
[07/0012] | train_loss:2.2236 val_acc:67.5676 val_loss:0.5922
[07/0013] | train_loss:2.2081 val_acc:69.3694 val_loss:0.6023
[07/0014] | train_loss:2.2193 val_acc:67.5676 val_loss:0.5842
[07/0015] | train_loss:2.2448 val_acc:67.5676 val_loss:0.5889
[07/0016] | train_loss:2.2182 val_acc:69.3694 val_loss:0.5755
[07/0017] | train_loss:2.1894 val_acc:67.5676 val_loss:0.5951
[07/0018] | train_loss:2.1864 val_acc:71.1712 val_loss:0.5796
[07/0019] | train_loss:2.1363 val_acc:68.4685 val_loss:0.5943
[07/0020] | train_loss:2.2159 val_acc:68.4685 val_loss:0.6146
[07/0021] | train_loss:2.2017 val_acc:66.6667 val_loss:0.5883
[07/0022] | train_loss:2.1919 val_acc:66.6667 val_loss:0.5848
[07/0023] | train_loss:2.1551 val_acc:68.4685 val_loss:0.6038
[07/0024] | train_loss:2.191 val_acc:68.4685 val_loss:0.6003
[07/0025] | train_loss:2.236 val_acc:66.6667 val_loss:0.6062
[07/0026] | train_loss:2.2265 val_acc:68.4685 val_loss:0.5947
[07/0027] | train_loss:2.2214 val_acc:66.6667 val_loss:0.6076
[07/0028] | train_loss:2.2074 val_acc:68.4685 val_loss:0.5902
[07/0029] | train_loss:2.1857 val_acc:66.6667 val_loss:0.5884
[07/0030] | train_loss:2.1825 val_acc:64.8649 val_loss:0.6076
[07/0031] | train_loss:2.1452 val_acc:69.3694 val_loss:0.5944
[07/0032] | train_loss:2.1268 val_acc:65.7658 val_loss:0.6101
[07/0033] | train_loss:2.1931 val_acc:69.3694 val_loss:0.6023
[07/0034] | train_loss:2.1799 val_acc:68.4685 val_loss:0.5973
[07/0035] | train_loss:2.2184 val_acc:67.5676 val_loss:0.5958
[07/0036] | train_loss:2.1575 val_acc:67.5676 val_loss:0.5863
[07/0037] | train_loss:2.1568 val_acc:66.6667 val_loss:0.5943
[07/0038] | train_loss:2.1409 val_acc:66.6667 val_loss:0.5852
[07/0039] | train_loss:2.1622 val_acc:64.8649 val_loss:0.5933
[07/0040] | train_loss:2.1761 val_acc:66.6667 val_loss:0.6027
[07/0041] | train_loss:2.2091 val_acc:68.4685 val_loss:0.606
[07/0042] | train_loss:2.1998 val_acc:70.2703 val_loss:0.5954
[07/0043] | train_loss:2.2222 val_acc:68.4685 val_loss:0.5857
[07/0044] | train_loss:2.1521 val_acc:67.5676 val_loss:0.5728
[07/0045] | train_loss:2.0935 val_acc:68.4685 val_loss:0.5901
[07/0046] | train_loss:2.2223 val_acc:68.4685 val_loss:0.6129
[07/0047] | train_loss:2.1539 val_acc:71.1712 val_loss:0.5772
[07/0048] | train_loss:2.0942 val_acc:71.1712 val_loss:0.5766
[07/0049] | train_loss:2.1352 val_acc:67.5676 val_loss:0.5898
[07/0050] | train_loss:2.067 val_acc:70.2703 val_loss:0.5943
[07/0051] | train_loss:2.1043 val_acc:65.7658 val_loss:0.6135
[07/0052] | train_loss:2.1936 val_acc:68.4685 val_loss:0.5793
[07/0053] | train_loss:2.1711 val_acc:70.2703 val_loss:0.5846
[07/0054] | train_loss:2.1414 val_acc:69.3694 val_loss:0.6025
[07/0055] | train_loss:2.1272 val_acc:64.8649 val_loss:0.5912
[07/0056] | train_loss:2.0887 val_acc:63.964 val_loss:0.6191
Fold: [7/10] Test is finish !! 
 Test Metrics are: test_acc:79.2793 test_loss:0.5178fold [7/10] is start!!
[08/0001] | train_loss:2.6301 val_acc:65.7658 val_loss:0.6403
model is saved at epoch 1!![08/0002] | train_loss:2.3873 val_acc:67.5676 val_loss:0.6185
model is saved at epoch 2!![08/0003] | train_loss:2.3417 val_acc:73.8739 val_loss:0.5763
model is saved at epoch 3!![08/0004] | train_loss:2.3471 val_acc:81.0811 val_loss:0.534
model is saved at epoch 4!![08/0005] | train_loss:2.3535 val_acc:77.4775 val_loss:0.5517
[08/0006] | train_loss:2.3123 val_acc:78.3784 val_loss:0.5204
[08/0007] | train_loss:2.2012 val_acc:75.6757 val_loss:0.5196
[08/0008] | train_loss:2.2612 val_acc:75.6757 val_loss:0.5405
[08/0009] | train_loss:2.3243 val_acc:77.4775 val_loss:0.5198
[08/0010] | train_loss:2.3184 val_acc:72.0721 val_loss:0.5351
[08/0011] | train_loss:2.2686 val_acc:75.6757 val_loss:0.5258
[08/0012] | train_loss:2.2765 val_acc:70.2703 val_loss:0.5208
[08/0013] | train_loss:2.2408 val_acc:76.5766 val_loss:0.5198
[08/0014] | train_loss:2.2769 val_acc:79.2793 val_loss:0.5131
[08/0015] | train_loss:2.2711 val_acc:74.7748 val_loss:0.536
[08/0016] | train_loss:2.215 val_acc:75.6757 val_loss:0.5083
[08/0017] | train_loss:2.2783 val_acc:75.6757 val_loss:0.5157
[08/0018] | train_loss:2.3019 val_acc:76.5766 val_loss:0.5276
[08/0019] | train_loss:2.2225 val_acc:74.7748 val_loss:0.5172
[08/0020] | train_loss:2.2622 val_acc:75.6757 val_loss:0.5121
[08/0021] | train_loss:2.2129 val_acc:81.982 val_loss:0.5128
model is saved at epoch 21!![08/0022] | train_loss:2.2703 val_acc:77.4775 val_loss:0.5141
[08/0023] | train_loss:2.2343 val_acc:81.982 val_loss:0.506
[08/0024] | train_loss:2.2145 val_acc:77.4775 val_loss:0.5038
[08/0025] | train_loss:2.1558 val_acc:74.7748 val_loss:0.5116
[08/0026] | train_loss:2.1825 val_acc:75.6757 val_loss:0.5048
[08/0027] | train_loss:2.1628 val_acc:74.7748 val_loss:0.5094
[08/0028] | train_loss:2.3057 val_acc:74.7748 val_loss:0.5292
[08/0029] | train_loss:2.2096 val_acc:73.8739 val_loss:0.5176
[08/0030] | train_loss:2.2065 val_acc:74.7748 val_loss:0.516
[08/0031] | train_loss:2.1602 val_acc:74.7748 val_loss:0.52
[08/0032] | train_loss:2.1768 val_acc:72.973 val_loss:0.5205
[08/0033] | train_loss:2.1861 val_acc:72.0721 val_loss:0.5093
[08/0034] | train_loss:2.1588 val_acc:76.5766 val_loss:0.5125
[08/0035] | train_loss:2.17 val_acc:75.6757 val_loss:0.5201
[08/0036] | train_loss:2.2647 val_acc:71.1712 val_loss:0.5494
[08/0037] | train_loss:2.2873 val_acc:77.4775 val_loss:0.503
[08/0038] | train_loss:2.2139 val_acc:73.8739 val_loss:0.5089
[08/0039] | train_loss:2.2228 val_acc:74.7748 val_loss:0.506
[08/0040] | train_loss:2.2628 val_acc:73.8739 val_loss:0.5159
[08/0041] | train_loss:2.2026 val_acc:73.8739 val_loss:0.5045
[08/0042] | train_loss:2.207 val_acc:74.7748 val_loss:0.508
[08/0043] | train_loss:2.1293 val_acc:78.3784 val_loss:0.5207
[08/0044] | train_loss:2.207 val_acc:74.7748 val_loss:0.5172
[08/0045] | train_loss:2.1543 val_acc:79.2793 val_loss:0.5012
[08/0046] | train_loss:2.1717 val_acc:72.973 val_loss:0.5022
[08/0047] | train_loss:2.1366 val_acc:78.3784 val_loss:0.4923
[08/0048] | train_loss:2.174 val_acc:72.0721 val_loss:0.521
[08/0049] | train_loss:2.2109 val_acc:75.6757 val_loss:0.52
[08/0050] | train_loss:2.1921 val_acc:73.8739 val_loss:0.5089
[08/0051] | train_loss:2.1912 val_acc:72.0721 val_loss:0.5135
[08/0052] | train_loss:2.2054 val_acc:72.973 val_loss:0.5225
[08/0053] | train_loss:2.169 val_acc:70.2703 val_loss:0.5512
[08/0054] | train_loss:2.1804 val_acc:75.6757 val_loss:0.5189
[08/0055] | train_loss:2.175 val_acc:71.1712 val_loss:0.5195
[08/0056] | train_loss:2.1881 val_acc:69.3694 val_loss:0.5293
[08/0057] | train_loss:2.1305 val_acc:73.8739 val_loss:0.5261
[08/0058] | train_loss:2.1501 val_acc:73.8739 val_loss:0.5273
[08/0059] | train_loss:2.1342 val_acc:72.973 val_loss:0.5097
[08/0060] | train_loss:2.1224 val_acc:72.0721 val_loss:0.5327
[08/0061] | train_loss:2.1812 val_acc:76.5766 val_loss:0.5136
[08/0062] | train_loss:2.1474 val_acc:76.5766 val_loss:0.5083
[08/0063] | train_loss:2.1958 val_acc:76.5766 val_loss:0.5081
[08/0064] | train_loss:2.1487 val_acc:73.8739 val_loss:0.5174
[08/0065] | train_loss:2.1263 val_acc:76.5766 val_loss:0.5189
[08/0066] | train_loss:2.1494 val_acc:72.973 val_loss:0.5092
[08/0067] | train_loss:2.1509 val_acc:77.4775 val_loss:0.5065
[08/0068] | train_loss:2.1977 val_acc:70.2703 val_loss:0.5376
[08/0069] | train_loss:2.1887 val_acc:76.5766 val_loss:0.5165
[08/0070] | train_loss:2.1802 val_acc:70.2703 val_loss:0.5165
[08/0071] | train_loss:2.1403 val_acc:76.5766 val_loss:0.5013
[08/0072] | train_loss:2.2098 val_acc:76.5766 val_loss:0.5164
Fold: [8/10] Test is finish !! 
 Test Metrics are: test_acc:78.3784 test_loss:0.5088fold [8/10] is start!!
[09/0001] | train_loss:2.7366 val_acc:69.3694 val_loss:0.6497
model is saved at epoch 1!![09/0002] | train_loss:2.4971 val_acc:71.1712 val_loss:0.6191
model is saved at epoch 2!![09/0003] | train_loss:2.3579 val_acc:73.8739 val_loss:0.5939
model is saved at epoch 3!![09/0004] | train_loss:2.3203 val_acc:72.973 val_loss:0.5553
[09/0005] | train_loss:2.307 val_acc:73.8739 val_loss:0.5354
[09/0006] | train_loss:2.2757 val_acc:74.7748 val_loss:0.5772
model is saved at epoch 6!![09/0007] | train_loss:2.3033 val_acc:72.973 val_loss:0.599
[09/0008] | train_loss:2.313 val_acc:72.973 val_loss:0.5593
[09/0009] | train_loss:2.2999 val_acc:69.3694 val_loss:0.5965
[09/0010] | train_loss:2.3187 val_acc:73.8739 val_loss:0.6247
[09/0011] | train_loss:2.3439 val_acc:73.8739 val_loss:0.5675
[09/0012] | train_loss:2.2669 val_acc:73.8739 val_loss:0.5465
[09/0013] | train_loss:2.2565 val_acc:73.8739 val_loss:0.5648
[09/0014] | train_loss:2.2853 val_acc:72.0721 val_loss:0.5395
[09/0015] | train_loss:2.2729 val_acc:73.8739 val_loss:0.5326
[09/0016] | train_loss:2.2849 val_acc:72.0721 val_loss:0.5561
[09/0017] | train_loss:2.2539 val_acc:73.8739 val_loss:0.5552
[09/0018] | train_loss:2.2498 val_acc:74.7748 val_loss:0.5495
[09/0019] | train_loss:2.2231 val_acc:72.0721 val_loss:0.5444
[09/0020] | train_loss:2.2138 val_acc:72.973 val_loss:0.5433
[09/0021] | train_loss:2.2144 val_acc:72.0721 val_loss:0.5305
[09/0022] | train_loss:2.208 val_acc:70.2703 val_loss:0.6015
[09/0023] | train_loss:2.2637 val_acc:72.0721 val_loss:0.5637
[09/0024] | train_loss:2.2355 val_acc:69.3694 val_loss:0.5905
[09/0025] | train_loss:2.2678 val_acc:72.973 val_loss:0.5666
[09/0026] | train_loss:2.2477 val_acc:72.0721 val_loss:0.537
[09/0027] | train_loss:2.2421 val_acc:71.1712 val_loss:0.5419
[09/0028] | train_loss:2.2722 val_acc:73.8739 val_loss:0.5319
[09/0029] | train_loss:2.25 val_acc:72.973 val_loss:0.5418
[09/0030] | train_loss:2.1879 val_acc:72.973 val_loss:0.5255
[09/0031] | train_loss:2.1898 val_acc:72.0721 val_loss:0.5295
[09/0032] | train_loss:2.2481 val_acc:72.0721 val_loss:0.5542
[09/0033] | train_loss:2.2098 val_acc:72.0721 val_loss:0.5269
[09/0034] | train_loss:2.2187 val_acc:72.973 val_loss:0.5393
[09/0035] | train_loss:2.232 val_acc:71.1712 val_loss:0.5341
[09/0036] | train_loss:2.2019 val_acc:72.973 val_loss:0.5837
[09/0037] | train_loss:2.1771 val_acc:70.2703 val_loss:0.5277
[09/0038] | train_loss:2.1781 val_acc:72.973 val_loss:0.5333
[09/0039] | train_loss:2.1709 val_acc:73.8739 val_loss:0.5421
[09/0040] | train_loss:2.2125 val_acc:72.973 val_loss:0.5713
[09/0041] | train_loss:2.2753 val_acc:74.7748 val_loss:0.547
[09/0042] | train_loss:2.2297 val_acc:74.7748 val_loss:0.5421
[09/0043] | train_loss:2.2054 val_acc:76.5766 val_loss:0.5467
model is saved at epoch 43!![09/0044] | train_loss:2.2182 val_acc:74.7748 val_loss:0.5246
[09/0045] | train_loss:2.2119 val_acc:77.4775 val_loss:0.5505
model is saved at epoch 45!![09/0046] | train_loss:2.1729 val_acc:75.6757 val_loss:0.5367
[09/0047] | train_loss:2.1631 val_acc:73.8739 val_loss:0.5671
[09/0048] | train_loss:2.1917 val_acc:72.0721 val_loss:0.5627
[09/0049] | train_loss:2.2497 val_acc:72.0721 val_loss:0.5734
[09/0050] | train_loss:2.2174 val_acc:72.973 val_loss:0.5667
[09/0051] | train_loss:2.2603 val_acc:74.7748 val_loss:0.5474
[09/0052] | train_loss:2.271 val_acc:74.7748 val_loss:0.5475
[09/0053] | train_loss:2.2554 val_acc:72.973 val_loss:0.5703
[09/0054] | train_loss:2.2708 val_acc:74.7748 val_loss:0.5596
[09/0055] | train_loss:2.2177 val_acc:74.7748 val_loss:0.5166
[09/0056] | train_loss:2.2291 val_acc:75.6757 val_loss:0.5343
[09/0057] | train_loss:2.2436 val_acc:72.0721 val_loss:0.5574
[09/0058] | train_loss:2.2469 val_acc:71.1712 val_loss:0.5314
[09/0059] | train_loss:2.2431 val_acc:71.1712 val_loss:0.557
[09/0060] | train_loss:2.2164 val_acc:74.7748 val_loss:0.5647
[09/0061] | train_loss:2.1986 val_acc:76.5766 val_loss:0.5368
[09/0062] | train_loss:2.2066 val_acc:76.5766 val_loss:0.523
[09/0063] | train_loss:2.2052 val_acc:72.973 val_loss:0.5441
[09/0064] | train_loss:2.1937 val_acc:72.973 val_loss:0.5385
[09/0065] | train_loss:2.2322 val_acc:72.0721 val_loss:0.5802
[09/0066] | train_loss:2.2284 val_acc:74.7748 val_loss:0.5889
[09/0067] | train_loss:2.1923 val_acc:74.7748 val_loss:0.5634
[09/0068] | train_loss:2.154 val_acc:73.8739 val_loss:0.5501
[09/0069] | train_loss:2.2157 val_acc:72.973 val_loss:0.5216
[09/0070] | train_loss:2.2042 val_acc:74.7748 val_loss:0.5576
[09/0071] | train_loss:2.1969 val_acc:73.8739 val_loss:0.5299
[09/0072] | train_loss:2.2054 val_acc:72.0721 val_loss:0.5391
[09/0073] | train_loss:2.1778 val_acc:72.0721 val_loss:0.5497
[09/0074] | train_loss:2.1693 val_acc:75.6757 val_loss:0.5187
[09/0075] | train_loss:2.121 val_acc:77.4775 val_loss:0.498
[09/0076] | train_loss:2.1509 val_acc:72.973 val_loss:0.5502
[09/0077] | train_loss:2.1559 val_acc:72.0721 val_loss:0.5414
[09/0078] | train_loss:2.1327 val_acc:73.8739 val_loss:0.5357
[09/0079] | train_loss:2.1537 val_acc:72.973 val_loss:0.528
[09/0080] | train_loss:2.167 val_acc:72.973 val_loss:0.552
[09/0081] | train_loss:2.1322 val_acc:72.973 val_loss:0.5166
[09/0082] | train_loss:2.1591 val_acc:73.8739 val_loss:0.5613
[09/0083] | train_loss:2.1485 val_acc:72.0721 val_loss:0.5113
[09/0084] | train_loss:2.1217 val_acc:72.973 val_loss:0.5421
[09/0085] | train_loss:2.1471 val_acc:74.7748 val_loss:0.5181
[09/0086] | train_loss:2.1326 val_acc:72.973 val_loss:0.5343
[09/0087] | train_loss:2.1585 val_acc:72.0721 val_loss:0.5603
[09/0088] | train_loss:2.1885 val_acc:75.6757 val_loss:0.5041
[09/0089] | train_loss:2.1748 val_acc:76.5766 val_loss:0.5365
[09/0090] | train_loss:2.15 val_acc:73.8739 val_loss:0.5021
[09/0091] | train_loss:2.1169 val_acc:72.973 val_loss:0.5176
[09/0092] | train_loss:2.1115 val_acc:74.7748 val_loss:0.5345
[09/0093] | train_loss:2.1375 val_acc:75.6757 val_loss:0.5411
[09/0094] | train_loss:2.2424 val_acc:72.973 val_loss:0.549
[09/0095] | train_loss:2.1989 val_acc:73.8739 val_loss:0.5462
[09/0096] | train_loss:2.1796 val_acc:74.7748 val_loss:0.5126
Fold: [9/10] Test is finish !! 
 Test Metrics are: test_acc:83.7838 test_loss:0.4812fold [9/10] is start!!
[10/0001] | train_loss:2.5919 val_acc:63.0631 val_loss:0.6535
model is saved at epoch 1!![10/0002] | train_loss:2.4945 val_acc:65.7658 val_loss:0.6278
model is saved at epoch 2!![10/0003] | train_loss:2.3601 val_acc:78.3784 val_loss:0.5226
model is saved at epoch 3!![10/0004] | train_loss:2.5129 val_acc:80.1802 val_loss:0.5597
model is saved at epoch 4!![10/0005] | train_loss:2.3969 val_acc:81.0811 val_loss:0.5014
model is saved at epoch 5!![10/0006] | train_loss:2.3173 val_acc:81.0811 val_loss:0.4856
[10/0007] | train_loss:2.3209 val_acc:79.2793 val_loss:0.474
[10/0008] | train_loss:2.2653 val_acc:81.0811 val_loss:0.4853
[10/0009] | train_loss:2.2982 val_acc:79.2793 val_loss:0.4758
[10/0010] | train_loss:2.3357 val_acc:80.1802 val_loss:0.4876
[10/0011] | train_loss:2.2894 val_acc:79.2793 val_loss:0.4758
[10/0012] | train_loss:2.3833 val_acc:75.6757 val_loss:0.5451
[10/0013] | train_loss:2.3339 val_acc:81.0811 val_loss:0.4768
[10/0014] | train_loss:2.2874 val_acc:79.2793 val_loss:0.5075
[10/0015] | train_loss:2.2581 val_acc:79.2793 val_loss:0.4774
[10/0016] | train_loss:2.2884 val_acc:81.0811 val_loss:0.4824
[10/0017] | train_loss:2.2034 val_acc:77.4775 val_loss:0.4834
[10/0018] | train_loss:2.2895 val_acc:80.1802 val_loss:0.4564
[10/0019] | train_loss:2.3144 val_acc:81.0811 val_loss:0.5022
[10/0020] | train_loss:2.3061 val_acc:77.4775 val_loss:0.4702
[10/0021] | train_loss:2.2359 val_acc:81.0811 val_loss:0.4557
[10/0022] | train_loss:2.3069 val_acc:78.3784 val_loss:0.4789
[10/0023] | train_loss:2.2118 val_acc:81.0811 val_loss:0.4723
[10/0024] | train_loss:2.2303 val_acc:80.1802 val_loss:0.4771
[10/0025] | train_loss:2.2333 val_acc:80.1802 val_loss:0.4949
[10/0026] | train_loss:2.2555 val_acc:79.2793 val_loss:0.4841
[10/0027] | train_loss:2.2866 val_acc:79.2793 val_loss:0.4782
[10/0028] | train_loss:2.2463 val_acc:80.1802 val_loss:0.4673
[10/0029] | train_loss:2.2408 val_acc:81.0811 val_loss:0.4787
[10/0030] | train_loss:2.269 val_acc:77.4775 val_loss:0.5211
[10/0031] | train_loss:2.2924 val_acc:79.2793 val_loss:0.4772
[10/0032] | train_loss:2.2455 val_acc:81.0811 val_loss:0.4976
[10/0033] | train_loss:2.2322 val_acc:79.2793 val_loss:0.4562
[10/0034] | train_loss:2.2673 val_acc:81.982 val_loss:0.4884
model is saved at epoch 34!![10/0035] | train_loss:2.2776 val_acc:81.982 val_loss:0.4843
[10/0036] | train_loss:2.2129 val_acc:81.0811 val_loss:0.4563
[10/0037] | train_loss:2.2031 val_acc:81.0811 val_loss:0.4701
[10/0038] | train_loss:2.2103 val_acc:81.0811 val_loss:0.4603
[10/0039] | train_loss:2.2059 val_acc:80.1802 val_loss:0.4693
[10/0040] | train_loss:2.2025 val_acc:80.1802 val_loss:0.4647
[10/0041] | train_loss:2.225 val_acc:81.982 val_loss:0.4726
[10/0042] | train_loss:2.2099 val_acc:79.2793 val_loss:0.4668
[10/0043] | train_loss:2.2023 val_acc:80.1802 val_loss:0.4681
[10/0044] | train_loss:2.2026 val_acc:82.8829 val_loss:0.4504
model is saved at epoch 44!![10/0045] | train_loss:2.2167 val_acc:82.8829 val_loss:0.4841
[10/0046] | train_loss:2.1797 val_acc:80.1802 val_loss:0.484
[10/0047] | train_loss:2.2391 val_acc:81.982 val_loss:0.4681
[10/0048] | train_loss:2.189 val_acc:80.1802 val_loss:0.4472
[10/0049] | train_loss:2.209 val_acc:83.7838 val_loss:0.4472
model is saved at epoch 49!![10/0050] | train_loss:2.1899 val_acc:81.0811 val_loss:0.4689
[10/0051] | train_loss:2.1793 val_acc:81.982 val_loss:0.4684
[10/0052] | train_loss:2.1749 val_acc:81.982 val_loss:0.4613
[10/0053] | train_loss:2.1411 val_acc:80.1802 val_loss:0.4572
[10/0054] | train_loss:2.1724 val_acc:83.7838 val_loss:0.4843
[10/0055] | train_loss:2.1893 val_acc:81.0811 val_loss:0.4626
[10/0056] | train_loss:2.2199 val_acc:80.1802 val_loss:0.503
[10/0057] | train_loss:2.2103 val_acc:81.982 val_loss:0.4656
[10/0058] | train_loss:2.1909 val_acc:79.2793 val_loss:0.4894
[10/0059] | train_loss:2.2027 val_acc:81.982 val_loss:0.4511
[10/0060] | train_loss:2.1949 val_acc:82.8829 val_loss:0.4514
[10/0061] | train_loss:2.1818 val_acc:81.982 val_loss:0.4489
[10/0062] | train_loss:2.1524 val_acc:79.2793 val_loss:0.4656
[10/0063] | train_loss:2.2117 val_acc:81.982 val_loss:0.4772
[10/0064] | train_loss:2.1892 val_acc:81.982 val_loss:0.4688
[10/0065] | train_loss:2.203 val_acc:81.982 val_loss:0.487
[10/0066] | train_loss:2.1911 val_acc:80.1802 val_loss:0.472
[10/0067] | train_loss:2.1238 val_acc:80.1802 val_loss:0.4899
[10/0068] | train_loss:2.1867 val_acc:78.3784 val_loss:0.4923
[10/0069] | train_loss:2.1779 val_acc:82.8829 val_loss:0.4671
[10/0070] | train_loss:2.2408 val_acc:81.0811 val_loss:0.4847
[10/0071] | train_loss:2.2348 val_acc:81.0811 val_loss:0.4708
[10/0072] | train_loss:2.1664 val_acc:79.2793 val_loss:0.4847
[10/0073] | train_loss:2.1716 val_acc:81.0811 val_loss:0.4819
[10/0074] | train_loss:2.181 val_acc:81.0811 val_loss:0.4758
[10/0075] | train_loss:2.1983 val_acc:83.7838 val_loss:0.4587
[10/0076] | train_loss:2.2039 val_acc:83.7838 val_loss:0.4738
[10/0077] | train_loss:2.2179 val_acc:81.0811 val_loss:0.475
[10/0078] | train_loss:2.1347 val_acc:81.0811 val_loss:0.4749
[10/0079] | train_loss:2.153 val_acc:81.0811 val_loss:0.4795
[10/0080] | train_loss:2.155 val_acc:81.0811 val_loss:0.4915
[10/0081] | train_loss:2.1449 val_acc:80.1802 val_loss:0.4832
[10/0082] | train_loss:2.1662 val_acc:82.8829 val_loss:0.4835
[10/0083] | train_loss:2.171 val_acc:82.8829 val_loss:0.469
[10/0084] | train_loss:2.1654 val_acc:81.0811 val_loss:0.4843
[10/0085] | train_loss:2.1771 val_acc:81.0811 val_loss:0.4695
[10/0086] | train_loss:2.1287 val_acc:81.982 val_loss:0.4925
[10/0087] | train_loss:2.1652 val_acc:81.982 val_loss:0.4751
[10/0088] | train_loss:2.1591 val_acc:81.982 val_loss:0.4931
[10/0089] | train_loss:2.1154 val_acc:83.7838 val_loss:0.4661
[10/0090] | train_loss:2.146 val_acc:81.982 val_loss:0.4768
[10/0091] | train_loss:2.2033 val_acc:81.0811 val_loss:0.4915
[10/0092] | train_loss:2.1133 val_acc:82.8829 val_loss:0.4809
[10/0093] | train_loss:2.1291 val_acc:80.1802 val_loss:0.4977
[10/0094] | train_loss:2.1202 val_acc:81.982 val_loss:0.4938
[10/0095] | train_loss:2.1139 val_acc:81.0811 val_loss:0.5066
[10/0096] | train_loss:2.1217 val_acc:81.0811 val_loss:0.4883
[10/0097] | train_loss:2.0772 val_acc:81.0811 val_loss:0.4712
[10/0098] | train_loss:2.0823 val_acc:81.0811 val_loss:0.4902
[10/0099] | train_loss:2.1258 val_acc:81.982 val_loss:0.504
[10/0100] | train_loss:2.1146 val_acc:81.982 val_loss:0.5272
Fold: [10/10] Test is finish !! 
 Test Metrics are: test_acc:78.3784 test_loss:0.4815
all fold acc is: 
[77.67857313156128, 71.42857313156128, 82.14285969734192, 71.17117047309875, 79.2792797088623, 71.17117047309875, 79.2792797088623, 78.37837934494019, 83.7837815284729, 78.37837934494019] 
Test is finish !! 
 Test Metrics are: acc_mean:77.2691 acc_std:4.3056